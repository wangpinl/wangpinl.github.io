<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[flink流式计算学习02]]></title>
    <url>%2F2020%2F04%2F28%2Fflink%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A002%2F</url>
    <content type="text"><![CDATA[API操作API介绍 flink提供了不同的抽象级别以开发流式或者批式应用程序 Stateful Stream Processing 最低级的抽象接口是状态化的数据流接口(stateful streaming).这个接口是通过ProcessFunction集成到DataStreamAPI中的.该接口允许用户自由的处理来自一个或多个流中的事件,并使用一致的容错状态.另外,用户也可以通过注册event time和processing time处理回调函数的方法来实现复杂的计算 DataStream/DataSetAPI 是Flink提供的核心API,DataSet处理有界的数据集,DataStream处理有界或者无界的数据流.用户可以通过各种方法(map/flatmap/window/keyby/sum/max/min/avg/join等)将数据进行转换/计算TableAPI Table API 提供了例如select/project/join/group-by/aggregate等操作,使用起来更加简洁,可以在表与DataStream/DataSet之间无缝切换,也允许程序将TableAPI与DataStream以及DataSet混合使用 SQL Flink 提供的最高层级的抽象是SQL.这一层抽象在语法与表达能力上与Table API类似.SQL抽象与TableAPI交互密切,同时SQL查询可以直接在TableAPI定义的表上执行. DataFlows数据流图 在flink的世界观中,一切都是数据流.所以对于批计算来说,只是流计算的一个特例 flink dataflows由三部分组成,分别是:source,transformation,sink source 源源不断的产生数据 tranformation 进行各种业务逻辑的数据处理 slink 输出到外部 当source数据源的数据量比较大或者计算逻辑相对比较复杂的情况下,需要提高并行度来处理数据,采用并行数据流 通过设置不同算子的并行度,source的并行度设置为2,map也是2…来代表启动多个并行的线程来处理数据 wordCount Dataflows算子链 为了更高效的分布执行,flink会尽可能地将operator的subtask链接(chain)在一起形成一个task.每一个task在一个线程中执行,将operators链接成task是非常有效的优化,它能减少线程之间的切换,减少消息的序列化和反序列化,减少数据在缓冲区的交换,减少了延迟的同时提高了整体的吞吐量 flink任务调度规则只需要记住两点: 不同task的subtask分到同一个taskslot,提高数据传输效率 相同task的subtask分到不同的taskslot,充分利用集群资源 flink并行度设置方式 在算子上设置(.setParallelism()方法) 在env上下文中设置 提交client时设置 配置文件中配置 以上优先度逐渐降低 Dataflows DataSource数据源HDFS,KAFKA,socker,collections,自定义数据源…… Dataflows Transformations将一个或者多个算子转化成一个新的数据流,处理复杂的业务场景 常见的算子例如map/flatmap/keyBy/reduce/aggregations/union/connect/comap/coflatmap/split/select/iterate… 效果基本跟spark差不多 Dataflows分区策略 shuffle 增大分区,提高并行度,解决数据倾斜(分区元素随机均分到下游分区,网络开销较大) rebalance 增大分区,提高并行度,解决数据倾斜(分区元素轮询分发到下游分区,每个分区比较均匀,数据倾斜时有奇效,网络开销大) rescale 减少分区,防止大量的网络传输,不发生全量的重新分区(轮询分区元素,将一个元素集合从上游发给下游,发送单位是集合,而不是一条条数据,本地数据传输) broadcast 使用映射表,并且映射表经常发生变动(每一个元素广播到下游的每一个分区中) global 并行度降为1(上游分区的数据只发给下游的第一个分区,我感觉应该没啥使用场景) forward 一对一的数据分发,map/flatmap/filter等都是这种分区策略(上游数据分发到下游对应的分区中) keyBy 与业务场景匹配(根据上游分区元素的hash值与下游分区数取模计算出将当前元素分发到下游哪一个分区中) partitionCustom 自定义分区策略 Dataflows Sink处理后的数据可以输出到HDFS,KAFKA,Redis,ES,mysql等等]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink流式计算学习01]]></title>
    <url>%2F2020%2F04%2F22%2Fflink%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A001%2F</url>
    <content type="text"><![CDATA[流式计算相关学习 概念及术语flink的世界观是数据流对于flink而言,其所要处理的主要场景就是流数据,批数据是流数据的一个特例,所以flink与spark不同,是真正的流批统一的计算引擎 什么是无界流/Unbounded streams?无界流 有定义流的开始,但是没有定义流的结束,它们会永无休止的产生数据. 无界流的数据必须持续处理,即数据被社区后需要立刻处理,我们不能等到所有数据到达后再处理,任何时候输入都不会完成. 什么是有界流/Bounded streams?有界流 有定流的的开始,也有定义流的结束.有界流可以在摄取所有的数据后再进行计算,有界流的所有数据可以被排序,所以无需有序摄取,通常被称之为批处理. 什么是有状态的计算/stateful computations?有状态的计算 每次进行数据计算的时候基于之前数据的计算结果,也就是俗称的状态做计算,并且每次计算结果都会保存到存储介质中,计算关联上下文context. 基于有状态的计算不需要对历史数据进行重新计算,提高了计算效率. 无状态的计算 每次进行数据计算只考虑当前的数据,不会使用之前的数据的计算结果. 安装＆部署 基本架构flink系统中包含了两个角色,分别是jobManager和taskManager,是典型的master-slave架构. jobManager相当于是master,taskManager相当于是slave jobManager负责整个集群的资源管理和任务管理,在一个集群中只能有一个正在工作(active)的jm.如果是HA集群,那么另一个jm一定是standby状态. 资源调度 集群启动,tm会将当前节点的资源信息注册给jm,所有tm全部注册完毕,集群启动成功,此时jm就掌握了整个集群的资源情况 client提交application给jm后,jm会根据集群中的资源情况,为当前的application分配taskslot资源(也就是槽位,可以理解为spark的executor,在yarn集群中就是container) 任务调度 根据各个tm节点上的资源,分发task到taskslot中运行 job执行过程中,jm会根据设置的触发策略触发checkpoint,通知tm开始checkpoint 任务执行完毕,jm会将job执行信息反馈给client,并且释放tm资源 taskManager 负责当前节点上的任务运行及当前节点上的资源管理.tm资源通过taskslot进行了划分,每个taskslot代表了一份固定的资源.例如,具有三个slot的tm会将其管理的内存资源划成三份给每个slot.划分资源意味着subtask之间不会竞争内存资源,但是也意味着它们只拥有自己的固定资源,也就是俗称的内存隔离.这里并没有cpu隔离,只是划分了内存资源. 负责tm之间的数据交换 client客户端负责将当前的任务提交给jm,并获取任务的执行信息. 提交的常用方式有以下几种: 命令提交 web页面提交 standalone集群安装＆测试wget https://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.9.3/apache-flink-1.9.3.tar.gz tar -zxvf apache-flink-1.9.3.tar.gz https://mirrors.tuna.tsinghua.edu.cn/apache/flink/ 自己选吧,总有一款适合你 修改配置文件 vim flink-conf.yaml jobmanager.rpc.address #jm地址 jobmanager.rpc.port #jm端口号 jobmanager.heap.size #jm所能使用的堆内存大小 taskmanager.heap.size #tm所能使用的堆内存大小 taskmanager.numberOfTaskSlots #tm管理的slots个数,一般情况下一个cpu核心对应一个slots,用lscpu指令看你自己的机器 rest.port #web的端口号 修改slaves配置文件,配上所有的子机名就行 分发包到其他节点 主节点环境变量 vim /etc/profile export FLINK_HOME=/opt/flink-1.9.3 export PATH=$FLINK_HOME/bin source /etc/profile 启动脚本 ./start-cluster.sh ./stop-cluster.sh 查看webUI,如果操作无误那么此时webUI会正常显示 提交任务到集群 命令提交 flink run -c #主类 -d #后台运行 -p #指定并行度 web页面提交 注意:web.submit.enable:true一定要开启,否则不支持web提交(人性化啊兄弟,有人手贱去关这个?) 启动scala-shell测试 start-scala-shell.sh remote &lt;hostname&gt; &lt;portnumber&gt; flink on yarn 如果你看不懂,你可能需要补一下yarn的资源申请流程 每当创建一个flink的yarn session的时候,客户端会先检查请求的资源(container和memory是否可用),然后将所有包含flink的相关jar包和配置上传到hdfs 客户端会向resourcemanager申请一个yarn的container用来启动applicationmaster.由于客户端已经将配置和jar都上传到了hdfs,applicationmaster会下载这些jar和配置,然后启动application jm和am运行于一个container(jobmanager等于applicationmaster) am申请启动taskmanager,rm返回的这些container会从hdfs上面下载jar和配置. 两种运行模式 每次启动的时候都重新启动一个application 优点:无需长服务,yarn资源相对清闲,可以抽出资源去跑其他任务,比如spark 缺点:每次都重新申请资源,速度慢 启动一个application长服务,每次运行的时候只跑任务 优点:长服务,每次启动任务无需重新申请资源,直接跑任务就可以 缺点:yarn抽出了大部分资源给flink,没有其他过多资源给别的任务 需要下载一个hadoop插件并放到flink/lib目录下 wget https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-10.0/flink-shaded-hadoop-2-uber-2.7.5-10.0.jar https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/ 总有一款适合你 启动命令#启动 yarn-session.sh -n 3 -s 3 -nm flink-session -d -q -n,--container 在yarn中启动的container数量,实际就是tm的数量 -s,--slots 每个tm管理的slot的个数 -nm,--name 给application起个名字 -d,--detached 后台启动 -tm,--taskManagerMemory tm的内存大小,单位mb -jm,--jobManagerMemory jm的内存大小,单位mb -q,--query 显示yarn集群可用资源 #关闭 yarn application -kill applicationId #样例 flink run -c com.mio.wordCount -yid 1111 ~/StudyFlink-1.0-SNAPSHOT.jar -yid #可以指定application的ID,不用也行无所谓,在tmp目录下你可以看到一个隐藏的小文件,其中写了这个id vim /tmp/.yarn-properties-root flink run -m yarn-cluster -yn 3 -ys 3 -ynm flink-job -c com.mio.wordCount ~/StudyFlink-1.0-SNAPSHOT.jar -yn,--container #表示分配的container数量 -d,--detached #设置在后台运行 -yjm,--jobManagerMemory -ytm,--taskManaagerMemory -ynm,--name -yq,--query -yqu,--queue #指定yarn资源队列 -ys,--slots HA集群模式 修改yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;The maximum number of application master execution attempts AppMaster,最大重试次数&lt;/description&gt; &lt;/property&gt; 修改flink-conf.yaml high-availability: zookeeper high-availability.storageDir: hdfs://node01:9000/flink/ha/ high-availability.zookeeper.quorum: node01:2181,node02:2181,node03:2181 下一期开始api实操]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka]]></title>
    <url>%2F2020%2F03%2F24%2Fkafka%2F</url>
    <content type="text"><![CDATA[消息队列-中间件 通常使用消息队列进行缓冲,系统间解耦和削峰填谷等业务场景,常见的消息队列工作模式大致会分为两大类: 至多一次:消息生产者将数据写入消息系统,然后由消费者去拉消息服务器中的消息,一旦消息被确认消费之后,由消息服务器主动删除队列中的数据,这种消费方式一般只允许被一个消费者消费,并且消息队列中的数据不允许被重复消费 没有限制:同上面消费形式不同,生产者发送完数据之后,该消息可以被多个消费者同时消费,并且同一个消费者可以多次消费消息服务器中的同一个记录.主要是因为消息服务器一般可以长时间存储海量消息 kafka集群以topic形式负责分类集群中的record,每一个record属于一个topic.每个topic底层都会对应一组分区的日志用于持久化topic中的record.同时在kafka集群中,topic的每一个日志的分区都一定会有一个borker担当该分区的leader,其他的broker担当该分区的follower,leader负责分区数据的独写操作,follower负责同步的数据.这样如果分区的leader宕机,该分区的其他follower会选取出新的leader继续负责该分区数据的读写.其中集群的leader的监控和topic的部分元数据是存储在zookeeper中 举例: 一个topic对应三个分区,每一个分区对应一台主机上的kafka实例,每一个分区存在一个broker来担当leader,其他的broker来担当follower,leader负责独写,follower负责同步,每一个分区数据还有一个备份数据存在于其他机器上 未完待续……]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2020%2F03%2F24%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker练习 之前我认为docker其实并没有什么用,软件还是手动部署来得实在 现在我只能说,没有人能逃得过真香定论 安装dockerstep 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2 Step 2: 添加软件源信息sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo Step 3: 更新并安装Docker-CEsudo yum makecache fast sudo yum -y install docker-ce Step 4: 开启Docker服务sudo service docker start 注意：官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，您可以通过以下方式开启。同理可以开启各种测试版本等。 vim /etc/yum.repos.d/docker-ee.repo 将[docker-ce-test]下方的enabled=0修改为enabled=1 安装指定版本的Docker-CE:Step 1: 查找Docker-CE的版本:yum list docker-ce.x86_64 --showduplicates | sort -r Loading mirror speeds from cached hostfileLoaded plugins: branch, fastestmirror, langpacksdocker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stabledocker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stableAvailable Packages Step2: 安装指定版本的Docker-CE:(VERSION例如上面的17.03.0.ce.1-1.el7.centos) sudo yum -y install docker-ce-[VERSION] docker pull hello-world docker run hello-world 指定版本,默认最新版docker pull tomcat &lt;:tags&gt; 查看本地镜像docker images 创建容器,启动应用docker run 镜像名 &lt;:tags&gt; 查看正在运行中的镜像docker ps 删除,参数代表强制删除docker rm &lt;-f&gt; 容器id docker的中央仓库,存放着所有镜像 www.hub.docker.com 容器:程序docker run -p 8000:8080 进入容器docker exec -it 容器id /bin/bash 埋坑点 docker安装的tomcat默认的webapps目录为空,需要进入容器删除这个目录并把webapps.dist目录替换成webapps 关于构建镜像docker build -t &lt;组织机构名&gt;/&lt;镜像名&gt;:&lt;版本号&gt; 路径 RUN 构建镜像时运行 CMD 容器启动时执行(默认命令,不一定必须为完整指令,也可以为指令的一部分;如 docker run centos ls,表示构建镜像时直接执行命令,此时dockerfile中如果存在cmd命令,则文件中的命令失效) ENTRYPOINT 容器启动时一定会执行,如果存在多行,则只有最后一行会被执行;如果和CMD组合使用,会产生很多变化,见如下案例 dockerfile文件FROM centos RUN [&quot;echo&quot;,&quot;ookamimio!&quot;] ENTRYPOINT [&quot;ps&quot;] CMD [&quot;-ef&quot;] 此时会联合使用,相当于ps -ef,联合使用的优点,可以随时替换参数,如使用ENTRYPOINT指定命令,CMD更换参数 docker run ookamimio -aux 对外暴露端口EXPOSE &lt;端口号&gt; 关于挂载镜像相当于将tomcat镜像中的文件指向到本地路径,以此做到多个tomcat指向同一个本地路径 docker run --name t1 -p 8000:8080 -d -v /usr/webapps:/usr/local/tomcat/webapps tomcat 配置略显麻烦,产生共享容器概念(true为占位符,无实际意义) docker create --name webpage -v /usr/webapps:/usr/local/tomcat/webapps tomcat /bin/true docker run -p 8002:8080 --volumes-from webpage --name -t3 -d tomcat docker compose,官方提供的容器编排工具 sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose -version 安装mysql 5.7docker run -d -p 3306:3306 --name mysql -v $PWD/mysql5.7/conf:/etc/mysql/mysql.conf.d -v $PWD/mysql5.7/logs:/logs -v $PWD/mysql5.7/data:/mysql_data -e MYSQL_ROOT_PASSWORD=xiaoguaishouxiaowu mysql:5.7 docker exec -it 62349aa31687 /bin/bash mysql -uroot -p mysql&gt; GRANT ALL ON *.* TO &#39;root&#39;@&#39;%&#39;; mysql&gt; flush privileges; mysql&gt; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39; PASSWORD EXPIRE NEVER; mysql&gt; ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; mysql&gt; flush privileges; 安装oracle11gdocker run -d -p 1521:1521 --name oracle11g -e ORACLE_PWD=xiaoguaishouxiaowu registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g 下次抽时间学习k8s]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查询优化]]></title>
    <url>%2F2019%2F12%2F20%2FMysql%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Mysql查询优化查询缓慢的原因主要有以下几点: 网络 CPU IO 上下文切换 系统调用 生成统计信息 锁等待时间(M:共享读锁,独占写锁,锁数据块;I:共享锁,排他锁,锁索引) 优化数据访问查询性能低下的主要原因是访问的数据量太多,某些查询不可避免的需要筛选大量的数据,我们可以通过减少访问数量的方式进行优化 确认应用程序是否在检索大量超过需要的数据 确认mysql服务器层是否在分析大量超过需要的数据行 是否向数据库请求了不需要的数据 查询不需要的记录 我们常常会误以为mysql会只返回需要的数据,实际上mysql却是先返回全部结果再进行计算,在日常的开发习惯中,经常是先用select语句查询大量的结果,然后获取前面的N行后关闭结果集. 优化方式是在查询后面添加limit 多表关联时返回全部列 总是取出全部列 在公司的企业需求中,禁止使用select *,虽然这种方式能够简化开发,但是会影响查询的性能,所以尽量不要使用 重复查询相同的数据(在mysql8中已经被废弃) 如果需要不断的重复执行相同的查询,且每次返回完全相同的数据,因此,基于这样的应用场景,我们可以将这部分数据缓存起来,这样的话能够提高查询效率 执行过程的优化查询缓存在解析一个查询语句之前,如果查询缓存是打开的,那么mysql会优先检查这个查询是否命中查询缓存中的数据,如果查询恰好命中了查询缓存,那么会在返回结果之前会检查用户权限,如果权限没有问题,那么mysql会跳过所有的阶段,就直接从缓存中拿到结果并返回给客户端 查询优化处理mysql查询完缓存之后会经过以下几个步骤：解析SQL、预处理、优化SQL执行计划,这几个步骤出现任何的错误,都可能会终止查询 语法解析器和预处理mysql通过关键字将SQL语句进行解析,并生成一颗解析树(抽象语法树),mysql解析器将使用mysql语法规则验证和解析查询,例如验证使用使用了错误的关键字或者顺序是否正确等等,预处理器会进一步检查解析树是否合法,例如表名和列名是否存在,是否有歧义,还会验证权限等等 查询优化器当语法树没有问题之后,相应的要由优化器将其转成执行计划,一条查询语句可以使用非常多的执行方式,最后都可以得到对应的结果,但是不同的执行方式带来的效率是不同的,优化器的最主要目的就是要选择最有效的执行计划 mysql使用的是基于成本的优化器,在优化的时候会尝试预测一个查询使用某种查询计划时候的成本,并选择其中成本最小的一个 查询最后一次查询的信息信息show status like &#39;last_query_cost&#39;; 其中的含义分别为 每个表或者索引的页面个数 索引的基数 索引和数据行的长度 索引的分布情况 在很多情况下mysql会选择错误的执行计划 索引信息不准确 InnoDB因为其mvcc的架构,并不能维护一个数据表的行数的精确统计信息 执行计划的成本估算不等同于实际执行的成本 有时候某个执行计划虽然需要读取更多的页面,但是他的成本却更小,因为如果这些页面都是顺序读或者这些页面都已经在内存中的话,那么它的访问成本将很小,mysql层面并不知道哪些页面在内存中,哪些在磁盘,所以查询之际执行过程中到底需要多少次IO是无法得知的 mysql的最优可能跟你想的不一样 mysql的优化是基于成本模型的优化,但是有可能不是最快的优化 mysql不考虑其他并发执行的查询 mysql不会考虑不受其控制的操作成本 执行存储过程或者用户自定义函数的成本 优化器的优化策略 静态优化直接对解析树进行分析,并完成优化 动态优化动态优化与查询的上下文有关,也可能跟取值,索引对应的行数有关 mysql对查询的静态优化只需要一次,但是对动态优化在每次执行时都需要重新评估 优化器的优化类型 重新定义关联表的顺序 数据表的关联并不总是按照在查询中指定的顺序进行,决定关联顺序时优化器很重要的功能 将外连接转化成内连接,内连接的效率要高于外连接 使用等价变换规则,mysql可以使用一些等价变化来简化并规划表达式 优化count(),min(),max() 索引和列是否可以为空通常可以帮助mysql优化这类表达式：例如,要找到某一列的最小值,只需要查询索引的最左端的记录即可,不需要全文扫描比较 预估并转化为常数表达式,当mysql检测到一个表达式可以转化为常数的时候,就会一直把该表达式作为常数进行处理 explain select film.film_id,film_actor.actor_id from film inner join film_actor using(film_id) where film.film_id = 1 索引覆盖扫描,当索引中的列包含所有查询中需要使用的列的时候,可以使用覆盖索引 子查询优化 mysql在某些情况下可以将子查询转换一种效率更高的形式,从而减少多个查询多次对数据进行访问,例如将经常查询的数据放入到缓存中 等值传播 如果两个列的值通过等式关联,那么mysql能够把其中一个列的where条件传递到另一个上： explain select film.film_id from film inner join film_actor using(film_id) where film.film_id &gt; 500; 这里使用film_id字段进行等值关联,film_id这个列不仅适用于film表而且适用于film_actor表 explain select film.film_id from film inner join film_actor using(film_id) where film.film_id &gt; 500 and film_actor.film_id &gt; 500; 关联查询join的实现方式原理 Simple Nested-Loop Join Index Nested-Loop Join Block Nested-Loop Join Join Buffer会缓存所有参与查询的列而不是只有Join的列. 可以通过调整join_buffer_size缓存大小 join_buffer_size的默认值是256K,join_buffer_size的最大值在MySQL 5.1.22版本前是4G-1,而之后的版本才能在64位操作系统下申请大于4G的Join Buffer空间. 使用Block Nested-Loop Join算法需要开启优化器管理配置的optimizer_switch的设置block_nested_loop为on,默认为开启. show variables like &#39;%optimizer_switch%&#39;; show variables like &#39;%join_buffer%;&#39; 案例演示查看不同的顺序执行方式对查询性能的影响： explain select film.film_id,film.title,film.release_year,actor.actor_id,actor.first_name,actor.last_name from film inner join film_actor using(film_id) inner join actor using(actor_id); 查看执行的成本： show status like &#39;last_query_cost&#39;; 按照自己预想的规定顺序执行： explain select straight_join film.film_id,film.title,film.release_year,actor.actor_id,actor.first_name,actor.last_name from film inner join film_actor using(film_id) inner join actor using(actor_id); 查看执行的成本： show status like &#39;last_query_cost&#39;; 排序优化排序的算法 两次传输排序 第一次数据读取是将需要排序的字段读取出来,然后进行排序,第二次是将排好序的结果按照需要去读取数据行. 这种方式效率比较低,原因是第二次读取数据的时候因为已经排好序,需要去读取所有记录而此时更多的是随机IO,读取数据成本会比较高 两次传输的优势,在排序的时候存储尽可能少的数据,让排序缓冲区可以尽可能多的容纳行数来进行排序操作 单次传输排序 先读取查询所需要的所有列,然后再根据给定列进行排序,最后直接返回排序结果,此方式只需要一次顺序IO读取所有的数据,而无须任何的随机IO,问题在于查询的列特别多的时候,会占用大量的存储空间,无法存储大量的数据 当需要排序的列的总大小加上orderby的列大小超过max_length_for_sort_data定义的字节,mysql会选择双次排序,反之使用单次排序,当然,用户可以设置此参数的值来选择排序的方式]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引总结]]></title>
    <url>%2F2019%2F12%2F19%2FMysql%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Mysql索引总结需要掌握的前提索引是帮助 MySQL 高效获取数据的数据结构 索引存储在文件系统中 索引的文件存储形式与存储引擎有关 索引文件的结构 hash 二叉树 B树 B+树 索引的分类mysql索引的五种类型:主键索引,唯一索引,普通索引,全文索引和组合索引.通过给字段添加索引 可以提高数据的读取速度,提高项目的并发能力和抗压能力. 主键索引:主键是一种唯一性索引,但它必须指定为PRIMARY KEY,每个表只能有一个主键. 唯一索引:索引列的所有值都只能出现一次,即必须唯一,值可以为空. 普通索引:基本的索引类型,值可以为空,没有唯一性的限制.(覆盖索引) 全文索引,MyISAM支持,Innodb在5.6之后支持:全文索引的索引类型为FULLTEXT.全文索引可以在varchar,char,text类型的列上创建 组合索引:多列值组成一个索引,专门用于组合搜索(最左匹配原则) 存储引擎对比 在mysql命令行查看当前使用的存储引擎 show variables like &#39;storage_engine&#39;; --5版本使用此命令 show engines; --8版本以上使用此命令 可以发现默认的存储引擎是InnoDB,当然,5.1默认是MyISAM(没事别瞎看红框,出问题我可不负责) 面试灾区回表先定位主键值,再定位行记录,它的性能较扫一遍索引树更低.样例sql: select * from hololive where name = &#39;minatoaqua&#39;; 覆盖索引包含查询所检索的所有列的索引.该查询不使用索引值作为查找完整表行的指针,而是从索引结构返回值,从而节省了磁盘I/O. InnoDB可以将这种优化技术应用于比MyISAM更多的索引,因为InnoDB辅助索引还包括主键列.InnoDB在事务结束之前,不能将这种技术应用于对事务修改的表的查询.样例sql: select id,name from hololive where name = &#39;minatoaqua&#39;; 最左匹配多列索引最左优先,以最左边的为起点任何连续的索引都能匹配上.同时遇到范围查询(&gt;,&lt;,between,like)就会停止匹配. 样例sql: select * from hololive where name = &#39;minatoaqua&#39; and age = 14; -- explain type:simple,符合最左匹配 select * from hololive where age = 14; -- explain type:index,不符合最左匹配 索引下推索引下推只适用于二级索引列,一般查询场景用于所查询的字段(select列)不全是联合索引的字段,查询条件为多条件并且查询子句(where)字段全是联合索引.在联合索引中,有两种执行的可能性. 第一种:联合索引查询所有包含”aqua”的索引,然后回表查询出相应的全行数据,再筛选出age&lt;14的用户数据 第二种:联合索引查询所有包含”aqua”的索引,然后直接筛选出age&lt;14的所有索引,之后再回表查询全行数据 样例sql: select * from hololive where like = &#39;%aqua&#39; and age &lt; 14; -- 默认开启索引下推,age和name为联合索引 索引维护索引在插入新的值的时候,为了维护索引的有序性,必须要维护, 在维护索引的时候需要需要分以下集中情况: 如果插入一个比较大的值,直接插入即可,几乎没有成本 如果插入的是中间的某一个值,需要逻辑上移动后续的元素,空出位置 如果需要插入的数据页满了,就需要单独申请一个新的数据页,然后移动部分数据过去,叫做页分裂,此时性能会受影响同时空间的使用率也会降低,除了页分裂之外还包含页合并 尽量使用自增主键作为索引 组合索引下图为组合索引的查询参考示例图:其中a,b,c三列为组合索引 聚簇索引与非聚簇索引聚簇索引不是单独的索引类型,而是一种数据存储方式,指的是数据行跟相邻的键值紧凑的存储在一起 优点: 可以把相关数据保存在一起 数据访问更快,因为索引和数据保存在同一个树中 使用覆盖索引扫描的查询可以直接使用页节点中的主键值 缺点: 聚簇数据最大限度地提高了IO密集型应用的性能,如果数据全部在内存,那么聚簇索引就没有什么优势 插入速度严重依赖于插入排序,按照主键的顺序插入是最快的方式 更新聚簇索引列的代价很高,因为会强制将每个被更新的行移动到新的位置 基于聚簇索引的表在插入新行,或者主键被更新导致需要移动行的时候,可能面临页分裂的问题 聚簇索引可能导致全表扫描变慢,尤其是行比较稀疏,或者由于页分裂导致数据存储不连续的时候 非聚簇索引 数据文件跟索引文件分开存放 其他关于索引的一些意见 使用索引列进行查询的时候不要使用表达式,把计算放到业务层而不是数据库层 尽量使用主键查询,而不是其他索引,因为主键查询不会触发回表查询 union all,in,or都能使用索引,但是推荐使用in 范围列可以用到索引,如&lt;,&lt;=,&gt;,&gt;=,between,但是范围列后面的列无法使用索引,索引最多用于一个范围列 强制类型转换会导致全表扫描 更新频繁的字段上不宜建立索引 创建索引的列,不允许为null,可能会得到不符合预期的结果 当需要进行表连接的时候,不要超过三张表 尽量使用limit 单表索引数量控制在5个以内 索引并非越多越好 在不了解系统的情况下进行优化 索引监控show status like &#39;Handler_read%&#39;; 参数解释 Handler_read_first:读取索引第一个条目的次数 Handler_read_key:通过index获取数据的次数 Handler_read_last:读取索引最后一个条目的次数 Handler_read_next:通过索引读取下一条数据的次数 Handler_read_prev:通过索引读取上一条数据的次数 Handler_read_rnd:从固定位置读取数据的次数 Handler_read_rnd_next:从数据节点读取下一条数据的次数]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http协议学习]]></title>
    <url>%2F2019%2F12%2F19%2FHttp%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Http协议学习什么是http超文本传输协议(Hyper Text Transfer Protocol),规范了浏览器和服务器的数据交互. HTTP是应用层协议,同其他应用层协议一样,是为了实现某一类具体应用的协议,并由某一运行在用户空间的应用程序来实现其功能.HTTP是一种协议规范,这种规范记录在文档上,为真正通过HTTP协议进行通信的HTTP的实现程序. HTTP协议是基于C/S架构进行通信的,而HTTP协议的服务器端实现程序有httpd,nginx等,其客户端的实现程序主要是Web浏览器,例如Firefox,InternetExplorer,Google chrome,Safari,Opera等,此外,客户端的命令行工具还有elink,crul等.Web服务是基于TCP的,因此为了能够随时响应客户端的请求,Web服务器需要监听在80/TCP端口,客户端浏览器和Web服务器之间就可以通过HTTP协议进行通信了. 特点简单快速灵活无连接 限制每次连接只处理一个请求.服务器处理完客户的请求,并收到客户的应答后,即断开连接.采用这种方式可以节省传输时间. 早期这么做的原因是 HTTP 协议产生于互联网,因此服务器需要处理同时面向全世界数十万,上百万客户端的网页访问,但每个客户端(即浏览器)与服务器之间交换数据的间歇性较大(即传输具有突发性,瞬时性),并且网页浏览的联想性,发散性导致两次传送的数据关联性很低,大部分通道实际上会很空闲,无端占用资源.因此 HTTP 的设计者有意利用这种特点将协议设计为请求时建连接,请求完释放连接,以尽快将资源释放出来服务其他客户端. 随着时间的推移,网页变得越来越复杂,里面可能嵌入了很多图片,这时候每次访问图片都需要建立一次 TCP 连接就显得很低效.后来,Keep-Alive 被提出用来解决这效率低的问题. Keep-Alive 功能使客户端到服务器端的连接持续有效,当出现对服务器的后继请求时,Keep-Alive 功能避免了建立或者重新建立连接.市场上的大部分 Web 服务器,包括 iPlanet,IIS 和 Apache,都支持 HTTP Keep-Alive.对于提供静态内容的网站来说,这个功能通常很有用.但是,对于负担较重的网站来说,这里存在另外一个问题:虽然为客户保留打开的连接有一定的好处,但它同样影响了性能,因为在处理暂停期间,本来可以释放的资源仍旧被占用.当Web服务器和应用服务器在同一台机器上运行时,Keep-Alive 功能对资源利用的影响尤其突出. 这样一来,客户端和服务器之间的 HTTP 连接就会被保持,不会断开(超过 Keep-Alive 规定的时间,意外断电等情况除外),当客户端发送另外一个请求时,就使用这条已经建立的连接. 无状态无状态是指协议对于事务处理没有记忆能力,服务器不知道客户端是什么状态.即我们给服务器发送 HTTP 请求之后,服务器根据请求,会给我们发送数据过来,但是,发送完,不会记录任何信息. HTTP 是一个无状态协议,这意味着每个请求都是独立的,Keep-Alive 没能改变这个结果. 缺少状态意味着如果后续处理需要前面的信息,则它必须重传,这样可能导致每次连接传送的数据量增大.另一方面,在服务器不需要先前信息时它的应答就较快. HTTP 协议这种特性有优点也有缺点,优点在于解放了服务器,每一次请求”点到为止”不会造成不必要连接占用,缺点在于每次请求会传输大量重复的内容信息. 客户端与服务器进行动态交互的 Web 应用程序出现之后,HTTP 无状态的特性严重阻碍了这些应用程序的实现,毕竟交互是需要承前启后的,简单的购物车程序也要知道用户到底在之前选择了什么商品.于是,两种用于保持 HTTP 连接状态的技术就应运而生了,一个是 Cookie,而另一个则是 Session. Cookie可以保持登录信息到用户下次与服务器的会话,换句话说,下次访问同一网站时,用户会发现不必输入用户名和密码就已经登录了(当然,不排除用户手工删除Cookie).而还有一些Cookie在用户退出会话的时候就被删除了,这样可以有效保护个人隐私. Cookies 最典型的应用是判定注册用户是否已经登录网站,用户可能会得到提示,是否在下一次进入此网站时保留用户信息以便简化登录手续,这些都是 Cookies 的功用.另一个重要应用场合是”购物车”之类处理.用户可能会在一段时间内在同一家网站的不同页面中选择不同的商品,这些信息都会写入 Cookies,以便在最后付款时提取信息. 与 Cookie 相对的一个解决方案是 Session,它是通过服务器来保持状态的. 当客户端访问服务器时,服务器根据需求设置 Session,将会话信息保存在服务器上,同时将标示 Session 的 SessionId 传递给客户端浏览器,浏览器将这个 SessionId 保存在内存中,我们称之为无过期时间的 Cookie.浏览器关闭后,这个 Cookie 就会被清掉,它不会存在于用户的 Cookie 临时文件. 以后浏览器每次请求都会额外加上这个参数值,服务器会根据这个 SessionId,就能取得客户端的数据信息. 如果客户端浏览器意外关闭,服务器保存的 Session 数据不是立即释放,此时数据还会存在,只要我们知道那个 SessionId,就可以继续通过请求获得此 Session 的信息,因为此时后台的 Session 还存在,当然我们可以设置一个 Session 超时时间,一旦超过规定时间没有客户端请求时,服务器就会清除对应 SessionId 的 Session 信息. 支持B/S和C/S架构Brower/Server,即(浏览器/服务器) Client/Server即(客户端/服务器) HTTP协议的请求格式 抓包的request结构如下: GET /root/aqua.html?name=minatoaqua&amp;password=123456 HTTP/1.1 Host: www.secondaries.cn Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding: gzip, deflate, sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 name=aqua 请求行GET为请求方法,/root/aqua.html?name=minatoaqua&amp;password=123456为要访问的资源,HTTP/1.1是协议版本. 请求方法分为 GET(请求获取由Request-URI所标识的资源) POST(在Request-RUI所标识的资源后附加新的数据) HEAD(请求获取由Request-URI所标识的资源的响应消息报头) DELETE(请求服务器删除由Reqest-URI所标识的资源) TRACE(请求服务器会送收到的请求信息,用于测试或诊断) CONNECT(保留将来使用) OPTIONS(请求查询服务器的性能,或者查询与资源相关的选项和需求) PUT(请求服务器存储一个资源,并用Request-URI作为其标识) 请求头部从第二行起为请求头部,Host指出请求的目的地（主机域名）;User-Agent是客户端的信息,它是检测浏览器类型的重要信息,由浏览器定义,并且在每个请求中自动发送. 空行请求头后面必须有一个空行 请求数据请求的数据也叫请求体,可以添加任意的其它数据.这个例子的请求体为name=aqua. get和post方法的区别(面试灾区) 1.get请求参数是直接显示在地址栏的,而post在地址栏不显示 GET请求会把请求的参数拼接在URL后面,以?分隔,多个参数之间用&amp;连接;如果是英文或数字,原样发送,如果是空格或中文,则用Base64编码 POST请求会把提交的数据放在请求体中,不会在URL中显示出来 2.get方式不安全,post安全 GET: 请求参数在URL后面,可以直接看到,尤其是登录时,如果登录界面被浏览器缓存,其他人就可以通过查看历史记录,拿到账户和密码 POST: 请求参数在请求体里面传输,无法直接拿到,相对GET安全性较高;但是通过抓包工具,还是可以看到请求参数的 3.get请求参数是又长度限制的,post没有限制 GET: 浏览器和服务器会限制URL的长度,所以传输的数据有限,一般是2K POST: 由于数据不是通过URL传递,所以一般可以传输较大量的数据 4.数据解析 GET: 通过Request.QueryString获取变量的值 POST: 通过Request.form获取变量的值 HTTP协议的响应格式一般情况下,服务器收到客户端的请求后,就会有一个HTTP的响应消息,HTTP响应也由4部分组成,分别是:状态行,响应头,空行和响应体. 抓包的数据如下: HTTP/1.1 200 OK Server: nginx Date: Thu, 19 Dec 2019 09:13:59 GMT Content-Lenth: 4393 Content-Type: application/json;charset=UTF-8 Pragrma: no-cache Expires: Thu, 19 Dec 2019 07:33:04 GMT { &quot;Name&quot;:&quot;aqua&quot; &quot;id&quot;:&quot;push-code&quot; } 状态行 状态行由协议版本号,状态码,状态消息组成 响应头 响应头是客户端可以使用的一些信息,如:Date（生成响应的日期）,Content-Type（MIME类型及编码格式）,Connection（默认是长连接）等等 空行 响应头和响应体之间必须有一个空行 响应体 响应正文,本例中是键值对信息 状态码HTTP协议的状态码由3位数字组成,第一个数字定义了响应的类别,共有5中类别: 1xx: 信息,服务器收到请求,需要请求者继续执行操作 2xx: 成功,操作被成功接受并处理 3xx: 重定向,需要进一步的操作以完成请求 4xx: 客户端错误,请求包含语法错误或无法完成请求 5xx: 服务器操作,服务器在处理请求的过程中发生了错误 其中,常用的状态码如下 200 OK //客户端请求成功 400 Bad Request //客户端请求有语法错误,不能被服务器所理解 401 Unauthorized //请求未经授权,这个状态代码必须和WWW-Authenticate 报头域一起使用 403 Forbidden //服务器收到请求,但是拒绝提供服务 404 Not Found //请求资源不存在,eg：输入了错误的 URL 500 Internal Server Error //服务器发生不可预期的错误 503 Server Unavailable //服务器当前不能处理客户端的请求,一段时间后可能恢复正常]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDBC简明教程]]></title>
    <url>%2F2019%2F11%2F12%2FJDBC%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[简单的JDBC教程首先去到这个路径,把Oracle的JDBC包搞出来 打开IDE,把这个包加到依赖里面 点开依赖,看到这个就代表成功了 下面是一个最基础的JDBC链接代码 /** * please @mio * @Author OkamiMio * @Date: 2019/11/11 18:10 */ public class YozoraMel { public static void main(String[] args) throws Exception { // Class.forName(String className)的作用有两个，第一是CLASSPATH下指定名字的.class文件加载到Java虚拟机内存中， 第二是初始化这个类 // 点开OracleDriver这个类看,详情请见下个代码块 Class.forName(&quot;oracle.jdbc.driver.OracleDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:oracle:thin:@192.168.3.71:1521:orcl&quot;, &quot;scott&quot;, &quot;12345678&quot;); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;select * from tab&quot;); while (resultSet.next()) { System.out.println(resultSet.getString(1) + &quot;,&quot; + resultSet.getString(2)); } resultSet.close(); statement.close(); connection.close(); } } 点开OracleDriver类,可以发现如下静态代码块,这就是为什么需要Class.forName的原因,它需要对一些静态属性和静态代码块做一些初始化和赋值操作 但是会有同学问为什么不需要new出来,因为这个Driver中我们并没有调用任何方法,我们需要做的是拿到Connection来调用Driver中的各种方法去连接数据库,开发者已经帮我们做好了,至于实际的实现无需关心 static { try { if (defaultDriver == null) { defaultDriver = new oracle.jdbc.OracleDriver(); DriverManager.registerDriver(defaultDriver); } AccessController.doPrivileged(new PrivilegedAction() { public Object run() { OracleDriver.registerMBeans(); return null; } }); Timestamp var0 = Timestamp.valueOf(&quot;2000-01-01 00:00:00.0&quot;); } catch (SQLException var5) { Logger.getLogger(&quot;oracle.jdbc.driver&quot;).log(Level.SEVERE, &quot;SQLException in static block.&quot;, var5); } catch (RuntimeException var6) { Logger.getLogger(&quot;oracle.jdbc.driver&quot;).log(Level.SEVERE, &quot;RuntimeException in static block.&quot;, var6); } try { ClassRef var7 = ClassRef.newInstance(&quot;oracle.security.pki.OraclePKIProvider&quot;); Object var1 = var7.get().newInstance(); } catch (Throwable var4) { } systemTypeMap = new Hashtable(3); try { systemTypeMap.put(&quot;SYS.XMLTYPE&quot;, ClassRef.newInstance(&quot;oracle.xdb.XMLTypeFactory&quot;)); } catch (ClassNotFoundException var3) { } try { systemTypeMap.put(&quot;SYS.ANYDATA&quot;, ClassRef.newInstance(&quot;oracle.sql.AnyDataFactory&quot;)); systemTypeMap.put(&quot;SYS.ANYTYPE&quot;, ClassRef.newInstance(&quot;oracle.sql.TypeDescriptorFactory&quot;)); } catch (ClassNotFoundException var2) { } _Copyright_2007_Oracle_All_Rights_Reserved_ = null; } 可以发现,如果有很多连接的话,那么代码会变得非常臃肿.于是可以将这些公共的连接代码抽出来一个工具类,将来统一使用这个工具类进行连接 /** * please @mio * * @Author OkamiMio * @Date: 2019/11/11 18:30 */ public class DBUtil { public static final String URL = &quot;jdbc:oracle:thin:@192.168.3.71:1521:orcl&quot;; public static final String USERNAME = &quot;scott&quot;; public static final String PASSWORD = &quot;12345678&quot;; public static Connection getConnection() { try { return DriverManager.getConnection(URL, USERNAME, PASSWORD); } catch (SQLException e) { e.printStackTrace(); } return null; } public static void closeConnection(Connection connection) { if (connection != null) { try { connection.close(); } catch (SQLException e) { e.printStackTrace(); } } } public static void closeConnection(Connection connection, Statement statement) { if (connection != null &amp;&amp; statement != null) { try { statement.close(); connection.close(); } catch (SQLException e) { e.printStackTrace(); } } } public static void closeConnection(Connection connection, Statement statement, ResultSet resultSet) { if (connection != null &amp;&amp; statement != null &amp;&amp; resultSet != null) { try { resultSet.close(); statement.close(); connection.close(); } catch (SQLException e) { e.printStackTrace(); } } } } 创建和oracle表中对应的实体类,这里我居然卡在了toString方法上,使用快捷键ctrl+o生成的toString只能默认super,如果想使用idea帮你生成的toString请使用快捷键alt+insert里面的toString /** * please @mio * * @Author OkamiMio */ public class Emp { private Integer empno; private String ename; private String job; private Integer mrg; private String hiredate; private Double sal; private Double comm; private Integer deptno; @Override public String toString() { return &quot;Emp{&quot; + &quot;empno=&quot; + empno + &quot;, ename=&#39;&quot; + ename + &#39;\&#39;&#39; + &quot;, job=&#39;&quot; + job + &#39;\&#39;&#39; + &quot;, mrg=&quot; + mrg + &quot;, hiredate=&#39;&quot; + hiredate + &#39;\&#39;&#39; + &quot;, sal=&quot; + sal + &quot;, comm=&quot; + comm + &quot;, deptno=&quot; + deptno + &#39;}&#39;; } public Emp(Integer empno, String ename, String job, Integer mrg, String hiredate, Double sal, Double comm, Integer deptno) { this.empno = empno; this.ename = ename; this.job = job; this.mrg = mrg; this.hiredate = hiredate; this.sal = sal; this.comm = comm; this.deptno = deptno; } public Emp() { } public Integer getEmpno() { return empno; } public void setEmpno(Integer empno) { this.empno = empno; } public String getEname() { return ename; } public void setEname(String ename) { this.ename = ename; } public String getJob() { return job; } public void setJob(String job) { this.job = job; } public Integer getMrg() { return mrg; } public void setMrg(Integer mrg) { this.mrg = mrg; } public String getHiredate() { return hiredate; } public void setHiredate(String hiredate) { this.hiredate = hiredate; } public Double getSal() { return sal; } public void setSal(Double sal) { this.sal = sal; } public Double getComm() { return comm; } public void setComm(Double comm) { this.comm = comm; } public Integer getDeptno() { return deptno; } public void setDeptno(Integer deptno) { this.deptno = deptno; } } /** * please @mio * * @Author OkamiMio * @Date: 2019/11/12 10:15 */ public class Dept { private int deptno; private String dname; private String loc; public Dept() { } public Dept(int deptno, String dname, String loc) { this.deptno = deptno; this.dname = dname; this.loc = loc; } public int getDeptno() { return deptno; } public void setDeptno(int deptno) { this.deptno = deptno; } public String getDname() { return dname; } public void setDname(String dname) { this.dname = dname; } public String getLoc() { return loc; } public void setLoc(String loc) { this.loc = loc; } @Override public String toString() { return &quot;Dept{&quot; + &quot;deptno=&quot; + deptno + &quot;, dname=&#39;&quot; + dname + &#39;\&#39;&#39; + &quot;, loc=&#39;&quot; + loc + &#39;\&#39;&#39; + &#39;}&#39;; } } CRUD /** * 四大种类合起来就是传说中的CRUD接口 * @Author: OkamiMio * @Date: 2019/11/11 10:35 */ public interface EmpDao { /** * Create,俗称C * @param emp */ void insert(Emp emp); /** * Delete,俗称D * @param emp */ void delete(Emp emp); /** * Update,俗称U * @param emp */ void update(Emp emp); /** * Read,俗称R * @param empNo * @return */ Emp select(Integer empNo); /** * 防止SQL注入攻击 * @param name * @return */ Emp selectByName(String name); } 静态处理Statement /** * 传统的Statement实现,用于执行静态 SQL 语句并返回它所生成结果的对象 * please @mio * @author OkamiMio */ public class StatementEmpDaoImpl implements EmpDao { @Override public void insert(Emp emp) { Connection connection = DBUtil.getConnection(); Statement statement = null; try { statement = connection.createStatement(); statement.executeQuery(&quot;insert into emp values(&quot; + emp.getEmpno() + &quot;,&#39;&quot; + emp.getEname() + &quot;&#39;,&#39;&quot; + emp.getJob() + &quot;&#39;,&quot; + emp.getMrg() + &quot;,to_date(&#39;&quot; + emp.getHiredate() + &quot;&#39;,&#39;YYYY-MM-DD&#39;),&quot; + emp.getSal() + &quot;,&quot; + emp.getComm() + &quot;,&quot; + emp.getDeptno() + &quot;)&quot;); } catch (SQLException e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, statement); } } @Override public void delete(Emp emp) { Connection connection = DBUtil.getConnection(); Statement statement = null; try { statement = connection.createStatement(); int result = statement.executeUpdate(&quot;delete from EMP where EMPNO = &quot; + emp.getEmpno()); System.out.println(&quot;受影响的行数是&quot; + result); } catch (SQLException e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, statement); } } @Override public void update(Emp emp) { Connection connection = DBUtil.getConnection(); Statement statement = null; try { statement = connection.createStatement(); int result = statement.executeUpdate(&quot;update EMP set ENAME = &#39;&quot; + emp.getEname() + &quot;&#39; where EMPNO = &quot; + emp.getEmpno()); System.out.println(&quot;受影响的行数是&quot; + result); } catch (SQLException e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, statement); } } @Override public Emp select(Integer empNo) { Connection connection = DBUtil.getConnection(); Statement statement = null; ResultSet resultSet = null; Emp emp = null; try { connection.setAutoCommit(true); statement = connection.createStatement(); resultSet = statement.executeQuery(&quot;select * from EMP where EMPNO = &quot; + empNo); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); while (resultSet.next()) { emp = new Emp(resultSet.getInt(&quot;empno&quot;), resultSet.getString(&quot;ename&quot;), resultSet.getString(&quot;job&quot;), resultSet.getInt(&quot;mgr&quot;), simpleDateFormat.format(resultSet.getDate(&quot;hiredate&quot;)), resultSet.getDouble(&quot;sal&quot;), resultSet.getDouble(&quot;comm&quot;), resultSet.getInt(&quot;deptno&quot;)); } } catch (SQLException e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, statement, resultSet); } return emp; } @Override public Emp selectByName(String name) { Connection connection = DBUtil.getConnection(); Statement statement = null; ResultSet resultSet = null; Emp emp = null; SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); try { statement = connection.createStatement(); String sql = &quot;select * from EMP where ENAME = &quot; + name; System.out.println(sql); resultSet = statement.executeQuery(sql); while (resultSet.next()) { emp = new Emp(resultSet.getInt(&quot;empno&quot;), resultSet.getString(&quot;ename&quot;), resultSet.getString(&quot;job&quot;), resultSet.getInt(&quot;mgr&quot;), simpleDateFormat.format(resultSet.getDate(&quot;hiredate&quot;)), resultSet.getDouble(&quot;sal&quot;), resultSet.getDouble(&quot;comm&quot;), resultSet.getInt(&quot;deptno&quot;)); } } catch (SQLException e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, statement, resultSet); } return emp; } public static void main(String[] args) { EmpDao empDao = new StatementEmpDaoImpl(); // Emp mio = new Emp(1, &quot;okamimio&quot;, &quot;vtuber&quot;, 1, &quot;2018-08-20&quot;, 1000.00, 1000.00, 1); // empDao.insert(emp); // empDao.delete(emp); // Emp aqua = new Emp(1, &quot;minatoaqua&quot;, &quot;vtuber&quot;, 1, &quot;2018-08-20&quot;, 1000.00, 1000.00, 1); // empDao.insert(mio); // empDao.update(aqua); // System.out.println(empDao.select(1)); // SQL注入攻击场景复现,由此产生PreparedStatement来避免这一现象产生 System.out.println(empDao.selectByName(&quot;&#39;minatoaqua&#39; or 1 = 1&quot;)); } } 占位符预处理的PreparedStatement,可以有效防止SQL攻击 /** * please @mio * 使用PreparedStatement防止SQL注入攻击 * * @Author OkamiMio * @Date: 2019/11/12 10:23 */ public class PreparedStatementEmpDaoImpl implements EmpDao { private static Connection connection = DBUtil.getConnection(); private static PreparedStatement preparedStatement = null; private static ResultSet resultSet = null; private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); private static Emp emp = null; @Override public void insert(Emp emp) { try { connection.setAutoCommit(true); String sql = &quot;insert into emp values(?,?,?,?,?,?,?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, emp.getEmpno()); preparedStatement.setString(2, emp.getEname()); preparedStatement.setString(3, emp.getJob()); preparedStatement.setInt(4, emp.getMrg()); preparedStatement.setDate(5, new java.sql.Date(new SimpleDateFormat(&quot;yyyy-MM-DD&quot;).parse(emp.getHiredate()).getTime())); preparedStatement.setDouble(6, emp.getSal()); preparedStatement.setDouble(7, emp.getComm()); preparedStatement.setInt(8, emp.getDeptno()); int result = preparedStatement.executeUpdate(); System.out.println(&quot;受影响的行数是&quot; + result); } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement); } } @Override public void delete(Emp emp) { try { connection.setAutoCommit(true); String sql = &quot;delete from EMP where EMPNO = ?&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, emp.getEmpno()); int result = preparedStatement.executeUpdate(); System.out.println(&quot;受影响的行数是&quot; + result); } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement); } } @Override public void update(Emp emp) { try { connection.setAutoCommit(true); String sql = &quot;update EMP set ENAME = ? where EMPNO = ?&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(2, emp.getEmpno()); preparedStatement.setString(1, emp.getEname()); int result = preparedStatement.executeUpdate(); System.out.println(&quot;受影响的行数是&quot; + result); } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement); } } @Override public Emp select(Integer empNo) { try { connection.setAutoCommit(true); String sql = &quot;select * from emp where EMPNO = ?&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, empNo); resultSet = preparedStatement.executeQuery(); while (resultSet.next()) { emp = new Emp(resultSet.getInt(&quot;empno&quot;), resultSet.getString(&quot;ename&quot;), resultSet.getString(&quot;job&quot;), resultSet.getInt(&quot;mgr&quot;), simpleDateFormat.format(resultSet.getDate(&quot;hiredate&quot;)), resultSet.getDouble(&quot;sal&quot;), resultSet.getDouble(&quot;comm&quot;), resultSet.getInt(&quot;deptno&quot;)); } } catch (SQLException e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement); } return emp; } @Override public Emp selectByName(String name) { try { connection.setAutoCommit(true); String sql = &quot;select * from EMP where ENAME = ?&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,name); resultSet = preparedStatement.executeQuery(); while (resultSet.next()){ emp = new Emp(resultSet.getInt(&quot;empno&quot;), resultSet.getString(&quot;ename&quot;), resultSet.getString(&quot;job&quot;), resultSet.getInt(&quot;mgr&quot;), simpleDateFormat.format(resultSet.getDate(&quot;hiredate&quot;)), resultSet.getDouble(&quot;sal&quot;), resultSet.getDouble(&quot;comm&quot;), resultSet.getInt(&quot;deptno&quot;)); } } catch (SQLException e) { e.printStackTrace(); } return emp; } public static void main(String[] args) { EmpDao empDao = new PreparedStatementEmpDaoImpl(); // Emp mio = new Emp(2, &quot;okamimio&quot;, &quot;vtuber&quot;, 1, &quot;2018-08-20&quot;, 1000.00, 1000.00, 1); // empDao.insert(mio); // empDao.delete(mio); // Emp mio = new Emp(1, &quot;okamimio&quot;, &quot;vtuber&quot;, 1, &quot;2018-08-20&quot;, 1000.00, 1000.00, 1); // empDao.update(mio); // System.out.println(empDao.select(1)); System.out.println(empDao.selectByName(&quot;okamimio&quot;)); } } 预处理及单处理性能对比测试,可以看出来建立连接是非常耗时耗资源的 /** * please @mio * 测试批处理性能 * * @Author OkamiMio * @Date: 2019/11/12 11:48 */ public class BatchDaoImpl { public static void main(String[] args) { long startTime = System.currentTimeMillis(); insertBatch(); long endTime = System.currentTimeMillis(); System.out.println(&quot;批处理时间:&quot; + (endTime - startTime)); System.out.println(&quot;-------&quot;); startTime = System.currentTimeMillis(); for (int i = 201; i &lt; 300; i++) { notInsertBatch(i); } endTime = System.currentTimeMillis(); System.out.println(&quot;非批处理时间:&quot; + (endTime - startTime)); } private static void notInsertBatch(int i) { Connection connection = DBUtil.getConnection(); PreparedStatement preparedStatement = null; try { connection.setAutoCommit(true); String sql = &quot;insert into EMP (empno, ename) values (?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, i); preparedStatement.setString(2, &quot;aqua&quot; + i); int update = preparedStatement.executeUpdate(); } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement); } } private static void insertBatch() { Connection connection = DBUtil.getConnection(); PreparedStatement preparedStatement = null; try { String sql = &quot;insert into EMP (empno, ename) values (?,?)&quot;; connection.setAutoCommit(true); preparedStatement = connection.prepareStatement(sql); for (int i = 101; i &lt; 200; i++) { preparedStatement.setInt(1, i); preparedStatement.setString(2, &quot;mio&quot; + i); preparedStatement.addBatch(); } int[] results = preparedStatement.executeBatch(); } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement); } } } 100条SQL性能对比 利用反射进行统一的查询 /** * please @mio * * @Author OkamiMio * @Date: 2019/11/12 13:34 */ public class BaseDaoImpl { /** * 统一查询表的方法 * * @param sql sql语句 * @param params sql语句的参数 * @param clazz sql语句返回的对象 * @return list */ public List getRows(String sql, Object[] params, Class clazz) { List list = new ArrayList(); Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try { connection = DBUtil.getConnection(); preparedStatement = connection.prepareStatement(sql); if (params != null) { for (int i = 0; i &lt; params.length; i++) { preparedStatement.setObject(i, params[i]); } } resultSet = preparedStatement.executeQuery(); // 取出结果集的元数据对象 ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 查询到每一行的记录中包含多少个列 int columnCount = resultSetMetaData.getColumnCount(); while (resultSet.next()) { Object clazzInstance = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) { // resultSet下标从1开始 Object objectValue = resultSet.getObject(i + 1); // 从元数据中获取列名并转化为大写 String columnName = resultSetMetaData.getColumnName(i + 1).toLowerCase(); // 获取所有属性 Field declaredField = clazz.getDeclaredField(columnName); // 获取所有方法 Method method = clazz.getMethod(getSetName(columnName), declaredField.getType()); if (objectValue instanceof Number) { Number number = (Number) objectValue; String fname = declaredField.getType().getName(); if (&quot;int&quot;.equals(fname) || &quot;java.lang.Integer&quot;.equals(fname)) { method.invoke(clazzInstance, number.intValue()); } else if (&quot;byte&quot;.equals(fname) || &quot;java.lang.Byte&quot;.equals(fname)) { method.invoke(clazzInstance, number.byteValue()); } else if (&quot;short&quot;.equals(fname) || &quot;java.lang.Short&quot;.equals(fname)) { method.invoke(clazzInstance, number.shortValue()); } else if (&quot;long&quot;.equals(fname) || &quot;java.lang.Long&quot;.equals(fname)) { method.invoke(clazzInstance, number.longValue()); } else if (&quot;float&quot;.equals(fname) || &quot;java.lang.Float&quot;.equals(fname)) { method.invoke(clazzInstance, number.floatValue()); } else if (&quot;double&quot;.equals(fname) || &quot;java.lang.Double&quot;.equals(fname)) { method.invoke(clazzInstance, number.doubleValue()); } } else { method.invoke(clazzInstance, objectValue); } } list.add(clazzInstance); } } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.closeConnection(connection, preparedStatement, resultSet); } return list; } private String getSetName(String columnName) { return &quot;set&quot; + columnName.substring(0, 1).toUpperCase() + columnName.substring(1); } public static void main(String[] args) { BaseDaoImpl baseDao = new BaseDaoImpl(); List rows = baseDao.getRows(&quot;select deptno,dname,loc from dept&quot;, new Object[]{}, Dept.class); for (Iterator it = rows.iterator(); it.hasNext(); ) { Dept dept = (Dept) it.next(); System.out.println(dept); } } }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala初识04]]></title>
    <url>%2F2019%2F10%2F17%2FScala%E5%88%9D%E8%AF%8604%2F</url>
    <content type="text"><![CDATA[scala初识04map方法 Builds a new collection by applying a function to all elements of this 通过将函数应用于此元素的所有元素来构建新集合 package study2.collections /** * @Author: mio * @Date: 2019/10/17 15:13 */ object Main { def main(args: Array[String]): Unit = { // val array = Array[Int](1, 2, 3, 4, 5, 6) // array.foreach((x: Int) =&gt; println(x)) // array.foreach(println(_)) // array.foreach(println) val list = List(1, 2, 3, 4, 5, 6) // 普通的foreach写法 // list.foreach(println) // 进阶的写法 list.map((x: Int) =&gt; println(x)) } } flatmap方法 Builds a new collection by applying a function to all elements of this general collection and using the elements of the resulting collections. For example: 通过将函数应用于此常规集合的所有元素并使用所得集合的元素来构建新集合。例如： def getWords(lines: Seq[String]): Seq[String] = lines flatMap (line =&gt; line split &quot;\\W+&quot;) The type of the resulting collection is guided by the static type of [Cannot find macro: $coll.] This might cause unexpected results sometimes. For example: 结果集合的类型由[找不到宏：$ coll。]的静态类型指导。这有时可能会导致意外结果。例如： // lettersOf will return a Seq[Char] of likely repeated letters, instead of a Set def lettersOf(words: Seq[String]) = words flatMap (word =&gt; word.toSet) // lettersOf will return a Set[Char], not a Seq def lettersOf(words: Seq[String]) = words.toSet flatMap (word =&gt; word.toSeq) // xs will be an Iterable[Int] val xs = Map(&quot;a&quot; -&gt; List(11,111), &quot;b&quot; -&gt; List(22,222)).flatMap(_._2) // ys will be a Map[Int, Int] val ys = Map(&quot;a&quot; -&gt; List(1 -&gt; 11,1 -&gt; 111), &quot;b&quot; -&gt; List(2 -&gt; 22,2 -&gt; 222)).flatMap(_._2) package study2.collections /** * @Author: mio * @Date: 2019/10/17 15:13 */ object Main { def main(args: Array[String]): Unit = { val strings: List[String] = List[String](&quot;minato aqua&quot;, &quot;otogibara era&quot;, &quot;nakiri ayame&quot;) val strings2 = strings.flatMap((x: String) =&gt; x.split(&quot; &quot;)) strings2.foreach(println) } } flatMap是Map的一种扩展，Map主要将某个函数应用到集合中的每个元素，并产生一个结果集合，而flatMap跟Map类似，只是传入的函数对每个输入都会返回一个集合（而不是一个元素），然后，flatMap把生成的多个集合“拍扁”成为一个集合。 使用迭代器进行优化,因为迭代器不存数据,可以大大加快效率 package study2.collections /** * @Author: mio * @Date: 2019/10/17 15:13 */ object Main { def main(args: Array[String]): Unit = { // 使用迭代器优化 val iterator: Iterator[String] = strings.iterator val stringsiterator: Iterator[String] = iterator.flatMap((x: String) =&gt; x.split(&quot; &quot;)) // stringsiterator.foreach(println) val tuples2 = stringsiterator.map((_, 1)) while (tuples2.hasNext) { println(tuples2.next()) } } }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala初识03]]></title>
    <url>%2F2019%2F10%2F17%2FScala%E5%88%9D%E8%AF%8603%2F</url>
    <content type="text"><![CDATA[Scala03Trait基础scala 中的Trait和Java中的接口（interface）极其类似 接口是彻底的抽象类，所以在Trait中方法是抽象方法(abstract）不给出具体的方法体。在Java中实现接口使用implement 关键字，但是在Scala中无论是继承类还是实现接口都是使用extends关键字 在scala中类继承Trait，必须是实现其中的抽象方法，实现时不需要使用override关键字，同时Scala同Java一样，不支持多继承类，但是支持多继承接口，使用with关键字 package trait_ /** * @Author: mio * @Date: 2019/10/11 9:22 */ object Main { def main(args: Array[String]): Unit = { val otogibaraEra = new Otogibara(&quot;Era&quot;) otogibaraEra.say() otogibaraEra.sayByAlice() otogibaraEra.sayByMeiji() otogibaraEra.sing() } } class Otogibara(name: String) extends Meiji with Alice { def say() = { println(s&quot;$name say hello&quot;) } override def sing(): Unit = { println(s&quot;$name singing song&quot;) } } trait Meiji { def sayByMeiji() = { println(&quot;Meiji say hello&quot;) } } trait Alice { def sayByAlice() = { println(&quot;Alice say hello&quot;) } // 有点像java的接口,需要子类自己去实现 def sing() } 样例类object Main { def main(args: Array[String]): Unit = { val gibara = new Vtuber(&quot;Otogibara Era&quot;, 14) val meiji = new Vtuber(&quot;Otogibara Era&quot;, 14) // 是false毫无疑问,但是如果类加上case关键字就会变成true println(gibara.equals(meiji)) println(gibara == meiji) } } case class Vtuber(name: String, age: Int) { } 隐式转换package implicit_ import java.util /** * 场景:别人写的类,不想去改他的源码,然后添加一些属于自己的方法 * * @Author: mio * @Date: 2019/10/11 11:13 */ object Main { def main(args: Array[String]): Unit = { val linkedList = new util.LinkedList[Int]() linkedList.add(1) linkedList.add(2) linkedList.add(3) // linkedList.forEach() // def foreach[T](linkedList: util.LinkedList[T], f: (T) =&gt; Unit) = { // val iter = linkedList.iterator() // while (iter.hasNext) f(iter.next()) // } // // foreach(linkedList, println) // println(&quot;-----&quot;) // val vtuber = new otoGibara[Int](linkedList) // vtuber.foreach(println) println(&quot;----- 隐式转换 -----&quot;) implicit def minatoaqua[T](linkedList: util.LinkedList[T]): otoGibara[T] = { new otoGibara(linkedList) } linkedList.foreach(println) } } class otoGibara[T](linkedList: util.LinkedList[T]) { def foreach(f: (T) =&gt; Unit): Unit = { val iter = linkedList.iterator() while (iter.hasNext) f(iter.next()) } } 官网资料一个从类型 S 到类型 T 的隐式转换由一个函数类型 S =&gt; T 的隐式值来定义，或者由一个可转换成所需值的隐式方法来定义。 隐式转换在两种情况下会用到： 如果一个表达式 e 的类型为 S， 并且类型 S 不符合表达式的期望类型 T。 在一个类型为 S 的实例对象 e 中调用 e.m， 如果被调用的 m 并没有在类型 S 中声明。 在第一种情况下，搜索转换 c，它适用于 e，并且结果类型为 T。 在第二种情况下，搜索转换 c，它适用于 e，其结果包含名为 m 的成员。 如果一个隐式方法 List[A] =&gt; Ordered[List[A]]，以及一个隐式方法 Int =&gt; Ordered[Int] 在上下文范围内，那么对下面两个类型为 List[Int] 的列表的操作是合法的： List(1, 2, 3) &lt;= List(4, 5) 在 scala.Predef.intWrapper 已经自动提供了一个隐式方法 Int =&gt; Ordered[Int]。下面提供了一个隐式方法 List[A] =&gt; Ordered[List[A]] 的例子。 import scala.language.implicitConversions implicit def list2ordered[A](x: List[A]) (implicit elem2ordered: A =&gt; Ordered[A]): Ordered[List[A]] = new Ordered[List[A]] { //replace with a more useful implementation def compare(that: List[A]): Int = 1 } 自动导入的对象 scala.Predef 声明了几个预定义类型 (例如 Pair) 和方法 (例如 assert)，同时也声明了一些隐式转换。 例如，当调用一个接受 java.lang.Integer 作为参数的 Java 方法时，你完全可以传入一个 scala.Int。那是因为 Predef 包含了以下的隐式转换： import scala.language.implicitConversions implicit def int2Integer(x: Int) = java.lang.Integer.valueOf(x) 因为如果不加选择地使用隐式转换可能会导致陷阱，编译器会在编译隐式转换定义时发出警告。 要关闭警告，执行以下任一操作： 将 scala.language.implicitConversions 导入到隐式转换定义的上下文范围内 启用编译器选项 -language:implicitConversions 在编译器应用隐式转换时不会发出警告。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala初识02]]></title>
    <url>%2F2019%2F10%2F17%2FScala%E5%88%9D%E8%AF%8602%2F</url>
    <content type="text"><![CDATA[Scala初识02对象和类在同一个object中如果有两个main方法,可以自己选择运行哪个方法 object ObjectAndClass { def main(args: Array[String]): Unit = { val num = 11; println(num) } } object Class { def main(args: Array[String]): Unit = { println(&quot;hello from class&quot;) } } 在object中,代码可以写在方法外面,而java中不可以,暂时可以理解为类似于java中的静态代码块,但是这种说法是不准确的 这种代码是相当于直接在默认构造器中的,有默认构造器,人无需关心 所以作为人关心的应该是个性化构造 类名构造器中的参数就是类的成员属性且默认是private的val类型 类名构造器和个性化构造器会产生冲突 package objectandclass /** * @Author: mio * @Date: 2019/10/9 8:43 */ object ObjectAndClass { def main(args: Array[String]): Unit = { val vtuber = new vtb(14) vtuber.speak(); } } class vtb(sex: String) { var name = &quot;hololive default name&quot; def this(name: Int) { // 这里必须调用默认构造 this(name.toString) // this.name = name.toString } println(&quot;i&#39;m from hololive&quot;) def speak(): Unit = { println(s&quot;i&#39;m $name and i&#39;m a $sex&quot;) } println(&quot;i&#39;m a gamer&quot;) } if,while和forif else和java没有太大区别 package if_while_for object Main { def main(args: Array[String]): Unit = { val age = 14; if (age &gt; 0) println(s&quot;$age &gt; 0&quot;) else if (age &lt; 0) println(s&quot;$age &lt; 0&quot;) else println(s&quot;$age = 0&quot;) } } while也很简单,只不过没有++和–等,取而代之换成了-=和+= package if_while_for object Main { def main(args: Array[String]): Unit = { while (age &gt; 0) { println(age) age -= 1 } } } scala中没有步进for循环,只有迭代型循环,这是一种步进型写法 package if_while_for object Main { def main(args: Array[String]): Unit = { val inclusive: Range.Inclusive = 1 to (10,2) println(inclusive) } } 边界及守卫// 包含边界 val inclusive: Range.Inclusive = 1 to(10, 2) println(inclusive) // 不包含 val range: Range = 1 until (10) println(range) for (i &lt;- range) { println(i) } println(&quot;守卫&quot;) // 守卫 for (i &lt;- range if (i % 2 == 0)) { println(i) } 九九乘法表的练习 // 九九乘法表练习 for (i &lt;- 1 to 9) { for (j &lt;- 1 to 9) { if (j &lt;= i) print(s&quot;$i * $j = ${i * j}\t&quot;) if (j == i) println() } } var num = 0; // 代码改良 for (i &lt;- 1 to 9; j &lt;- 1 to 9) { if (j &lt;= i) print(s&quot;$i * $j = ${i * j}\t&quot;) if (j == i) println() num += 1 } println(num) // 代码继续改良,加入守卫,限制循环次数,节省时间提升性能 num = 0 for (i &lt;- 1 to 9; j &lt;- 1 to 9 if (j &lt;= i)) { if (j &lt;= i) print(s&quot;$i * $j = ${i * j}\t&quot;) if (j == i) println() num += 1 } println(num) yield关键字for循环中的 yield 会把当前的元素记下来，保存在集合中，循环结束后将返回该集合。Scala中for循环是有返回值的。如果被循环的是Map，返回的就是Map，被循环的是List，返回的就是List，以此类推。 val seqss: immutable.IndexedSeq[Int] = for (i &lt;- 1 to 10) yield { i } println(seqss) method和function// method和function def vtuber(): Unit = { println(&quot;hello hololive!&quot;) } val unit = vtuber() println(unit) 有return必须手动给出返回值类型,没有return编译器会自动帮你确定返回类型 object Main { def main(args: Array[String]): Unit = { def hololive()={ 14 } println(hololive()) } } 递归需要明确给出返回值类型 object Main { def main(args: Array[String]): Unit = { def hololive(num: Int):Int = { if (num == 1) { num } else { num * hololive(num - 1) } } println(hololive(5)) } } 默认值函数 object Main { def main(args: Array[String]): Unit = { def hololive(aqua: Int = 14, mea: Int = 15) = { println(aqua + mea) } // hololive() // hololive(16,17) hololive(mea = 18) } } 匿名函数的类型(函数的签名,也就是(参数类型列表)=&gt;返回值类型,参考(Int, Int) =&gt; Int) object Main { def main(args: Array[String]): Unit = { val function: (Int, Int) =&gt; Int = (aqua: Int, mea: Int) =&gt; { aqua + mea } println(function(3,4)) } } 嵌套函数 object Main { def main(args: Array[String]): Unit = { def hololive(a:String)={ def vtuber()={ println(a) } vtuber() } hololive(&quot;aqua&quot;) } } 偏应用函数 import java.util.Date object Main { def main(args: Array[String]): Unit = { def hololive(date: java.util.Date, tp: String, message: String): Unit = { println(s&quot;$date,$tp,$message&quot;) } hololive(new java.util.Date(), &quot;info&quot;, &quot;message&quot;) val info = hololive(_: java.util.Date, &quot;info&quot;, _: String) val error = hololive(_: Date, &quot;error&quot;, _: String) info(new java.util.Date(), &quot;message&quot;) error(new java.util.Date(), &quot;message&quot;) } } 可变参数列表,要求参数的类型一致 object Main { def main(args: Array[String]): Unit = { def hololive(aqua: Int*): Unit = { // for (element &lt;- aqua) println(element) // aqua.foreach((x: Int) =&gt; { // println(x) // }) // aqua.foreach(println(_)) aqua.foreach(println) } hololive(1, 2, 3, 4, 5) } } 高阶函数,在函数体中如果参数列表的参数按顺序依次出现在函数体中,可以省略参数列表并且将参数全部换成下划线 object Main { def main(args: Array[String]): Unit = { def compare(aqua: Int, mea: Int, f: (Int, Int) =&gt; Int): Unit = { val result: Int = f(aqua, mea) println(result) } compare(3, 8, (x: Int, y: Int) =&gt; { x + y }) compare(3, 8, _ + _) compare(3, 8, (x: Int, y: Int) =&gt; { x * y }) compare(3, 8, _ * _) } } object Main { def main(args: Array[String]): Unit = { def compare(aqua: Int, mea: Int, f: (Int, Int) =&gt; Int): Unit = { val result: Int = f(aqua, mea) println(result) } def factory(i: String): (Int, Int) =&gt; Int = { def plus(x: Int, y: Int): Int = { x + y } if (i.equals(&quot;+&quot;)) { plus } else { (x: Int, y: Int) =&gt; { x * y } } } compare(3, 8, factory(&quot;*&quot;)) } } 柯里化object Main { def main(args: Array[String]): Unit = { def hololive(mea: Int)(aqua: Int)(alice: String) = { println(s&quot;$mea,$aqua,$alice&quot;) } hololive(2)(1)(&quot;alice&quot;) def vtuber(a: Int*)(b: String*): Unit = { a.foreach(println) b.foreach(println(_)) } vtuber(1,2)(&quot;alice&quot;) } } 官网参考资料在Scala中，所有的值都有类型，包括数值和函数。下图阐述了类型层次结构的一个子集。 Scala类型层次结构Any是所有类型的超类型，也称为顶级类 型。它定义了一些通用的方法如equals、hashCode和toString。Any有两个直接子类：AnyVal和AnyRef。 AnyVal代表值类型。有9个预定义的非空的值类型分别是：Double、Float、Long、Int、Short、Byte、Char、Unit和Boolean。Unit是不带任何意义的值类型，它仅有一个实例可以像这样声明：()。所有的函数必须有返回，所以说有时候Unit也是有用的返回类型。 AnyRef代表引用类型。所有非值类型都被定义为引用类型。在Scala中，每个用户自定义的类型都是AnyRef的子类型。如果Scala被应用在Java的运行环境中，AnyRef相当于java.lang.Object。 这里有一个例子，说明了字符串、整型、布尔值和函数都是对象，这一点和其他对象一样： val list: List[Any] = List( &quot;a string&quot;, 732, // an integer &#39;c&#39;, // a character true, // a boolean value () =&gt; &quot;an anonymous function returning a string&quot; ) list.foreach(element =&gt; println(element)) 这里定义了一个类型List&lt;Any&gt;的变量list。这个列表里由多种类型进行初始化，但是它们都是scala.Any的实例，所以可以把它们加入到列表中。 下面是程序的输出： a string 732 c true &lt;function&gt; 类型转换值类型可以按照下面的方向进行转换： 例如： val x: Long = 987654321 val y: Float = x // 9.8765434E8 (note that some precision is lost in this case) val face: Char = &#39;☺&#39; val number: Int = face // 9786 转换是单向，下面这样写将不会通过编译。 val x: Long = 987654321 val y: Float = x // 9.8765434E8 val z: Long = y // Does not conform 你可以将一个类型转换为子类型，这点将在后面的文章介绍。 Nothing和NullNothing是所有类型的子类型，也称为底部类型。没有一个值是Nothing类型的。它的用途之一是给出非正常终止的信号，如抛出异常、程序退出或者一个无限循环（可以理解为它是一个不对值进行定义的表达式的类型，或者是一个不能正常返回的方法）。 Null是所有引用类型的子类型（即AnyRef的任意子类型）。它有一个单例值由关键字null所定义。Null主要是使得Scala满足和其他JVM语言的互操作性，但是几乎不应该在Scala代码中使用。我们将在后面的章节中介绍null的替代方案。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala初识01]]></title>
    <url>%2F2019%2F09%2F25%2FScala%E5%88%9D%E8%AF%8601%2F</url>
    <content type="text"><![CDATA[Scala初识课前预备基础知识编程语言分为编译型和解释型两种 编译型语言（1）只须编译一次就可以把源代码编译成机器语言，后面的执行无须重新编译，直接使用之前的编译结果就可以；因此其执行的效率比较高；（2）编译性语言代表：C、C++、Pascal/Object Pascal（Delphi）；（3）程序执行效率比较高，但比较依赖编译器，因此跨平台性差一些； 解释型语言（1）源代码不能直接翻译成机器语言，而是先翻译成中间代码，再由解释器对中间代码进行解释运行，源代码—&gt;中间代码—&gt;机器语言 （2）程序不需要编译，程序在运行时才翻译成机器语言，每执行一次都要翻译一次；（3）解释性语言代表：Python、JavaScript、Shell、Ruby、MATLAB等；（4）运行效率一般相对比较低，依赖解释器，跨平台性好； 比较（1）一般，编译性语言的运行效率比解释性语言更高；但是不能一概而论，部分解释性语言的解释器通过在运行时动态优化代码，甚至能使解释性语言的性能超过编译性语言； （2）编译性语言的跨平台特性比解释性语言差一些； java呢?java属于编译型+解释型语言 java文件先编译成与平台无关的.class的字节码文件，然后.class的字节码文件既可以在Windows平台上的java虚拟机（JVM）上进行解释运行，也可以在Linux平台上的JVM上解释运行；而JVM的翻译过程时解释性的，JVM从.class的字节码文件中读出一条指令，翻译一条指令，然后执行一条指令，这个过程就称为java的解释执行； 语言的类型强类型语言 强类型语言也称为强类型定义语言，是一种总是强制类型定义的语言，要求变量的使用要严格符合定义，所有变量都必须先定义后使用。 Java、.Net和C++等一些语言都是强制类型定义的，也就是说，一旦一个变量被指定了某个数据类型，如果不经过强制转换，那么它就永远是这个数据类型了。 例如你有一个整数，如果不显式地进行转换，你不能将其视为一个字符串。 弱类型语言 弱类型语言也称为弱类型定义语言，与强类型定义相反。像VB，PHP等一些语言就属于弱类型语言。 简单理解就是一种变量类型可以被忽略的语言。比如VBScript是弱类型定义的，在VBScript中就可以将字符串’12’和整数3进行连接得到字符串’123’，然后可以把它看成整数123，而不用显示转换。但其实他们的类型没有改变，VB只是在判断出一个表达式含有不同类型的变量之后，自动在这些变量前加了一个clong()或(int)()这样的转换函数而已。能做到这一点其实是归功于VB的编译器的智能化而已，这并非是VB语言本身的长处或短处。 强类型语言和弱类型语言比较 强类型语言在速度上可能略逊色于弱类型语言，但是强类型语言带来的严谨性可以有效地帮助避免许多错误。 语言的模型面向过程:基本类型+指针 面向对象:基本类型+对象类型 面向函数:基本类型+对象类型+函数 SCALA:面向对象的函数式编程语言 准备工作scala官网 jdk下载地址 idea官网 idea的欢迎页面,寻找configure-plugins,搜索scala,安装,重启idea configure - projet defaults - project structure - global libraries - + - scala sdk - download - 2.11.12 - OK - OK,然后开始等待下载完成 右键scala sdk 2.11.12 - copy to library - OK 然后查看libraries里面有没有scala的sdk HelloWorld右键新建一个scala class,kind选择object,对比一下两个版本的helloworld,可以发现scala的语法简洁不少 好玩的知识点scala中类名可以和文件名不一样 object约等于static,是一个单例的对象 只有在类名构造器中的参数可以设置成var,其他方法函数中的参数都是val类型的,并且不允许设置成var类型 最后注意一下伴生关系,在同一个文件中定义一个与类名相同的object,那么这个object叫做这个类的半生对象,这个类叫做这个这个对象的伴生类,伴生对象和半生类可以互相访问彼此的私有属性 附上练习代码 package helloworld /** * @Author: okami mio * @Date: 2019/9/25 10:02 */ object HelloWorldScala01 { def main(args: Array[String]): Unit = { println(&quot;hello world from scala&quot;) } } /** * 可以写多个object和class */ object Hololive { println(&quot;hololive start!&quot;) // private val minato: Hololive = new Hololive() // private val vtber: Hololive = new Hololive(&quot;nakiri&quot;, 10) // private val vtber: Hololive = new Hololive(&quot;girl&quot;) private val vtber: Hololive = new Hololive(&quot;okami mio&quot;, 11) private val name = &quot;hololive object&quot; def main(args: Array[String]): Unit = { println(&quot;hello hololive!&quot;) // minato.printName() vtber.printName() } println(&quot;hololive stop!&quot;) } class Hololive(var sex: String) { var aqua = &quot;minato aqua&quot; var age = 13 val newAge = 14 var name = &quot;hololive class&quot; sex = &quot;girl&quot; println(&quot;hololive static code start&quot;) def this(name: String, age: Int) { // 如果是个性构造,必须调用默认构造 this(&quot;boy&quot;); this.aqua = name; this.age = age; } def printName(): Unit = { println(s&quot;hello world from $aqua&quot;) println(s&quot;hello world from ${age + age}&quot;) println(s&quot;minato aqua&#39;age$newAge&quot;) // println(Hololive.minato) println(Hololive.vtber) println(s&quot;this is sex $sex&quot;) println(sex) println(s&quot;this is hololive&#39;name : ${Hololive.name}&quot;) println(Hololive.name) println(name) } println(&quot;hololive static code end&quot;) }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leecode刷题记录-数据库]]></title>
    <url>%2F2019%2F09%2F19%2Fleecode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95-%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[leecode刷题记录类别:简单菜逼的leecode刷题记录开始 1.去重select distinct a.Email from Person a, Person b where a.Email = b.Email and a.Id != b.Id; select email from person GROUP BY email HAVING count(email) &gt; 1; 2.看起来简单的索引机制select name,population,area from world where population &gt; 25000000 or area &gt; 3000000; select name,population,area from World where area &gt; 3000000 union select name,population,area from World where population &gt; 25000000; 对于索引列来最好使用union all，因复杂的查询【包含运算等】将使or、in放弃索引而全表扫描，除非你能确定or、in会使用索引。 对于只有非索引字段来说你就老老实实的用or 或者in，因为 非索引字段本来要全表扫描而union all 只成倍增加表扫描的次数 3.一个判断奇偶你至于搞出这么多方法嘛?(lll￢ω￢)select * from cinema where id &amp; 1 and description != &quot;boring&quot; order by rating desc; select * from cinema where id % 2 = 1 and description &lt;&gt; &#39;boring&#39; order by rating desc; select * from cinema where mod(id, 2) = 1 and description != &#39;boring&#39; order by rating desc; 4.同时做交换update salary set sex = if(sex = &#39;m&#39;,&#39;f&#39;,&#39;m&#39;); update salary set sex = (case when sex = &#39;m&#39; then &#39;f&#39; when sex = &#39;f&#39; then &#39;m&#39; end); IF(expr1,expr2,expr3)如果 expr1 是TRUE (expr1 &lt;&gt; 0 and expr1 &lt;&gt; NULL)，则 IF()的返回值为expr2; 否则返回值则为 expr3。IF() 的返回值为数字值或字符串值，具体情况视其所在语境而定。 CASE WHEN的两种写法:Type 1: CASE value WHEN [compare-value] THEN result [WHEN [compare-value] THEN result …] [ELSE result] ENDType 2: CASE WHEN [condition] THEN result [WHEN [condition] THEN result …] [ELSE result] END 5.joinselect t1.firstname,t1.lastname,t2.city,t2.state from person t1 left join address t2 on t1.personid = t2.personid; 数据库在通过连接两张或多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。 在使用left jion时，on和where条件的区别如下： on条件是在生成临时表时使用的条件，它不管on中的条件是否为真，都会返回左边表中的记录。 where条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有left join的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。 6.笛卡尔积select e1.Name as Employee from Employee as e1 ,Employee as e2 where e1.ManagerId=e2.Id AND e1.Salary&gt;e2.Salary; select name as employee from employee as a where salary &gt; (select salary from employee where id = a.managerid); select e.Name as Employee from Employee as e inner join Employee as f on e.ManagerId=f.Id where e.salary&gt;f.salary; 7.offset关键字,这个真的不知道select distinct salary as SecondHighestSalary from employee order by salary desc limit 1 offset 1; 8.not in关键字select name Customers from Customers where Id not in (select CustomerId from Orders); 9.这题卡住了,哎SELECT Department.name AS &#39;Department&#39;, Employee.name AS &#39;Employee&#39;, Salary FROM Employee JOIN Department ON Employee.DepartmentId = Department.Id WHERE (Employee.DepartmentId , Salary) IN ( SELECT DepartmentId, MAX(Salary) FROM Employee GROUP BY DepartmentId ) ; 10.group by having的用法如果要用到group by 一般用到的就是“每”这个字 例如说明现在有一个这样的表：每个部门有多少人 就要用到分组的技术 having是分组（group by）后的筛选条件，分组后的数据组内再筛选,而where则是在分组前筛选 where子句中不能使用聚集函数，而having子句中可以，所以在集合函数中加上了HAVING来起到测试查询结果是否符合条件的作用即having子句的适用场景是可以使用聚合函数 having 子句限制的是组，而不是行having 子句中的每一个元素也必须出现在select列表中。有些数据库例外，如oracle当同时含有 where 子句、group by 子句 、having 子句及聚集函数时，执行顺序如下：执行where子句查找符合条件的数据；使用group by 子句对数据进行分组；对group by 子句形成的组运行聚集函数计算每一组的值；最后用having 子句去掉不符合条件的组 11.删除重复记录SELECT p1.* FROM Person p1, Person p2 WHERE p1.Email = p2.Email ; SELECT p1.* FROM Person p1, Person p2 WHERE p1.Email = p2.Email AND p1.Id &gt; p2.Id ; DELETE p1 FROM Person p1, Person p2 WHERE p1.Email = p2.Email AND p1.Id &gt; p2.Id 12.datediff函数(日期相差天数)select w1.id from weather w1,weather w2 where w1.temperature &gt; w2.temperature and DATEDIFF(w1.recorddate,w2.recorddate) = 1]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper初识]]></title>
    <url>%2F2019%2F09%2F18%2FZookeeper%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Zoopeeker初识分布式协调服务模型 具有两种运行状态,可用(leader存在)及不可用(leader不存在),不可用状态需要及时恢复到可用状态的时间需要越快越好,官方压测时间可以维持在200毫秒以内. 性能zookeeper在”读取主导”工作负载中特别快,运行在数千台计算机上并且读取写入比大约为10:1的时候性能最佳. 官方给出的读写占比及计算机数量性能图 结构目录树结构,node可以存储数据,单个节点存储的最大数据为1mb,节点可以分为几种,持久节点,临时节点(依赖session),序列节点(概念) 保证提供了一系列保证 顺序一致性 客户端的更新将按发送顺序应用 原子性 更新成功或失败,没有中间结果(最终一致性而并非强一致性,过半通过就可以) 单系统镜像 无论服务器连接到哪个服务器,客户端都将看到相同的服务视图 可靠性 一旦应用了更新,它将从那时起持续到客户端覆盖更新 及时性 系统的客户视图保证在特定时间范围内是最新的(最终一致性,有可能会在很短的时间内不能访问即时的数据) 安装及实践server.1=aqua:2888:3888 #常用指令 help ls / create get rmr stat 创建节点时,一定要给数据,哪怕是不要数据,也一定要给一个””,否则会创建失败 zookeeper存储是二进制安全的.只关心二进制化的字符串,不关心字符串的具体格式,里面有啥字符,只会严格的按照二进制的数据存取,不会以某种特殊格式解析字符串,即使用字节数组存储. zookeeper的顺序执行体现在id上,leader维护了一个id的递增. id格式详解: zookeeper 的每个节点都有三个 zxid 值：cZxid、mZxid 和 pZxid,分别代表的时间戳对应为: c(create,新增):对应为该节点的创建时间 m(mofify,修改):对应该节点的最近一次的修改时间 p(子节点):该节点或该节点的子节点最近一次创建/删除的时间,有本节点/子节点有关,与孙子节点无关 0x:代表16进制,每一个位代表四个二进制位,共有32位,也就是低32位,16进制数必须以 0x开头 x后面的2代表纪元,也就是说每次更换leader就会递增这个纪元 常见zookeeper指令每一个客户端有一个session连接的时候,都会创建一个sessionid. 临时节点会伴随着session的消失而消失. 当一个client连接一个server的时候,这个session会被所有的server知道,这就是所谓的统一视图. create -s (创建队列,防止多个客户端创建产生冲突) netstat -natp| egrep &#39;(2888|3888)&#39; 查资料得知 2181：对cline端提供服务 2888：集群内机器通讯使用（Leader监听此端口） 3888：选举leader使用 端口状态 LISTEN：(Listening for a connection.)侦听来自远方的TCP端口的连接请求 SYN-SENT：(Active; sent SYN. Waiting for a matching connection request after having sent a connection request.)再发送连接请求后等待匹配的连接请求 SYN-RECEIVED：(Sent and received SYN. Waiting for a confirming connection request acknowledgment after having both received and sent connection requests.)再收到和发送一个连接请求后等待对方对连接请求的确认 ESTABLISHED：(Connection established.)代表一个打开的连接 FIN-WAIT-1：(Closed; sent FIN.)等待远程TCP连接中断请求，或先前的连接中断请求的确认 FIN-WAIT-2：(Closed; FIN is acknowledged; awaiting FIN.)从远程TCP等待连接中断请求 CLOSE-WAIT：(Received FIN; waiting to receive CLOSE.)等待从本地用户发来的连接中断请求 CLOSING：(Closed; exchanged FIN; waiting for FIN.)等待远程TCP对连接中断的确认 LAST-ACK：(Received FIN and CLOSE; waiting for FIN ACK.)等待原来的发向远程TCP的连接中断请求的确认 TIME-WAIT：(In 2 MSL (twice the maximum segment length) quiet wait after close. )等待足够的时间以确保远程TCP接收到连接中断请求的确认 CLOSED：(Connection is closed.)没有任何连接状态]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的RDB和AOF]]></title>
    <url>%2F2019%2F09%2F17%2FRedis%E7%9A%84RDB%E5%92%8CAOF%2F</url>
    <content type="text"><![CDATA[Redis之RDB和AOFlinux管道命令将前一个命令的输出作为后一个命令的输入 需要注意,管道符前的指令必须有正确的输出,管道符后必须可以处理前面的正确输出结果,不能处理前面的错误输出 管道会触发两个子进程执行”|”两边的程序,重定向是在一个进程内执行 redis有两种持久化数据的方案,分别为RDB(Redis DataBase)和AOF(Append Only File) RDBRDB 是 Redis 默认的持久化方案。在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个dump.rdb文件。Redis 重启会通过加载dump.rdb文件恢复数据。 #打开redis-cli set k1 v1 set k2 v2 set k3 v3 set k4 v4 set k5 v5 keys * rewrite #然后找到dump.rdb linux -&gt; cp dump.rdb dump_bac.rdb 然后回到redis-cli flushall keys * #可以看到什么都没有了 #这时退出client,把dump_bac改回dump.rdb,然后重新进入client keys * #可以看到所有的keys都回来了 #个人理解,相当于做了一个临时快照,使得数据可以持久化到磁盘上,不至于断电后丢失 AOFAOF ：Redis 默认不开启。它的出现是为了弥补RDB的不足（数据的不一致性），所以它采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 #打开配置文件,需要手动把no改为yes appendonly yes #指定本地数据库文件名，默认值为 appendonly.aof appendfilename &quot;appendonly.aof&quot; #指定更新日志条件 # appendfsync always appendfsync everysec # appendfsync no #always：同步持久化，每次发生数据变化会立刻写入到磁盘中。性能较差当数据完整性比较好（慢，安全） #everysec：出厂默认推荐，每秒异步记录一次（默认值） #no：不同步 #配置重写触发机制 #当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发,一般都设置为3G,64M太小了 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb [root@itdragon bin]# vim appendonly.aof appendonly yes [root@itdragon bin]# ./redis-server redis.conf [root@itdragon bin]# ./redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379&gt; keys * (empty list or set) 127.0.0.1:6379&gt; set keyAOf valueAof OK 127.0.0.1:6379&gt; FLUSHALL OK 127.0.0.1:6379&gt; SHUTDOWN not connected&gt; QUIT [root@itdragon bin]# ./redis-server redis.conf [root@itdragon bin]# ./redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379&gt; keys * 1) &quot;keyAOf&quot; 127.0.0.1:6379&gt; SHUTDOWN not connected&gt; QUIT [root@itdragon bin]# vim appendonly.aof fjewofjwojfoewifjowejfwf [root@itdragon bin]# ./redis-server redis.conf [root@itdragon bin]# ./redis-cli -h 127.0.0.1 -p 6379 Could not connect to Redis at 127.0.0.1:6379: Connection refused not connected&gt; QUIT [root@itdragon bin]# redis-check-aof --fix appendonly.aof &#39;x 3e: Expected prefix &#39;*&#39;, got: &#39; AOF analyzed: size=92, ok_up_to=62, diff=30 This will shrink the AOF from 92 bytes, with 30 bytes, to 62 bytes Continue? [y/N]: y Successfully truncated AOF [root@itdragon bin]# ./redis-server redis.conf [root@itdragon bin]# ./redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379&gt; keys * 1) &quot;keyAOf&quot; 触发AOF快照根据配置文件触发，可以是每次执行触发，可以是每秒触发，可以不同步。 根据AOF文件恢复数据正常情况下，将appendonly.aof 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。但在实际开发中，可能因为某些原因导致appendonly.aof 文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof –fix appendonly.aof 进行修复 。从下面的操作演示中体会。 AOF的重写机制前面也说到了，AOF的工作原理是将写操作追加到文件中，文件的冗余内容会越来越多。所以聪明的 Redis 新增了重写机制。当AOF文件的大小超过所设定的阈值时，Redis就会对AOF文件的内容压缩。 重写的原理：Redis 会fork出一条新进程，读取内存中的数据，并重新写到一个临时文件中。并没有读取旧文件（你都那么大了，我还去读你？？？ o(ﾟДﾟ)っ傻啊！）。最后替换旧的aof文件。 触发机制：当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发。这里的“一倍”和“64M” 可以通过配置文件修改。 AOF 的优缺点优点：数据的完整性和一致性更高缺点：因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群]]></title>
    <url>%2F2019%2F09%2F17%2FRedis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[Redis集群#老版用法 help slaveof #新版用法 help replicaof replicaof 127.0.0.1 6379 redis集群搭建流程 aqua mea cierra 主 两个 从 两个 两个 哨兵 是 是 是 redis单节点容量问题解决方案代理模型1.哈希取模(弊端:模数固定,也就是说节点的拓展比较麻烦,新增一台机器必须重新取模) 2.随机(弊端:取回数据的时候比较麻烦,需要遍历) 3.一致性哈希算法(优点:的确可以分担其他节点的压力,也不会造成全局洗牌.弊端:新增节点会造成一小部分数据无法命中) 没有取模,client的key和node都需要进行计算,规划成一个哈希环 导致的问题: 1.可能造成击穿 2.增加复杂度,每次取离数据最近的2个物理节点 哈希环上的节点数尽可能多,可以解决数据倾斜的问题,也就是虚拟节点技术. 代理方案3种代理方式 twemproxy predixy cluster 预分区概念+槽位 数据分治,聚合操作很难实现,事物,由此产生hash tag,也就是相当于redis分区 redis集群有16384个哈希槽位 集群搭建 请同学们学习的时候不要无脑ctrl+c,一定要尝试理解每步的具体含义,如果按照本教程ctrl+c的话,未必能跑的通,因为有一些选项是需要根据你的实际情况更改的,我仅仅是贴出适合我环境的指令,并不提供语法模板. twemproxy准备三台机器,我的分别是aqua,mea,cierra,请不要在意名字 yum install automake libtool autoreconf -fvi 发现报错 原因:autoreconf版本低了,不受支持,去阿里镜像找更高版本的autoreconf,本人linux版本centos6.6 cd /etc/yum.repos.d/ 外网环境使用这种指令 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo 外网情况请执行如下指令 yum search autoconf yum install autoconf268 我是内网服务器,只能手工操作,不过这都是小事,内网环境可以这样做,去朋友那里找一台外网机器 yum install yum-plugin-downloadonly yum install --downloadonly --downloaddir=. autoconf268 ll 可以看到rpm包已经下载好了,我们便可以把这个包拷到内网环境上 rpm -ivh autoconf-2.63-5.1.el6.noarch.rpm rpm -ivh autoconf268-2.68-2.el6.noarch.rpm 可以看到已经有autoconf268的命令了,代表成功!撒花庆祝 autoconf268 在twemproxy目录下执行 autoconf268 -fvi ./configure make cd scripts cp nutcracker.init /etc/init.d/twemproxy cd twemproxy/conf 把配置文件全都拷过去 cp * /etc/nutcracker/ 这样做可以在任何路径下都能直接使用命令 cp nutcracker /usr/bin/ 进入redis目录 make redis的清除cache命令 make distclean 开始安装redis make install PREFIX=/opt/yupsoftware/redis-5.0.5/ cd /opt/yupsoftware/redis-5.0.5/utils ./install_server.sh 输入你自己配置的参数,然后检查一下服务是否正常 service redis_6379 status 搞定之后去搞其他的机器,等到三台机器全部搞好之后回主机,看看代理是否正常 service twemproxy start service twemproxy status 日,查找资料得修改redis的配置文件,这里应该能有找到报错的地方,不过现在我不知道,有时间应该查一下资料 #bind 127.0.0.1 protected-mode no redis-cli -h mea redis-cli -h cierra 继续 redis-cli -p 22121 卧槽?什么情况?查询资料得知,正常情况,不要担心,继续不支持keys *,事物,watch,flushall等等,详情请参照twemproxy官方文档 # 不支持 keys * 其实这里已经大功告成,可以新开client去连redis-server,直接就能找到key了 不过个人搭建期间还是遇到了不少问题,同学们一定要多多注意,我贴一下我遇到的一个小问题 解决方法也很简单,改配置文件就行了,注意缩进 predixy首先去predixy下载,然后解压,进入配置文件目录,打开predixy.conf文件,进行如下修改 Bind 127.0.0.1:7617 Include sentinel.conf # Include try.conf 保存,退出,打开sentinel.conf,参照example里面进行你自己的修改,使用vim的命令行工具 #光标放置SentinelServerPool这一行 .,$y #粘贴命令,p p .,$s/#// 普及一个小小的vim快捷键指令 gg 跳转到文档开头 G 跳转到文档末尾 继续修改配置文件 #Sentinels里面改成你的哨兵的端口号,group改成你的哨兵名 Sentinels { + aqua:26379 + mea:26379 + cierra:26379 } Group nakiri { } Group nemu { } 新建哨兵的配置文件26379.conf # 哨兵的启动端口 port 26379 # 哨兵监控的第一个master的端口 sentinel monitor nakiri aqua 36379 2 # 哨兵监控的第二个master的端口 sentinel monitor nemu aqua 46379 2 启动哨兵 redis-server 26379.conf --sentinel --bind aqua --protected-mode no redis-server 26379.conf --sentinel --bind mea --protected-mode no redis-server 26379.conf --sentinel --bind cierra --protected-mode no 启动主(aqua) # redis-server --port 36379 # redis-server --port 46379 redis-server --port 36379 --bind aqua --protected-mode no redis-server --port 46379 --bind aqua --protected-mode no 启动从(mea,cierra) redis-server --port 36380 --replicaof aqua 36379 --bind mea --protected-mode no redis-server --port 36380 --replicaof aqua 36379 --bind cierra --protected-mode no redis-server --port 46380 --replicaof aqua 46379 --bind mea --protected-mode no redis-server --port 46380 --replicaof aqua 46379 --bind cierra --protected-mode no 启动predixy ./predixy ../conf/predixy.conf 启动client redis-cli -p 7617 set hololive minatoaqua get hololive 可以看到结果,代表成功,然后新开cli去找一下数据在哪看看,可以看到数据已经打散分布到各个机器上 继续测试控制存储,插入数据 set {aqua}aqua mea set {aqua}mea aqua 可以看到,可以由你自己控制存在哪里 watch {aqua}aqua 不支持,因为有两套主从,想要事物,只能存在一个group 所以修改配置文件,删除一个group set aqua mea set mea aqua keys * watch aqua multi get aqua set aqua exec 可以发现已经支持事物了 继续测试,可以发现组二的已经全部查不到了,但是组一的还在,可以正常查 info看一下信息 # Proxy Version:1.0.5 Name:PredixyExample Bind:127.0.0.1:7617 RedisMode:proxy SingleThread:false WorkerThreads:1 Uptime:1568684273 UptimeSince:2019-09-17 09:37:53 # SystemResource UsedMemory:66776 MaxMemory:0 MaxRSS:4296704 UsedCpuSys:0.510 UsedCpuUser:0.025 # Stats Accept:2 ClientConnections:1 TotalRequests:2096 TotalResponses:2097 TotalRecvClientBytes:718 TotalSendServerBytes:101957 TotalRecvServerBytes:1146244 TotalSendClientBytes:307 # Servers Server:192.168.3.72:36380 Role:master Group:nakiri DC: CurrentIsFail:0 Connections:2 Connect:2 Requests:21 Responses:21 SendBytes:590 RecvBytes:140 Server:192.168.3.70:36379 Role:slave Group:nakiri DC: CurrentIsFail:0 Connections:0 Connect:0 Requests:0 Responses:0 SendBytes:0 RecvBytes:0 # LatencyMonitor LatencyMonitorName:all &lt;= 100 59 6 24.00% &lt;= 400 1407 4 40.00% &lt;= 500 2652 6 64.00% &lt;= 700 1953 3 76.00% &lt;= 800 782 1 80.00% &lt;= 900 1635 2 88.00% &lt;= 1000 1896 2 96.00% &lt;= 1200 1060 1 100.00% T 457 11444 25 LatencyMonitorName:get &lt;= 400 1407 4 26.67% &lt;= 500 2236 5 60.00% &lt;= 700 1953 3 80.00% &lt;= 900 815 1 86.67% &lt;= 1000 946 1 93.33% &gt; 1000 1060 1 100.00% T 561 8417 15 LatencyMonitorName:set &lt;= 800 782 1 50.00% &lt;= 900 820 1 100.00% T 801 1602 2 LatencyMonitorName:blist 可以看到现在server是cierra的36380 ctrl+c取消cierra的主服务 过一会,哨兵自动更换主 这时再回去info看一下 # Proxy Version:1.0.5 Name:PredixyExample Bind:127.0.0.1:7617 RedisMode:proxy SingleThread:false WorkerThreads:1 Uptime:1568684273 UptimeSince:2019-09-17 09:37:53 # SystemResource UsedMemory:70872 MaxMemory:0 MaxRSS:4354048 UsedCpuSys:1.189 UsedCpuUser:0.058 # Stats Accept:2 ClientConnections:1 TotalRequests:5649 TotalResponses:5657 TotalRecvClientBytes:1172 TotalSendServerBytes:275554 TotalRecvServerBytes:3118933 TotalSendClientBytes:13785 # Servers Server:192.168.3.72:36380 Role:slave Group:nakiri DC: CurrentIsFail:0 Connections:2 Connect:2 Requests:31 Responses:31 SendBytes:874 RecvBytes:202 Server:192.168.3.70:36379 Role:master Group:nakiri DC: CurrentIsFail:0 Connections:0 Connect:0 Requests:0 Responses:0 SendBytes:0 RecvBytes:0 # LatencyMonitor LatencyMonitorName:all &lt;= 100 459 15 33.33% &lt;= 300 854 3 40.00% &lt;= 400 3658 11 64.44% &lt;= 500 2652 6 77.78% &lt;= 600 513 1 80.00% &lt;= 700 1953 3 86.67% &lt;= 800 782 1 88.89% &lt;= 900 1635 2 93.33% &lt;= 1000 1896 2 97.78% &lt;= 1200 1060 1 100.00% T 343 15462 45 LatencyMonitorName:get &lt;= 300 854 3 12.00% &lt;= 400 3312 10 52.00% &lt;= 500 2236 5 72.00% &lt;= 600 513 1 76.00% &lt;= 700 1953 3 88.00% &lt;= 900 815 1 92.00% &lt;= 1000 946 1 96.00% &gt; 1000 1060 1 100.00% T 467 11689 25 LatencyMonitorName:set &lt;= 800 782 1 50.00% &lt;= 900 820 1 100.00% T 801 1602 2 LatencyMonitorName:blist 可以发现,主已经被更换,服务还能正常使用,并且对于7617的用户来说,并没有任何的感知,该怎么用还是怎么用 cluster(这地方挺简单的,就不贴图了) 不需要代理,redis自行实现,无主模型 cd utils/create-cluster vim create-cluster 直接启动 ./create-cluster start 进行分赃 ./create-cluster create 进去看看 # 自动跳转 redis-cli -c -p 30001 set aqua minatoaqua 尝试一下事物 multi set mea kaguramea get mea exec 不可以,更换 watch {aqua}aqua minatoaqua multi set {aqua}aqua minatoaqua get {aqua}aqua exec 成功 关闭集群 ./create-cluster stop ./create-cluster clean 这个脚本实际相当于 redis-cli --cluster help 手动实现 redis-cli --cluster create 127.0.0.1:40001 127.0.0.1:40002 127.0.0.1:40003 127.0.0.1:40004 127.0.0.1:40005 127.0.0.1:40006 --cluster-replicas 1 新开一个客户端 redis-cli -c -p 40001 redis-cli --cluster reshard 127.0.0.1:40001 redis-cli --cluster info 127.0.0.1:40001 redis-cli --cluster check 127.0.0.1:40001]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络和IO初识]]></title>
    <url>%2F2019%2F09%2F03%2F%E7%BD%91%E7%BB%9C%E5%92%8CIO%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[关于网络和IO鉴于最近课上已经有点开始懵,所以在这里普及一下关于网络和IO的基础知识 网络基础知识本来我写了感想,结果码成天书,故删除.推荐一个视频地址https://www.bilibili.com/video/av53978866/?p=28 IOBIO线程模型:传统BIO的阻塞点: socket.accept(); inputstrean.read(); 缺点:单线程,同一时间只能处理一个客户端,如果同时连上两个客户端,只能处理第一个客户端,第二个客户端虽然能连上但是服务器端不做处理 解决办法,拿到socket之后,把它交给另一个线程做单独处理 但是服务器端的线程并不是无限的,如果来了很多客户端,服务端会因为起过多线程,资源耗尽 解决方法,服务器端线程池,由线程池去拉起线程 Executors.newFixedThreadPool(100); 本质依然是去new Thread,只不过对数量可以做一个限制,超过了就丢掉或者队列 BIO模型总结 优点:一个线程为一个客户端服务,服务的质量好 缺点:需要为每一个客户端分配一个线程,并发连接的数量巨大时,县城所有用资源和cpu线程切换带来的开销巨大 NIO线程模型服务器端只关心socket,客户端关心的是io事件 由此,在服务器端抽象出一个selector,为socket管理者,意思是将大门交给selector看着,监听是否有accpet事件 listenSelector采用轮询的方式监听selector上是否由需要处理的事件 nio模型中没有new任何线程的概念,只有一个线程在处理 selector按照事件来处理,就是事件驱动,也就是多路复用模型 缺点:selector只有一个太忙,需要处理所有的socket,所以衍生出了netty框架 IO模型概念阻塞:客户端发来请求(在收到服务器请求之前必须等着,不能去干其他活)-&gt;服务器端收到请求-&gt;服务器端读取数据-&gt;服务器端读取完成-&gt;返回给客户端-&gt;客户端拿到数据去继续自己的任务 个人理解:除非svip,一般人没这待遇. 非阻塞:客户端发来请求(在收到服务器请求之前没必要非得等着,可以去干其他活)-&gt;服务器端收到请求-&gt;服务器端读取数据-&gt;客户端不停的问:我要的数据准备好了吗?服务器:还没呢,等一会.客户端不停的问:我要的数据准备好了吗?服务器:还没呢,等一会.客户端不停的问:我要的数据准备好了吗?服务器:准备好了,你拿走吧-&gt;返回给客户端-&gt;客户端拿到数据去继续自己的任务 个人理解:服务器的内心独白:客户端你是sb么?问个没完你不累? 复用模型selector:客户端发来请求(在收到服务器请求之前没必要非得等着,可以去干其他活)-&gt;服务器端收到请求-&gt;服务器端找个selector给你-&gt;客户端不停的问selector:我要的数据准备好了吗?selector去服务器申请干活并回答:还没呢,等一会,客户端不停的问selector:我要的数据准备好了吗?selector去服务器申请干活并回答:还没呢,等一会,客户端不停的问selector:我要的数据准备好了吗?selector:准备好了,你拿走吧-&gt;返回给客户端-&gt;客户端拿到数据去继续自己的任务-&gt;返回给客户端-&gt;客户端拿到数据去继续自己的任务 个人理解:除了抽象出一个selector,剩下与非阻塞没什么区别,在非阻塞中是每次都新拉起一个线程去回答客户端,这次是由线程池管理服务器端的线程,仅此而已 复用模型epoll:客户端发来请求(在收到服务器请求之前没必要非得等着,可以去干其他活)-&gt;服务器端收到请求-&gt;服务器端找个epoll给你-&gt;epoll说了:我懂你要干什么了客户端,你回家吧,请你闭嘴-&gt;服务器端开始读取数据-&gt;读取好了之后返回数据给客户端- 个人理解:客户端不必非得一直问一直问,相当于减轻了客户端的压力,变相的提升了服务器端的压力 对于一次IO访问，数据会先被拷贝到内核的缓冲区中，然后才会从内核的缓冲区拷贝到应用程序的地址空间。需要经历两个阶段： 准备数据 将数据从内核缓冲区拷贝到进程地址空间 由于存在这两个阶段，Linux产生了下面五种IO模型（以socket为例） 阻塞式IO: 当用户进程调用了recvfrom等阻塞方法时，内核进入IO的第1个阶段：准备数据（内核需要等待足够的数据再拷贝）这个过程需要等待，用户进程会被阻塞，等内核将数据准备好，然后拷贝到用户地址空间，内核返回结果，用户进程才从阻塞态进入就绪态 Linux中默认情况下所有的socket都是阻塞的 非阻塞式IO: 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。 用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作 一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回 非阻塞IO模式下用户进程需要不断地询问内核的数据准备好了没有 IO多路复用: 通过一种机制，一个进程可以监视多个文件描述符（套接字描述符）一旦某个文件描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作（这样就不需要每个用户进程不断的询问内核数据准备好了没） 常用的IO多路复用方式有select、poll和epoll 信号驱动IO: 内核文件描述符就绪后，通过信号通知用户进程，用户进程再通过系统调用读取数据。 此方式属于同步IO（实际读取数据到用户进程缓存的工作仍然是由用户进程自己负责的） 异步IO: 用户进程发起read操作之后，立刻就可以开始去做其它的事。内核收到一个异步IO read之后，会立刻返回，不会阻塞用户进程。 内核会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，内核会给用户进程发送一个signal告诉它read操作完成了 拓展:为啥出现IO多路复用 如果一个I/O流进来，我们就开启一个进程处理这个I/O流。那么假设现在有一百万个I/O流进来，那我们就需要开启一百万个进程一一对应处理这些I/O流（这就是传统意义下的多进程并发处理）。思考一下，一百万个进程，你的CPU占有率会多高，这个实现方式及其的不合理。所以人们提出了I/O多路复用这个模型，一个线程，通过记录I/O流的状态来同时管理多个I/O，可以提高服务器的吞吐能力。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程和高并发初识]]></title>
    <url>%2F2019%2F09%2F03%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E9%AB%98%E5%B9%B6%E5%8F%91%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[多线程和高并发线程的概念线程是一个程序内部的顺序控制流 线程和进程的区别 每个进程都有独立的代码和数据空间(进程上下文),进程间的切换会有较大的开销 线程可以看成是轻量级的进程,同一类线程共享代码和数据空间,每个线程有独立的运行栈和程序计数器,线程切换的开销小 多进程:在操作系统中能同时运行多个任务 多线程:在同一个应用程序中有多个顺序流同时执行 java的线程是通过java.lang.Thread类来实现的 VM启动时会有一个主方法main所定义的线程 可以通过创建Thread实例来创建新的线程 每个线程都是通过某个特定Thread对象所对应的run方法来完成其操作的,方法run称之为线程体 通过调用Thread类的start方法来启动一个线程 启动一个线程的两种方法 继承Thread类 实现Runnable接口 start和run调用run,main线程等run执行完再执行剩余代码 调用start,main线程同时执行start和剩余代码 sleep方法使线程休眠,暂停停止执行某某毫秒,时间到了之后自动复活,sleep完之后会回到就绪状态 由于是静态方法,可以直接使用类名调用 yield方法进入等待队列中,相当于优雅的暂时退出,让出cpu,给其他线程执行的机会 让出一下cpu,不过cpu会抢线程,也有可能把这个线程又抢回来执行,并不能保证其他线程一定会立刻执行 join方法合并某个线程 t1线程执行,t2线程执行调用t1的join方法,表示t2告诉t1麻烦去t1执行,t2在等着,等t1执行完之后t2再继续执行 一般用法是等待另一个线程的执行结束 synchronized关键字多个线程去访问同一个对象,必须给这个对象上锁 锁方法相当于锁当前对象 static方法由于没有this对象,如果加上synchronized关键字,代表synchronized T.class 对某个对象加锁 private int count = 10; private Object o = new Object(); public void m() { synchronized(o) { //任何线程要执行下面的代码，必须先拿到o的锁 count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } } 对某个对象加锁 private int count = 10; public void m() { synchronized(this) { //任何线程要执行下面的代码，必须先拿到this的锁 count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } } 对某个对象加锁 private int count = 10; public synchronized void m() { //等同于在方法的代码执行时要synchronized(this) count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } 对某个对象加锁 private int count = 10; public synchronized void m() { //等同于在方法的代码执行时要synchronized(this) count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } public void n() { //访问这个方法的时候不需要上锁 count++; } 对某个对象加锁 public class T implements Runnable { private /*volatile*/ int count = 100; public /*synchronized*/ void run() { count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } public static void main(String[] args) { T t = new T(); for(int i=0; i&lt;100; i++) { new Thread(t, &quot;THREAD&quot; + i).start(); } } } private static int count = 10; public synchronized static void m() { //这里等同于synchronized(T.class) count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } public static void mm() { synchronized(T.class) { //考虑一下这里写synchronized(this)是否可以？ count --; } } 线程不安全 private /*volatile*/ int count = 100; public /*synchronized*/ void run() { count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } public static void main(String[] args) { T t = new T(); for(int i=0; i&lt;100; i++) { new Thread(t, &quot;THREAD&quot; + i).start(); } } 安全 private int count = 10; public synchronized void run() { count--; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); } public static void main(String[] args) { for(int i=0; i&lt;5; i++) { T t = new T(); new Thread(t, &quot;THREAD&quot; + i).start(); } } 同步方法和非同步方法可以同时执行 public synchronized void m1() { System.out.println(Thread.currentThread().getName() + &quot; m1 start...&quot;); try { Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot; m1 end&quot;); } public void m2() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot; m2 &quot;); } public static void main(String[] args) { T t = new T(); /* new Thread(()-&gt;t.m1(), &quot;t1&quot;).start(); new Thread(()-&gt;t.m2(), &quot;t2&quot;).start(); */ new Thread(t::m1, &quot;t1&quot;).start(); new Thread(t::m2, &quot;t2&quot;).start(); /* //1.8之前的写法 new Thread(new Runnable() { @Override public void run() { t.m1(); } }); */ } dirty read public class Account { String name; double balance; public synchronized void set(String name, double balance) { this.name = name; try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } this.balance = balance; } public /*synchronized*/ double getBalance(String name) { return this.balance; } public static void main(String[] args) { Account a = new Account(); new Thread(()-&gt;a.set(&quot;zhangsan&quot;, 100.0)).start(); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(a.getBalance(&quot;zhangsan&quot;)); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(a.getBalance(&quot;zhangsan&quot;)); } } 把方法的注释synchronized打开就可以解决脏读问题,但是加锁的效率比不加锁低100倍,试实际情况而定 一个同步方法可以调用另外一个同步方法，一个线程已经拥有某个对象的锁，再次申请的时候仍然会得到该对象的锁 也就是说synchronized获得的锁是可重入的 synchronized void m1() { System.out.println(&quot;m1 start&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } m2(); System.out.println(&quot;m1 end&quot;); } synchronized void m2() { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;m2&quot;); } public static void main(String[] args) { new T().m1(); } 可重入锁示例,子类调用父类的同一把锁 public class T { synchronized void m() { System.out.println(&quot;m start&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;m end&quot;); } public static void main(String[] args) { new TT().m(); } } class TT extends T { @Override synchronized void m() { System.out.println(&quot;child m start&quot;); super.m(); System.out.println(&quot;child m end&quot;); } } 程序在执行过程中，如果出现异常，默认情况锁会被释放 所以，在并发处理的过程中，有异常要多加小心，不然可能会发生不一致的情况。 比如，在一个web app处理过程中，多个servlet线程共同访问同一个资源，这时如果异常处理不合适， 在第一个线程中抛出异常，其他线程就会进入同步代码区，有可能会访问到异常产生时的数据。 因此要非常小心的处理同步业务逻辑中的异常 public class T { int count = 0; synchronized void m() { System.out.println(Thread.currentThread().getName() + &quot; start&quot;); while(true) { count ++; System.out.println(Thread.currentThread().getName() + &quot; count = &quot; + count); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } if(count == 5) { int i = 1/0; //此处抛出异常，锁将被释放，要想不被释放，可以在这里进行catch，然后让循环继续 System.out.println(i); } } } public static void main(String[] args) { T t = new T(); Runnable r = new Runnable() { @Override public void run() { t.m(); } }; new Thread(r, &quot;t1&quot;).start(); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } new Thread(r, &quot;t2&quot;).start(); } } volatile 关键字，使一个变量在多个线程间可见 A B线程都用到一个变量，java默认是A线程中保留一份copy，这样如果B线程修改了该变量，则A线程未必知道 使用volatile关键字，会让所有线程都会读到变量的修改值 在下面的代码中，running是存在于堆内存的t对象中 当线程t1开始运行的时候，会把running值从内存中读到t1线程的工作区，在运行过程中直接使用这个copy，并不会每次都去 读取堆内存，这样，当主线程修改running的值之后，t1线程感知不到，所以不会停止运行 使用volatile，将会强制所有线程都去堆内存中读取running的值 可以阅读这篇文章进行更深入的理解 http://www.cnblogs.com/nexiyi/p/java_memory_model_and_thread.html volatile并不能保证多个线程共同修改running变量时所带来的不一致问题，也就是说volatile不能替代synchronized public class T { /*volatile*/ boolean running = true; //对比一下有无volatile的情况下，整个程序运行结果的区别 void m() { System.out.println(&quot;m start&quot;); while(running) { /* try { TimeUnit.MILLISECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); }*/ } System.out.println(&quot;m end!&quot;); } public static void main(String[] args) { T t = new T(); new Thread(t::m, &quot;t1&quot;).start(); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } t.running = false; } } public class TestVolatile { private static boolean ready; private static int number; private static class ReaderThread extends Thread { @Override public void run() { while(!ready) { Thread.yield(); } System.out.println(number); } } public static void main(String[] args) { new ReaderThread().start(); number = 42; ready = true; } } volatile并不能保证多个线程共同修改running变量时所带来的不一致问题，也就是说volatile不能替代synchronized 运行下面的程序，并分析结果 public class T { volatile int count = 0; void m() { for(int i=0; i&lt;10000; i++) count++; } public static void main(String[] args) { T t = new T(); List&lt;Thread&gt; threads = new ArrayList&lt;Thread&gt;(); for(int i=0; i&lt;10; i++) { threads.add(new Thread(t::m, &quot;thread-&quot;+i)); } threads.forEach((o)-&gt;o.start()); threads.forEach((o)-&gt;{ try { o.join(); } catch (InterruptedException e) { e.printStackTrace(); } }); System.out.println(t.count); } } 对比上一个程序，可以用synchronized解决，synchronized可以保证可见性和原子性，volatile只能保证可见性 public class T { /*volatile*/ int count = 0; synchronized void m() { for (int i = 0; i &lt; 10000; i++) count++; } public static void main(String[] args) { T t = new T(); List&lt;Thread&gt; threads = new ArrayList&lt;Thread&gt;(); for (int i = 0; i &lt; 10; i++) { threads.add(new Thread(t::m, &quot;thread-&quot; + i)); } threads.forEach((o) -&gt; o.start()); threads.forEach((o) -&gt; { try { o.join(); } catch (InterruptedException e) { e.printStackTrace(); } }); System.out.println(t.count); } } 解决同样的问题的更高效的方法，使用AtomXXX类 AtomXXX类本身方法都是原子性的，但不能保证多个方法连续调用是原子性的 public class T { /*volatile*/ //int count = 0; AtomicInteger count = new AtomicInteger(0); /*synchronized*/ void m() { for (int i = 0; i &lt; 10000; i++) //if count.get() &lt; 1000 count.incrementAndGet(); //count++ } public static void main(String[] args) { T t = new T(); List&lt;Thread&gt; threads = new ArrayList&lt;Thread&gt;(); for (int i = 0; i &lt; 10; i++) { threads.add(new Thread(t::m, &quot;thread-&quot; + i)); } threads.forEach((o) -&gt; o.start()); threads.forEach((o) -&gt; { try { o.join(); } catch (InterruptedException e) { e.printStackTrace(); } }); System.out.println(t.count); } } public class AtomicVsSync { static long count2 = 0L; static AtomicLong count = new AtomicLong(0L); public static void main(String[] args) throws Exception { Thread[] threads = new Thread[500]; for(int i=0; i&lt;threads.length; i++) { threads[i] = new Thread(()-&gt; { count.incrementAndGet(); }); } long start = System.currentTimeMillis(); for(Thread t : threads ) t.start(); for (Thread t : threads) t.join(); long end = System.currentTimeMillis(); TimeUnit.SECONDS.sleep(10); System.out.println(&quot;Atomic: &quot; + count.get() + &quot; time &quot; + (end-start)); //----------------------------------------------------------- Object lock = new Object(); for(int i=0; i&lt;threads.length; i++) { threads[i] = new Thread(new Runnable() { @Override public void run() { synchronized (lock) { count2++; } } }); } start = System.currentTimeMillis(); for(Thread t : threads ) t.start(); for (Thread t : threads) t.join(); end = System.currentTimeMillis(); System.out.println(&quot;Sync: &quot; + count2 + &quot; time &quot; + (end-start)); } static void microSleep(int m) { try { TimeUnit.MICROSECONDS.sleep(m); } catch (InterruptedException e) { e.printStackTrace(); } } } synchronized优化 同步代码块中的语句越少越好 比较m1和m2 public class T { int count = 0; synchronized void m1() { //do sth need not sync try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } //业务逻辑中只有下面这句需要sync，这时不应该给整个方法上锁 count ++; //do sth need not sync try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } } void m2() { //do sth need not sync try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } //业务逻辑中只有下面这句需要sync，这时不应该给整个方法上锁 //采用细粒度的锁，可以使线程争用时间变短，从而提高效率 synchronized(this) { count ++; } //do sth need not sync try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } } } 锁定某对象o，如果o的属性发生改变，不影响锁的使用 但是如果o变成另外一个对象，则锁定的对象发生改变 应该避免将锁定对象的引用变成另外的对象 public class T { Object o = new Object(); void m() { synchronized(o) { while(true) { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()); } } } public static void main(String[] args) { T t = new T(); //启动第一个线程 new Thread(t::m, &quot;t1&quot;).start(); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } //创建第二个线程 Thread t2 = new Thread(t::m, &quot;t2&quot;); t.o = new Object(); //锁对象发生改变，所以t2线程得以执行，如果注释掉这句话，线程2将永远得不到执行机会 t2.start(); } } 不要以字符串常量作为锁定对象 在下面的例子中，m1和m2其实锁定的是同一个对象 这种情况还会发生比较诡异的现象，比如你用到了一个类库，在该类库中代码锁定了字符串“Hello”， 但是你读不到源码，所以你在自己的代码中也锁定了”Hello”,这时候就有可能发生非常诡异的死锁阻塞， 因为你的程序和你用到的类库不经意间使用了同一把锁 public class T { String s1 = &quot;Hello&quot;; String s2 = &quot;Hello&quot;; void m1() { synchronized(s1) { } } void m2() { synchronized(s2) { } } } ThreadLocal线程局部变量 public class ThreadLocal1 { volatile static Person p = new Person(); public static void main(String[] args) { new Thread(()-&gt;{ try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(p.name); }).start(); new Thread(()-&gt;{ try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } p.name = &quot;lisi&quot;; }).start(); } } ThreadLocal线程局部变量ThreadLocal是使用空间换时间，synchronized是使用时间换空间 比如在hibernate中session就存在与ThreadLocal中，避免synchronized的使用运行下面的程序，理解ThreadLocal public class ThreadLocal2 { //volatile static Person p = new Person(); static ThreadLocal&lt;Person&gt; tl = new ThreadLocal&lt;&gt;(); public static void main(String[] args) { new Thread(()-&gt;{ try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(tl.get()); }).start(); new Thread(()-&gt;{ try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } tl.set(new Person()); }).start(); } static class Person { String name = &quot;zhangsan&quot;; } } 线程安全的单例模式： 阅读文章：http://www.cnblogs.com/xudong-bupt/p/3433643.html 更好的是采用下面的方式，既不用加锁，也能实现懒加载 public class Singleton { private Singleton() { System.out.println(&quot;single&quot;); } private static class Inner { private static Singleton s = new Singleton(); } public static Singleton getSingle() { return Inner.s; } public static void main(String[] args) { Thread[] ths = new Thread[200]; for(int i=0; i&lt;ths.length; i++) { ths[i] = new Thread(()-&gt;{ System.out.println(Singleton.getSingle()); }); } Arrays.asList(ths).forEach(o-&gt;o.start()); } } 同步容器类 1：Vector Hashtable ：早期使用synchronized实现 2：ArrayList HashSet ：未考虑多线程安全（未实现同步） 3：HashSet vs Hashtable StringBuilder vs StringBuffer 4：Collections.synchronized工厂方法使用的也是synchronized 使用早期的同步容器以及Collections.synchronized方法的不足之处，请阅读：http://blog.csdn.net/itm_hadf/article/details/7506529 使用新的并发容器http://xuganggogo.iteye.com/blog/321630 总结：1：对于map/set的选择使用 HashMap TreeMap LinkedHashMap Hashtable Collections.sychronizedXXX ConcurrentHashMap ConcurrentSkipListMap 2：队列 ArrayList LinkedList Collections.synchronizedXXX CopyOnWriteList Queue CocurrentLinkedQueue //concurrentArrayQueue BlockingQueue LinkedBQ ArrayBQ TransferQueue SynchronusQueue DelayQueue执行定时任务]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis初识]]></title>
    <url>%2F2019%2F09%2F03%2FRedis%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Redis课前基础知识磁盘基础知识 1.寻址:m/s 2.带宽:G/M 内存 1.寻址:ns 2.带宽:很大 秒&gt;毫秒&gt;微秒&gt;纳秒 内存寻址比硬盘寻址快10万倍 IO buffer 成本问题, 磁盘有磁道和扇区,一扇区512byte,带来成本变大:索引 4K对齐,操作系统无论读多少都是最少4k从磁盘拿 随着文件变大,速度变慢,IO成为瓶颈 数据库 data page 4k 关系型数据库建表:必须给出schema 类型:建表宽度 存:倾向于行级存储 内存维护一个b+树 磁盘:存储数据和索引 表很大的时候,性能下降? 如果有索引,增删改变慢 查询速度 1.如果一个或少量查询,性能依然快,因为where条件走的依然是一个data page 2.当并发量来了,可能不是加载一个data page到内存,受硬盘带宽影响,有可能性能下降(IOPS) 折中做法将内存中的一部分数据做缓存 memcached key value结构,value没有类型的概念 redis(秒级十万操作) 同为key value结构,value有类型,string(字符,数值,bitmaps),hashes,lists,sets,sorted sets 安装步骤make:在linux中属于编译命令,跟随文件makefile,但是这个命令需要config执行之后才会生成 vim /etc/profile #添加REDIS_HOME和PATH source /etc/profile cd $REDIS_HOME make #注意,需要提前安装g++,如果没装g++安装过程会报错,安装g++后记得make clean cd utils sh install_server.sh #期间需要自行指定一些参数,记得改,尽量不要用默认的var #这之后redis就可以作为服务进行启动了,执行文件为redis_端口号 处理流程linux系统中存在kernel(内核),eppoll是一种系统调用,redis是单进程单线程单实例并发很多的请求,如何变得很快的呢? eppoll概念 文件描述符从block(阻塞的)发展到noblock(非阻塞的),这时为同步非阻塞时期 此时发展出一个问题,用户进程轮询调用1000次kernel,成本问题 这时发展出了select概念 统一把这一千个文件描述符传给select,多路复用nio,减少了内核态和用户态的切换 这些所有的功能都是由jvm实现的,jvm由c编写 关于linux用户态和内核态问题:文件描述符相关数据拷来拷去成为累赘 内核有内核的空间,用户有用户的空间,所以伸展出一个内核和用户的共享空间 共享空间由红黑树+链表+mmap实现 man epoll epoll - I/O event notification facility(I/O事件通知功能)epoll is a variant of poll(2) that can be used either as an edge-triggered or a level-triggered interface and scales well to large numbers of watched file descriptors. The following system calls are provided to create and manage an epoll instance:(epoll是poll(2)的一个变体，它既可以用作边缘触发接口，也可以用作级别触发接口，并且可以很好地扩展到大量被监视的文件描述符。提供以下系统调用来创建和管理epoll实例:) An epoll instance created by epoll_create(2), which returns a file descriptor referring to the epoll instance. (The more recent epoll_create1(2) extends the functionality of epoll_create(2).)(由epoll_create(2)创建的epoll实例，它返回引用epoll实例的文件描述符。(最近的epoll_create1(2)扩展了epoll_create(2)的功能。) Interest in particular file descriptors is then registered via epoll_ctl(2). The set of file descriptors currently registered on an epoll instance is sometimes called an epoll set.(然后通过epoll_ctl(2)注册特定文件描述符。当前注册在epoll实例上的文件描述符集有时称为epoll集。) Finally, the actual wait is started by epoll_wait(2).(最后，实际的等待由epoll_wait(2)启动。) man 2 sendfile sendfile() copies data between one file descriptor and another. Because this copying is done within the kernel, sendfile() is more efficient than the combination of read(2) and write(2), which would require transferring data to and from user space.sendfile()在一个文件描述符和另一个文件描述符之间复制数据。因为这种复制是在内核中完成的，所以sendfile()比read(2)和write(2)的组合更有效，后者需要在用户空间和用户空间之间传输数据. in_fd should be a file descriptor opened for reading and out_fd should be a descriptor opened for writing.in_fd应该是一个打开用于读取的文件描述符，out_fd应该是一个打开用于写入的描述符。 If offset is not NULL, then it points to a variable holding the file offset from which sendfile() will start reading data from in_fd. When sendfile() returns, this variable will be set to the offset of the byte following the last byte that was read. If offset is not NULL, then sendfile() does not modify the current file offset of in_fd; otherwise the current file offset is adjusted to reflect the number of bytes read from in_fd.如果偏移量不为空，那么它指向一个保存文件偏移量的变量，sendfile()将从该变量开始从in_fd读取数据。当sendfile()返回时，该变量将被设置为读取的最后一个字节之后的字节的偏移量。如果偏移量不为空，则sendfile()不修改in_fd的当前文件偏移量;否则，将调整当前文件偏移量，以反映从in_fd读取的字节数。 count is the number of bytes to copy between the file descriptors.count是要在文件描述符之间复制的字节数。 Presently (Linux 2.6.9): in_fd, must correspond to a file which supports mmap(2)-like operations (i.e., it cannot be a socket); and out_fd must refer to a socket.当前(Linux 2.6.9): in_fd，必须对应于支持mmap(2)类操作的文件(即，不能是socket);out_fd必须引用socket。 Applications may wish to fall back to read(2)/write(2) in the case where sendfile() fails with EINVAL or ENOSYS.当sendfile()在EINVAL或ENOSYS中失败时，应用程序可能希望返回到read(2)/write(2)。 实际操作stringredis-cli help redis默认共有16个库,可以在配置文件中修改 set k380 hello get k380 help @generic keys * #清库,慎用,my friend flushdb flushall help@string #不存在的时候才去设置,一旦有设置,则新的设置无效(只能新建) set k1 ooxx nx #存在的时候才去设置,一旦没有设置,则新的设置无效(只能更新) set k2 hello xx #批量 mset k3 a k4 b mget k3 k4 append k1 world #正反向索引 getrange k1 5 -1 setrange k1 5 &quot; xiaowu&quot; strlen k1 #查看k1的value类型 type k1 set k2 99 type k2 object help #面向字符串可以有计算类型,这里的int指的是k2的value的编码类型而不是type object encoding k2 #加减 incrby k2 12 decr k2 decrby k2 22 incrbyfloat k2 0.5 #2中类型,embstr,row set k3 jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj object encoding k3 append k3 jjjjjjj object encoding k3 flushall #二进制安全演示 set k1 hello strlen k1 set k2 9 object encoding k2 strlen k2 append k2 999 get k2 object encoding k2 incr k2 object encoding k2 strlen k2 #utf-8和gbk编码 set k3 a get k3 strlen k3 append k3 中 get k3 strlen k3 #触发格式化操作,&quot;中&quot;字可以正常显示 redis-cli --raw #原子性操作 msetnx k1 a k2 b mget k1 k2 msetnx k2 c k3 d mget k1 k2 k3 flushall #一个字节8位 help setbit setbit k1 1 1 strlen k1 setbit k1 7 1 #01000001 setbit k1 9 1 strlen k1 get k1 #就算是只找第二个字节,依然会返回全量字节的索引下标 help bitpos bitpos k1 1 0 0 bitpos k1 1 1 1 bitpos k1 1 0 1 #统计次数 bitcount k1 1 0 1 flushall #位操作,与或非 setbit k1 1 1 setbit k1 7 1 get k1 setbit k2 1 1 setbit k2 6 1 #与操作 有0则0,全1为1 #01000000 bittop and andkey k1 k2 #或操作 有1则1,全0为0 #01000011 bittop or orkey k1 k2 listhelp @list #向k1中添加 lpush k1 a b c d e f #l代表左边,插入的结构实际为f e d c b a rpush k2 a b c d e f #r代表右边,插入的结构实际为a b c d e f #弹出元素 lpop k1 #这个东西与栈类似 #弹出元素 rpop k1 #栈:同向命令 #队列:反向命令 #查看 lrange k1 0 -1 #根据下标取 lindex k1 2 lindex k1 -1 #根据下标设置 lset k1 3 xxxx #删除 lrem k3 2 a #2为正数代表正向删除,为负数代表反向删除 linsert k3 after 6 a #在元素6之后插入,而不是下标了 #如果有两个相同元素,会在第一个元素6之后插入,而不是两个6全插入 linsert k3 before 3 a #反向删除 lrem k3 -2 a #统计数量 llen k3 #开三个客户端测试阻塞 blpop ooxx 0 #两个客户端阻塞,一个客户端新增数据,有机会测一下 #两端删除 ltrim k4 0 -1 ltrim k4 2 -2 #从下标2开始到下标-2留下,也就是说下标0,1,-1删除 hashmapset sean::name &#39;minato aqua&#39; get sean::name set sean::age 18 get sean::age keys sean* hset sean name &#39;minato aqua&#39; hmset sean age 18 address hovelive hget sean name hget sean address hmget sean name age hkeys sean hvals sean hgetall sean hincrbyfloat sean age 0.5 hincrbyfloat sean age -1 #hashmap应用场景:收藏,详情页,点赞,修改数据等等 set#set特点:无序+去重 help @set sadd k1 tom sean peter ooxx tom xxoo smembers k1 srem k1 xxoo ooxx smembers k1 sadd k2 1 2 3 4 5 sadd k3 4 5 6 7 8 sinter k2 k3 #插入到dest里头 sinterstore dest k2 k3 sunion k2 k3 #同上,存在rstore,并集 sdiff k2 k3 sdiff k3 k2 #外差集,两个结果并不一样,右侧为参考基准 srandmember k1 5 -5 10 -10 #为正数的时候,取出一个去重的结果集(不能超过已有结果集) #为负数的时候,取出一个带重复的结果集(一定会满足你要的数量) #为0,不返回结果 spop k1 sorted set#想让它们怎么排序?名称?大小?价格? help @sorted_set zadd k1 8 apple 2 banana 3 orange #物理内存左小右大,不随命令发生变化 #查看 zrange k1 0 -1 #查看分值 zrange k1 0 -1 withscores #取分值范围 zrangebyscore k1 3 8 #正向取 zrange k1 0 1 #反向取 zrevrange k1 0 1 #通过值取分值 zscore k1 apple #取出排名 zrank k1 apple #查看所有 zrange k1 0 -1 withscores #增加 zinsrby k1 2.5 banana #实时根据修改更新排序 zrange k1 0 -1 withscores #集合取并集 zadd k1 80 tom 60 sean 70 baby zadd k2 60 tom 100 sean 40 yiming zunionstore unkey 2 k1 k2 zrange unkey 0 -1 withscores #设置权重 zunionstore unkey1 2 k1 k2 weights 1 0.5 #k1里面的不变,k2里面的乘0.5 zrange unkey1 0 -1 withscores #取最大的 zunionstore unkey2 2 k1 k2 aggregate max zrange unkey2 0 -1 withscores 排序是怎么实现的怎删改查的速度? 引申出知识点:跳跃表(skip list),平衡树 缓存和数据库管道技术一个请求/相应服务可以实现为，即使客户端没有读取到旧请求的响应，服务端依旧可以处理新请求。通过这种方式，可以完全无需等待服务端应答地发送多条指令给服务端，并最终一次性读取所有应答。管道技术最显著的优势是提高了redis服务的性能。 yum install nc nc localhost 6379 keys * echo -e &quot;set k2 99\nincr k2\n get k2&quot; | nc localhost 6379 Pub/Sub订阅，取消订阅和发布实现了发布/订阅消息范式(引自wikipedia)，发送者（发布者）不是计划发送消息给特定的接收者（订阅者）。而是发布的消息分到不同的频道，不需要知道什么样的订阅者订阅。订阅者对一个或多个频道感兴趣，只需接收感兴趣的消息，不需要知道什么样的发布者发布的。这种发布者和订阅者的解耦合可以带来更大的扩展性和更加动态的网络拓扑。 Redis可以执行发布/订阅模式(publish/subscribe), 该模式可以解耦消息的发送者和接收者,使程序具有更好的扩展性.从宏观上来讲,Redis的发布/订阅模式具有如下特点: 客户端执行订阅以后,除了可以继续订阅(SUBSCRIBE或者PSUBSCRIBE),取消订阅(UNSUBSCRIBE或者PUNSUBSCRIBE), PING命令和结束连接(QUIT)外, 不能执行其他操作,客户端将阻塞直到订阅通道上发布消息的到来. 发布的消息在Redis系统中不存储.因此,必须先执行订阅,再等待消息发布. 但是,相反的顺序则不支持. 订阅的通道名称支持glob模式匹配.如果客户端同时订阅了glob模式的通道和非glob模式的通道,并且名称存在交集,则对于一个发布的消息,该执行订阅的客户端接收到两个消息. pub/sub APIRedis的发布/订阅设计模式相关的命令有六个: SUBSCRIBE命令执行订阅.客户端可以多次执行该命令, 也可以一次订阅多个通道. 多个客户端可以订阅相同的通道.该命令的响应包括三部分, 依次是:命令名称(字符串subscribe),订阅的通道名称,总共订阅的通道数(包含glob通道). PSUBSCRIBE命令执行glob模式订阅.客户端可以多次执行该命令, 也可以一次订阅多个glob通道. 多个客户端可以订阅相同的glob通道.该命令的响应包括三部分, 依次是:命令名称(字符串psubscribe),订阅的glob通道名称,总共订阅的通道数(包含非glob通道). UNSUBSCRIBE命令取消订阅指定的通道.可以指定一个或者多个取消的订阅通道名称,也可以不带任何参数,此时将取消所有的订阅的通道(不包括glob通道).该命令的响应包括三部分, 依次是:命令名称(字符串unsubscribe),取消的订阅通道名称,总共订阅的通道数(包含glob通道). PUNSUBSCRIBE命令取消订阅指定的glob模式通道.可以指定一个或者多个取消的glob模式的订阅通道名称,也可以不带任何参数,此时将取消所有的glob模式订阅的通道(不包括非glob通道).该命令的响应包括三部分, 依次是:命令名称(字符串punsubscribe),取消的glob模式的订阅通道名称,总共订阅的通道数(包含非glob通道). PUBLISH命令在指定的通道上发布消息.只能在一个通道上发布消息,不能在多个通道上同时发布消息.该命令的响应包括通知的接收者个数,需要注意的是,这里的接收者数目大于等于订阅该通道的客户端数目(因为一个客户端的glob通道和非glob通道同时匹配发布通道的话,则视为两个接收者).而在接收端,收到的响应包括三部分,依次是 :message或者pmessage字符串(取决于是否为glob匹配),匹配的通道名称,发布的消息内容. PUBSUB命令执行状态查询.支持若干子命令.需要注意的是,该命令不能在客户端进入订阅后执行. #消息通道(广播) help @pubsub #客户端执行 SUBSCRIBE aqua #服务端执行 PUBLISH aqua &quot;i&#39;m minato aqua&quot; #消费端链接以后才能正常接收消息,未连接客户端之前的消息不会被接收到 publish aqua hello #列出当前的活跃频道 PUBSUB channels #返回给定频道的订阅者数量,订阅模式的客户端不计算在内 PUBSUB numsub aqua mea #返回订阅模式的数量 client1:PSUBSCRIBE aqua mea client2:PSUBSCRIBE mea aqua client3:pubsub numpat 小结: 只有在订阅客户端上,才能取消订阅. 一个客户端连接不能为另一个客户端连接取消订阅. 这是显而易见的. 在发布/订阅模式下,通道名称是全局的,和客户端连接的Redis数据库没有关系. 缓存的数据(热数据)如何随着业务进行变化? 引出知识点: key的有效期 设置key的过期时间，超过时间后，将会自动删除该key。在Redis的术语中一个key的相关超时是不确定的 超时后只有对key执行DEL命令或者SET命令或者GETSET时才会清除。这意味着，从概念上讲所有改变key的值的操作都会使他清除。 例如，INCR递增key的值，执行LPUSH操作，或者用HSET改变hash的field所有这些操作都会触发删除动作 使用PERSIST命令可以清除超时，使其变成一个永久的key 如果key被RENAME命令修改，相关的超时时间会转移到新key上面 如果key被RENAME命令修改，比如原来就存在Key_A,然后调用RENAME Key_B Key_A命令，这时不管原来Key_A是永久的还是设置为超时的，都会由Key_B的有效期状态覆盖 对已经有过期时间的key执行EXPIRE操作，将会更新它的过期时间。有很多应用有这种业务场景，例如记录会话的session 返回值 integer-reply, 具体的: 1 如果成功设置过期时间。 0 如果key不存在或者不能设置过期时间。 set aqua minatoaqua get aqua expire aqua 10 ttl aqua ttl aqua set aqua &quot;i&#39;m minato aqua&quot; ttl aqua Keys的过期时间通常Redis keys创建时没有设置相关过期时间。他们会一直存在，除非使用显示的命令移除，例如，使用DEL命令 EXPIRE一类命令能关联到一个有额外内存开销的key。当key执行过期操作时，Redis会确保按照规定时间删除他们 key的过期时间和永久有效性可以通过EXPIRE和PERSIST命令（或者其他相关命令）来进行更新或者删除过期时间 过期精度在 Redis 2.4 及以前版本，过期期时间可能不是十分准确，有0-1秒的误差 从 Redis 2.6 起，过期时间误差缩小到0-1毫秒 过期和持久Keys的过期时间使用Unix时间戳存储(从Redis 2.6开始以毫秒为单位)。这意味着即使Redis实例不可用，时间也是一直在流逝的 要想过期的工作处理好，计算机必须采用稳定的时间。 如果你将RDB文件在两台时钟不同步的电脑间同步，有趣的事会发生(所有的 keys装载时就会过期) 即使正在运行的实例也会检查计算机的时钟，例如如果你设置了一个key的有效期是1000秒，然后设置你的计算机时间为未来2000秒，这时key会立即失效，而不是等1000秒之后 如何淘汰过期的keysRedis keys过期有两种方式：被动和主动方式 当一些客户端尝试访问它时，key会被发现并主动的过期 当然，这样是不够的，因为有些过期的keys，永远不会访问他们。 无论如何，这些keys应该过期，所以定时随机测试设置keys的过期时间。所有这些过期的keys将会从密钥空间删除 具体就是Redis每秒10次做的事情： 测试随机的20个keys进行相关过期检测。 删除所有已经过期的keys。 如果有多于25%的keys过期，重复步奏1. 这是一个平凡的概率算法，基本上的假设是，我们的样本是这个密钥控件，并且我们不断重复过期检测，直到过期的keys的百分百低于25%,这意味着，在任何给定的时刻，最多会清除1/4的过期keys 在复制AOF文件时如何处理过期为了获得正确的行为而不牺牲一致性，当一个key过期，DEL将会随着AOF文字一起合成到所有附加的slaves。在master实例中，这种方法是集中的，并且不存在一致性错误的机会 然而，当slaves连接到master时，不会独立过期keys（会等到master执行DEL命令），他们任然会在数据集里面存在，所以当slave当选为master时淘汰keys会独立执行，然后成为master 事物redis是单进程的,所以事物回滚存在一些与关系型数据库的差别 multi,exec,discard,watch(乐观锁的效果) MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。 如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。 使用redis-check-aof程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。 用法MULTI 命令用于开启一个事务，它总是返回 OK 。 MULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC命令被调用时， 所有队列中的命令才会被执行。 另一方面， 通过调用 DISCARD ， 客户端可以清空事务队列， 并放弃执行事务。 multi incr aqua incr mea exec keys * EXEC 命令的回复是一个数组， 数组中的每个元素都是执行事务中的命令所产生的回复。 其中， 回复元素的先后顺序和命令发送的先后顺序一致。 当客户端处于事务状态时， 所有传入的命令都会返回一个内容为 QUEUED 的状态回复（status reply）， 这些被入队的命令将在 EXEC 命令被调用时执行。 事务中的错误使用事务时可能会遇上以下两种错误： 事务在执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。 命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。 对于发生在 EXEC 执行之前的错误，客户端以前的做法是检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，就是入队失败。如果有命令在入队时失败，那么大部分客户端都会停止并取消这个事务。 不过，从 Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。 在 Redis 2.6.5 以前， Redis 只执行事务中那些入队成功的命令，而忽略那些入队失败的命令。 而新的处理方式则使得在流水线（pipeline）中包含事务变得简单，因为发送事务和读取事务的回复都只需要和服务器进行一次通讯。 至于那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。 从协议的角度来看这个问题，会更容易理解一些。 以下例子中， LPOP 命令的执行将出错， 尽管调用它的语法是正确的： multi set aqua minatoaqua lpop aqua exec EXEC 返回两条bulk-string-reply： 第一条是 OK ，而第二条是 -ERR 。 至于怎样用合适的方法来表示事务中的错误， 则是由客户端自己决定的。 最重要的是记住这样一条， 即使事务中有某条/某些命令执行失败了， 事务队列中的其他命令仍然会继续执行 —— Redis 不会停止执行事务中的命令。 以下例子展示的是另一种情况， 当命令在入队时产生错误， 错误会立即被返回给客户端： multi incr a b c 因为调用 INCR 命令的参数格式不正确， 所以这个 INCR 命令入队失败。 为什么 Redis 不支持回滚（roll back）如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。 以下是这种做法的优点： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR， 回滚是没有办法处理这些情况的。 放弃事务当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出： set aqua minatoaqua multi incr aqua discard get aqua 使用 check-and-set 操作实现乐观锁WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回nil-reply来表示事务已经失败。 举个例子， 假设我们需要原子性地为某个值进行增 1 操作（假设 INCR 不存在）。 首先我们可能会这样做： set aqua 17 mea = get aqua mea = val + 1 set aqua $mea 上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。 有了 WATCH ， 我们就可以轻松地解决这类问题了： watch aqua mea = get aqua mea = mea + 1 multi set aqua $mea exec 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。 这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。 WATCHWATCH 使得 EXEC 命令需要有条件地执行： 事务只能在所有被监视键都没有被修改的前提下执行， 如果这个前提不能满足的话，事务就不会被执行。 client1: set aqua 17 watch aqua MULTI get aqua set aqua 18 client2: set aqua 14 client1: exec #可以发现,事物并没有执行 WATCH 命令可以被调用多次。 对键的监视从 WATCH 执行之后开始生效， 直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键， 就像这样： watch aqua mea cierra 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。 另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。 使用 WATCH 实现 ZPOPWATCH 可以用于创建 Redis 没有内置的原子操作。举个例子， 以下代码实现了原创的 ZPOP 命令， 它可以原子地弹出有序集合中分值（score）最小的元素： WATCH zset element = ZRANGE zset 0 0 MULTI ZREM zset element EXEC 程序只要重复执行这段代码， 直到 EXEC 的返回值不是nil-reply回复即可。 Redis 脚本和事务从定义上来说， Redis 中的脚本本身就是一种事务， 所以任何在事务里可以完成的事， 在脚本里面也能完成。 并且一般来说， 使用脚本要来得更简单，并且速度更快。 因为脚本功能是 Redis 2.6 才引入的， 而事务功能则更早之前就存在了， 所以 Redis 才会同时存在两种处理事务的方法。 不过我们并不打算在短时间内就移除事务功能， 因为事务提供了一种即使不使用脚本， 也可以避免竞争条件的方法， 而且事务本身的实现并不复杂。 不过在不远的将来， 可能所有用户都会只使用脚本来实现事务也说不定。 如果真的发生这种情况的话， 那么我们将废弃并最终移除事务功能。 布隆过滤器主要命令 bf.add 添加元素 bf.exists 查询元素是否存在 bf.madd 一次添加多个元素 bf.mexists 一次查询多个元素是否存在 在 redis 中有两个值决定布隆过滤器的准确率： error_rate：允许布隆过滤器的错误率，这个值越低过滤器的位数组的大小越大，占用空间也就越大。 initial_size：布隆过滤器可以储存的元素个数，当实际存储的元素个数超过这个值之后，过滤器的准确率会下降。 redis 中有一个命令可以来设置这两个值： bf.reserve test 0.01 100 第一个值是过滤器的名字。 第二个值为 error_rate 的值。 第三个值为 initial_size 的值。 redis-server --loadmodule /opt/yupsoftware/redis-5.0.5/redisbloom.so bf.add hololive minatoaqua bf.exists hololive minatoaqua bf.exists hololive kaguramea bf.madd hololive shirakamifubuki kaguramea nakiriayame ozorasubaru bf.mexists hololive shirakamifubuki kaguramea nakiriayame ozorasubaru cierrarunis bf.reserve test 0.001 10000 注意必须在add之前使用bf.reserve指令显式创建，如果对应的 key 已经存在，bf.reserve会报错。同时设置的错误率越低，需要的空间越大。如果不使用 bf.reserve，默认的error_rate是 0.01，默认的initial_size是 100。 应用场景主要是解决大规模数据下不需要精确过滤的场景，如检查垃圾邮件地址，爬虫URL地址去重，解决缓存穿透问题等。 布谷鸟过滤器实现了布隆过滤器的删除功能]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于流年大佬]]></title>
    <url>%2F2019%2F08%2F25%2F%E5%85%B3%E4%BA%8E%E6%B5%81%E5%B9%B4%E5%A4%A7%E4%BD%AC%2F</url>
    <content type="text"><![CDATA[使用了流年大佬的图床,访问速度比以前提升了不少,非常感谢流年大佬的图床,么么哒~https://www.yremp.live喜欢的小伙伴们请关注流年大佬吧~https://yremp.livehttps://yremp.club近期可能会把github上的图片全部转存到流年大佬的图床上,毕竟白嫖流年大佬简直是太爽了.jpg]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tengine补充]]></title>
    <url>%2F2019%2F08%2F25%2FTengine%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[Tengine 补充开机启动chkconfig --list chkconfig --add nginx chkconfig nginx on 时间问题service ntpd status 虚拟目录location /www { alias /var/data/www1; index index.html index.htm a.html; } 自动索引location /art { alias /var/data/www1/; autoindex on; } 动静分离location / { proxy_pass http://192.168.150.11:803; } location ~ .*\.(gif|jpg|jpeg|png|bmp|swf|html|htm|css|js)$ { root /var/data/www1/; } SSL SSL 能够帮助系统在客户端和服务器之间建立一条安全通信通道。SSL 安全协议是由 Netscape Communication 公司在 1994 年设计开发，SSL 依赖于加密算法、极难窃听、有较高的安全性，因此 SSL 协议已经成为网络上最常用的安全保密通信协议，该安全协议主要用来提供对用户和服务器的认证；对传送的数据进行加密和隐藏；确保数据在传送中不被改变，即数据的完整性，现已成为该领域中全球化的标准。 SSL 和 TLS所有的X.509证书包含以下数据： 1、X.509版本号：指出该证书使用了哪种版本的X.509标准，版本号会影响证书中的一些特定信息。目前的版本是3。 2、证书持有人的公钥：包括证书持有人的公钥、算法(指明密钥属于哪种密码系统)的标识符和其他相关的密钥参数。 3、证书的序列号：由CA给予每一个证书分配的唯一的数字型编号，当证书被取消时，实际上是将此证书序列号放入由CA签发的CRL（Certificate Revocation List证书作废表，或证书黑名单表）中。这也是序列号唯一的原因。 4、主题信息：证书持有人唯一的标识符(或称DN-distinguished name)这个名字在 Internet上应该是唯一的。DN由许多部分组成，看起来象这样： CN=Bob Allen, OU=Total Network Security Division O=Network Associates, Inc. C=US 这些信息指出该科目的通用名、组织单位、组织和国家或者证书持有人的姓名、服务处所等信息。 5、证书的有效期：证书起始日期和时间以及终止日期和时间；指明证书在这两个时间内有效。 6、认证机构：证书发布者，是签发该证书的实体唯一的CA的X.509名字。使用该证书意味着信任签发证书的实体。(注意：在某些情况下，比如根或顶级CA证书，发布者自己签发证书) 7、发布者的数字签名：这是使用发布者私钥生成的签名，以确保这个证书在发放之后没有被撰改过。 8、签名算法标识符：用来指定CA签署证书时所使用的签名算法。算法标识符用来指定CA签发证书时所使用的公开密钥算法和HASH算法。 抓包工具青花瓷https://www.charlesproxy.com/latest-release/download.do 对称加密与非对称加密非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey），公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密。因为加密和解密使用的是两个不同的密钥，所以这种算法叫作非对称加密算法。 CA（Certificate Authority）CA 是负责签发证书、认证证书、管理已颁发证书的机关。它要制定政策和具体步骤来验证、识别用户身份，并对用户证书进行签名，以确保证书持有者的身份和公钥的拥有权。 常见的ca厂商 Symantec Comodo Godaddy GlobalSign Digicert VeriSign GeoTrust Thawte Network Solutions CA 供应商之间的区别主要有：机构品牌、证书加密方式、保险额度、服务与质量、浏览器支持率等 证书种类 DV（Domain Validation）证书只进行域名的验证，一般验证方式是提交申请之后CA会往你在whois信息里面注册的邮箱发送邮件，只需要按照邮件里面的内容进行验证即可。 OV（Organization Validation）证书在DV证书验证的基础上还需要进行公司的验证，一般他们会通过购买邓白氏等这类信息库来查询域名所属的公司以及这个公司的电话信息，通过拨打这个公司的电话来确认公司是否授权申请OV证书。 EV证书一般是在OV的基础上还需要公司的金融机构的开户许可证，不过不同CA的做法不一定一样，例如申请人是地方政府机构的时候是没有金融机构的开户证明的，这时候就会需要通过别的方式去鉴别申请人的实体信息。 多网站公用同一证书OPenSSL 自签名certmgr.msc key 私钥 = 明文 自己生成的 csr 公钥 = 由私钥生成 crt 证书 = 公钥 + 签名 下载 http://slproweb.com/products/Win32OpenSSL.html 生成私钥genrsa 制台输入 genrsa，会默认生成一个 2048 位的私钥 openssl genrsa -des3 -out server.key 1024 由私钥生成公钥openssl req -new -key c:/dev/my.key -out c:/dev/my.csr openssl req -new -key server.key -out server.csr 查看证书 Common Name，这里输入的域名即为我们要使用 HTTPS 访问的域名 req -text -in c:/dev/my.csr -noout 生成解密的key openssl rsa -in server.key -out server.key.unsecure 签名工具 https://sourceforge.net/projects/xca/ x509 -req -days 365 -in c:/dev/my.csr -signkey c:/dev/my.key -out c:/dev/sign.crt openssl x509 -req -days 365 -in server.csr -signkey server.key.unsecure -out server.crt 查看证书 x509 -text -in c:\openSSLDemo\fd.crt -noout Country Name (2 letter code) [XX]:CN #请求签署人的信息 State or Province Name (full name) []: #请求签署人的省份名字 Locality Name (eg, city) [Default City]:# 请求签署人的城市名字 Organization Name (eg, company) [Default Company Ltd]:#请求签署人的公司名字 Organizational Unit Name (eg, section) []:#请求签署人的部门名字 Common Name (eg, your name or your server’s hostname) []:#这里一般填写请求人的的服务器域名， 问题 4928:error:2807106B:UI routines:UI_process:processing error:crypto\ui\ui_lib.c:543:while reading strings 4928:error:0906906F:PEM routines:PEM_ASN1_write_bio:read key:crypto\pem\pem_lib.c:357: SSL/TLS协议四次握手SSL/TSL协议基本过程： 客户端向服务器端索要并验证公钥。 双方协商生成”对话密钥”。 双方采用”对话密钥”进行加密通信。 SSL/TSL通过四次握手，主要交换三个信息： 数字证书：该证书包含了公钥等信息，一般是由服务器发给客户端，接收方通过验证这个证书是不是由信赖的CA签发，或者与本地的证书相对比，来判断证书是否可信；假如需要双向验证，则服务器和客户端都需要发送数字证书给对方验证； 三个随机数：这三个随机数构成了后续通信过程中用来对数据进行对称加密解密的“对话密钥”。首先客户端先发第一个随机数N1，然后服务器回了第二个随机数N2（这个过程同时把之前提到的证书发给客户端），这两个随机数都是明文的；而第三个随机数N3（这个随机数被称为Premaster secret），客户端用数字证书的公钥进行非对称加密，发给服务器；而服务器用只有自己知道的私钥来解密，获取第三个随机数。这样，服务端和客户端都有了三个随机数N1+N2+N3，然后两端就使用这三个随机数来生成“对话密钥”，在此之后的通信都是使用这个“对话密钥”来进行对称加密解密。因为这个过程中，服务端的私钥只用来解密第三个随机数，从来没有在网络中传输过，这样的话，只要私钥没有被泄露，那么数据就是安全的。 加密通信协议：就是双方商量使用哪一种加密方式，假如两者支持的加密方式不匹配，则无法进行通信； 客户端发出请求（ClientHello）首先，客户端（通常是浏览器）先向服务器发出加密通信的请求，这被叫做ClientHello请求。 在这一步，客户端主要向服务器提供以下信息。 （1） 支持的协议版本，比如TLS 1.0版。 （2） 一个客户端生成的随机数，稍后用于生成&quot;对话密钥&quot;。 （3） 支持的加密方法，比如RSA公钥加密。 （4） 支持的压缩方法。 服务器回应（SeverHello）（1） 确认使用的加密通信协议版本，比如TLS 1.0版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。 （2） 确认使用的加密方法，比如RSA公钥加密，返回加密公钥 （3） 服务器证书 客户端回应a) 验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。 b) 如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密。 c) 使用约定好的HASH计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。 服务器a) 使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。 b) 使用密码加密一段握手消息，发送给浏览器。 自制CA证书我们用的操作系统（windows, linux, unix ,android, ios等）都预置了很多信任的根证书，比如我的windows中就包含VeriSign的根证书，那么浏览器访问服务器比如支付宝www.alipay.com时，SSL协议握手时服务器就会把它的服务器证书发给用户浏览器，而这本服务器证书又是比如VeriSign颁发的，自然就验证通过了。 创建CA证书 因为要创建根证书，这里选择序号为1的自认证证书，签名算法选择SHA 256，证书模版选择默认CA，再点击Apply all（这个不能漏） 再切到Subject页面，填好各个字段，都可以随便填，再点击Generate a new key生产私钥 最后点击OK，CA证书做好了，有效期默认10年 将根证书导出成只包含公钥的证书格式，这本根证书就是放在网站上供用户下载安装，或主动安装到客户机器中的： 制作服务器证书、客户端证书选择已经制作好的根CA，然后点击New Certificate 选择服务器端证书， 填好信息并创建私钥]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tengine]]></title>
    <url>%2F2019%2F08%2F25%2FTengine%2F</url>
    <content type="text"><![CDATA[TengineNginx和TengineNginxNginx (“engine x”) 是一个高性能的 HTTP 和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 •第一个公开版本0.1.0发布于2004年10月4日。 其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名 官方测试nginx能够支撑5万并发链接，并且cpu、内存等资源消耗却非常低，运行非常稳定 2011年6月1日，nginx 1.0.4发布。 Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，并在一个BSD-like 协议下发行。由俄罗斯的程序设计师Igor Sysoev所开发， 其特点是占有内存少，并发能力强，事实上nginx的并发能力确实在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：新浪、网易、腾讯等。 功能： web服务器 web reverse proxy smtp reverse proxy Nginx和apache的优缺点nginx相对于apache的优点： 轻量级，同样起web 服务，比apache 占用更少的内存及资源 抗并发，nginx 处理请求是异步非阻塞的，而apache 则是阻塞型的，在高并发下nginx 能保持低资源低消耗高性能 高度模块化的设计，编写模块相对简单 社区活跃，各种高性能模块出品迅速 apache 相对于nginx 的优点： rewrite ，比nginx 的rewrite 强大 模块超多，基本想到的都可以找到 少bug ，nginx 的bug 相对较多 Nginx 配置简洁, Apache 复杂 最核心的区别在于apache是同步多进程模型，一个连接对应一个进程； nginx是异步的，多个连接（万级别）可以对应一个进程 Nginx解决的问题 高并发 负载均衡 高可用 虚拟主机 伪静态 动静分离 安装准备工作操作系统最好使用linux操作系统，课上使用VirtualBox或VMware虚拟机搭建centos6.x做实验。 系统依赖组件 gcc openssl-devel pcre-devel zlib-devel 安装：yum install gcc openssl-devel pcre-devel zlib-devel Tengine下载和文档http://tengine.taobao.org/ Nginx官网和文档http://nginx.org 上传Nginx压缩包到服务器，一般安装在/usr/local目录下 编译安装./ configure --prefix=/安装路径 make &amp;&amp; make install 启动服务脚本自启动拷贝附件提供的Nginx启动脚本文件内容到/etc/init.d/nginx这个文件中 目录下如果没有这个文件的话需要手动创建 修改可执行权限chmod 777 nginx 启动服务service Nginx start 启动服务 service Nginx stop 停止 service Nginx status 状态 service Nginx reload 动态重载配置文件 脚本内容：#!/bin/sh # # nginx - this script starts and stops the nginx daemon # # chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \ # proxy and IMAP/POP3 proxy server # processname: nginx # config: /etc/nginx/nginx.conf # config: /etc/sysconfig/nginx # pidfile: /var/run/nginx.pid # Source function library. . /etc/rc.d/init.d/functions # Source networking configuration. . /etc/sysconfig/network # Check that networking is up. [ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0 nginx=&quot;/usr/local/sbin/nginx&quot; prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/usr/local/conf/nginx.conf&quot; [ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() { # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &#39;s/[^*]*--user=\([^ ]*\).*/\1/g&#39; -` options=`$nginx -V 2&gt;&amp;1 | grep &#39;configure arguments:&#39;` for opt in $options; do if [ `echo $opt | grep &#39;.*-temp-path&#39;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done } start() { [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval } stop() { echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval } restart() { configtest || return $? stop sleep 1 start } reload() { configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo } force_reload() { restart } configtest() { $nginx -t -c $NGINX_CONF_FILE } rh_status() { status $prog } rh_status_q() { rh_status &gt;/dev/null 2&gt;&amp;1 } case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}&quot; exit 2 esac Nginx配置解析定义Nginx运行的用户和用户组user www www; 进程数建议设置为等于CPU总核心数。 worker_processes 8; 全局错误日志全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /var/log/nginx/error.log info; 进程文件pid /var/run/nginx.pid; 打开的最多文件描述符一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。 worker_rlimit_nofile 65535; event单个进程最大连接数并发总数是 worker_processes 和 worker_connections 的乘积 即 max_clients = worker_processes * worker_connections 在设置了反向代理的情况下，max_clients = worker_processes worker_connections / 4 为什么为什么上面反向代理要除以4，应该说是一个经验值根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 8000 = 32000worker_connections 值的设置跟物理内存大小有关 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数 工作模式与连接数上限 events { 参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。 use epoll; 单个进程最大连接数（最大连接数=连接数*进程数） worker_connections 65535; } 可以打开的文件句柄数是多少 –$ cat /proc/sys/fs/file-max 输出：97320 并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置,使得并发总数小于操作系统可以打开的最大文件数目 – # 其实质也就是根据主机的物理CPU和内存进行配置 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。 查看系统限制 ulimit -a 打开文件句柄数量限制是Linux操作系统对一个进程打开的文件句柄数量的限制(也包含打开的SOCKET数量，可影响MySQL的并发连接数目) 系统总限制： /proc/sys/fs/file-max 当前使用句柄数：/proc/sys/fs/file-nr 修改句柄数：ulimit -SHn 65535 httpinclude mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型 charset utf-8; #默认编码 client_header_buffer_size 32k; #上传文件大小限制 sendfilesendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 sendfile()还能够用来在两个文件夹之间移动数据 tcp_nopush 在linux/Unix系统中优化tcp数据传输，仅在sendfile开启时有效 autoindex on;#开启目录列表访问，合适下载服务器，默认关闭。 keepalive_timeout 120;#长连接超时时间，单位是秒 gzipgzip on; 开启gzip压缩输出 gzip_min_length 1k; 设置允许压缩的页面最小字节数，页面字节数从header头得content-length中进行获取。默认值是0，不管页面多大都压缩。建议设置成大于2k的字节数，小于2k可能会越压越大。 gzip_buffers 4 16k; 设置系统获取几个单位的缓存用于存储gzip的压缩结果数据流。 例如 4 4k 代表以4k为单位，按照原始数据大小以4k为单位的4倍申请内存。 4 8k 代表以8k为单位，按照原始数据大小以8k为单位的4倍申请内存。 如果没有设置，默认值是申请跟原始数据相同大小的内存空间去存储gzip压缩结果。 gzip_http_version 1.0;压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; 压缩级别，1-10，数字越大压缩的越好，也越占用CPU时间 gzip_types text/plain application/x-javascript text/css application/xml; 压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 默认值: gzip_types text/html (默认不对js/css文件进行压缩) 压缩类型，匹配MIME类型进行压缩 设置哪压缩种文本文件可参考 conf/mime.types gzip_disable &quot;MSIE [1-6]\.&quot;;E6及以下禁止压缩 gzip_vary on;给CDN和代理服务器使用，针对相同url，可以根据头信息返回压缩和非压缩副本 serverlisten 80; 监听端口 server_name www.mashibing.com mashibing.com; 域名可以有多个，用空格隔开 charset koi8-r; 编码集 access_log &quot;pipe:rollback logs/host.access_log interval=1d baknum=7 maxsize=2G&quot; main; index index.html index.htm index.jsp; 默认页root /data/www/ha97; 主目录 虚拟主机虚拟主机是一种特殊的软硬件技术，它可以将网络上的每一台计算机分成多个虚拟主机，每个虚拟主机可以独立对外提供www服务，这样就可以实现一台主机对外提供多个web服务，每个虚拟主机之间是独立的，互不影响的 通过nginx可以实现虚拟主机的配置，nginx支持三种类型的虚拟主机配置 基于ip的虚拟主机， （一块主机绑定多个ip地址） 基于域名的虚拟主机（servername） 基于端口的虚拟主机（listen如果不写ip端口模式） http{ server{ #表示一个虚拟主机 } } location映射/虚拟目录 location = / { [ configuration A ] } location / { [ configuration B ] } location /documents/ { [ configuration C ] } location ^~ /images/ { [ configuration D ] } location ~* \.(gif|jpg|jpeg)$ { [ configuration E ] } location [ = | ~ | ~* | ^~ ] uri { ... } location URI {} 对当前路径及子路径下的所有对象都生效； location = URI {} 注意URL最好为具体路径。 精确匹配指定的路径，不包括子路径，因此，只对当前资源生效； location ~ URI {} location ~* URI {} 模式匹配URI，此处的URI可使用正则表达式，~区分字符大小写，~*不区分字符大小写； location ^~ URI {} 禁用正则表达式 优先级：= &gt; ^~ &gt; ~|~* &gt; /|/dir/ location配置规则location 的执行逻辑跟 location 的编辑顺序无关。矫正：这句话不全对，“普通 location ”的匹配规则是“最大前缀”，因此“普通 location ”的确与 location 编辑顺序无关； 但是“正则 location ”的匹配规则是“顺序匹配，且只要匹配到第一个就停止后面的匹配”； “普通location ”与“正则 location ”之间的匹配顺序是？先匹配普通 location ，再“考虑”匹配正则 location 。 注意这里的“考虑”是“可能”的意思，也就是说匹配完“普通 location ”后，有的时候需要继续匹配“正则 location ”，有的时候则不需要继续匹配“正则 location ”。两种情况下，不需要继续匹配正则 location ： （ 1 ）当普通 location 前面指定了“ ^~ ”，特别告诉 Nginx 本条普通 location 一旦匹配上，则不需要继续正则匹配； （ 2 ）当普通location 恰好严格匹配上，不是最大前缀匹配，则不再继续匹配正则 IP访问控制location { deny IP /IP段 deny 192.168.1.109; allow 192.168.1.0/24;192.168.0.0/16;192.0.0.0/8 } 用户认证访问模块ngx_http_auth_basic_module 允许使用“HTTP基本认证”协议验证用户名和密码来限制对资源的访问。 location ~ (.*)\.avi$ { auth_basic &quot;closed site&quot;; auth_basic_user_file conf/users; } httpd-toolsyum install httpd htpasswd -c -d /usr/local/users zhangyang nginx访问状态监控location /basic_status { stub_status on; } 反向代理通常的代理服务器，只用于代理内部网络对Internet的连接请求，客户机必须指定代理服务器,并将本来要直接发送到Web服务器上的http请求发送到代理服务器中由代理服务器向Internet上的web服务器发起请求，最终达到客户机上网的目的。 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器 经典的反向代理结构 Proxy_pass http://192.168.43.152/ 301重定向问题 upstream反向代理配合upstream使用 upstream httpds { server 192.168.43.152:80; server 192.168.43.153:80; } weight(权重)指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 upstream httpds { server 127.0.0.1:8050 weight=10 down; server 127.0.0.1:8060 weight=1; server 127.0.0.1:8060 weight=1 backup; } down：表示当前的server暂时不参与负载 weight：默认为1.weight越大，负载的权重就越大。 backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。 max_conns可以根据服务的好坏来设置最大连接数，防止挂掉，比如1000，我们可以设置800 upstream httpds { server 127.0.0.1:8050 weight=5 max_conns=800; server 127.0.0.1:8060 weight=1; } max_fails、 fail_timeoutmax_fails:失败多少次 认为主机已挂掉则，踢出，公司资源少的话一般设置2~3次，多的话设置1次 max_fails=3 fail_timeout=30s代表在30秒内请求某一应用失败3次，认为该应用宕机，后等待30秒，这期间内不会再把新请求发送到宕机应用，而是直接发到正常的那一台，时间到后再有请求进来继续尝试连接宕机应用且仅尝试1次，如果还是失败，则继续等待30秒…以此循环，直到恢复。 upstream httpds { server 127.0.0.1:8050 weight=1 max_fails=1 fail_timeout=20; server 127.0.0.1:8060 weight=1; } 负载均衡算法轮询+weight ip_hash url_hash least_conn least_time 健康检查模块配置一个status的location location /status { check_status; } 在upstream配置如下 check interval=3000 rise=2 fall=5 timeout=1000 type=http; check_http_send &quot;HEAD / HTTP/1.0\r\n\r\n&quot;; check_http_expect_alive http_2xx http_3xx; session共享Memcached安装 安装libevent 安装memcached 可以用yum方式安装 yum –y install memcached 启动memcachedmemcached -d -m 128 -u root -l 192.168.43.151 -p 11211 -c 256 -P /tmp/memcached.pid memcached-tool 192.168.2.51:11211 参数解释： -d:后台启动服务 -m:缓存大小 -p：端口 -l:IP -P:服务器启动后的系统进程ID，存储的文件 -u:服务器启动是以哪个用户名作为管理用户 Nginx配置upstream tomcat{ server 192.168.2.52:8080; server 192.168.2.53:8080; } location /tomcat { proxy_pass http://tomcat/; } Tomcat配置到tomcat的lib下，jar包见附件 每个tomcat里面的context.xml中加入 &lt;Manager className=&quot;de.javakaffee.web.msm.MemcachedBackupSessionManager&quot; memcachedNodes=&quot;n1:192.168.43.151:11211&quot; sticky=&quot;false&quot; lockingMode=&quot;auto&quot; sessionBackupAsync=&quot;false&quot; requestUriIgnorePattern=&quot;.*\.(ico|png|gif|jpg|css|js)$&quot; sessionBackupTimeout=&quot;1000&quot; transcoderFactoryClass=&quot;de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory&quot; /&gt; http_proxy 本地磁盘缓存proxy_cache_path /path/to/cache levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m use_temp_path=off; server { set $upstream http://ip:port location / { proxy_cache my_cache; proxy_pass $upstream; } } /path/to/cache #本地路径，用来设置Nginx缓存资源的存放地址 levels #默认所有缓存文件都放在同一个/path/to/cache下，但是会影响缓存的性能，因此通常会在/path/to/cache下面建立子目录用来分别存放不同的文件。假设levels=1:2，Nginx为将要缓存的资源生成的key为f4cd0fbc769e94925ec5540b6a4136d0，那么key的最后一位0，以及倒数第2-3位6d作为两级的子目录，也就是该资源最终会被缓存到/path/to/cache/0/6d目录中 key_zone #在共享内存中设置一块存储区域来存放缓存的key和metadata（类似使用次数），这样nginx可以快速判断一个request是否命中或者未命中缓存，1m可以存储8000个key，10m可以存储80000个key max_size #最大cache空间，如果不指定，会使用掉所有disk space，当达到配额后，会删除最少使用的cache文件 inactive #未被访问文件在缓存中保留时间，本配置中如果60分钟未被访问则不论状态是否为expired，缓存控制程序会删掉文件。inactive默认是10分钟。需要注意的是，inactive和expired配置项的含义是不同的，expired只是缓存过期，但不会被删除，inactive是删除指定时间内未被访问的缓存文件 use_temp_path #如果为off，则nginx会将缓存文件直接写入指定的cache文件中，而不是使用temp_path存储，official建议为off，避免文件在不同文件系统中不必要的拷贝 proxy_cache #启用proxy cache，并指定key_zone。另外，如果proxy_cache off表示关闭掉缓存。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua教程]]></title>
    <url>%2F2019%2F08%2F25%2FLua%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Lua教程Lua 是由巴西里约热内卢天主教大学（Pontifical Catholic University of Rio de Janeiro）里的一个研究小组于1993年开发的一种轻量、小巧的脚本语言，用标准 C 语言编写，其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。 官网：http://www.lua.org/ Redis 在 2.6 版本中推出了脚本功能，允许开发者将 Lua 语言编写的脚本传到 Redis 中执行。使用 Lua 脚本的优点有如下几点: 减少网络开销：本来需要多次请求的操作，可以一次请求完成，从而节约网络开销； 原子操作：Redis 会将整个脚本作为一个整体执行，中间不会执行其它命令； 复用：客户端发送的脚本会存储在 Redis 中，从而实现脚本的复用。 IDEEmmyLua插件https://github.com/EmmyLua/IntelliJ-EmmyLua https://emmylua.github.io/zh_CN/ LDT 基于eclipsehttps://www.eclipse.org/ldt/ Lua基础语法参考 http://book.luaer.cn/ hello worldprint(&quot;hello world!&quot;) 保留关键字and break do else elseif end false for function if in local nil not or repeat return then true until while 注释-- 两个减号是行注释 --[[ 这是块注释 这是块注释 --]] 变量数字类型Lua的数字只有double型，64bits 你可以以如下的方式表示数字 num = 1024 num = 3.0 num = 3.1416 num = 314.16e-2 num = 0.31416E1 num = 0xff num = 0x56 字符串可以用单引号，也可以用双引号 也可以使用转义字符‘\n’ （换行）， ‘\r’ （回车）， ‘\t’ （横向制表）， ‘\v’ （纵向制表）， ‘\’ （反斜杠）， ‘\”‘ （双引号）， 以及 ‘\” （单引号)等等 下面的四种方式定义了完全相同的字符串（其中的两个中括号可以用于定义有换行的字符串） a = ‘alo\n123”‘ a = “alo\n123\”” a = ‘\97lo\10\04923”‘ a = [[alo123”]] 空值C语言中的NULL在Lua中是nil，比如你访问一个没有声明过的变量，就是nil 布尔类型只有nil和false是 false 数字0，‘’空字符串（’\0’）都是true 作用域lua中的变量如果没有特殊说明，全是全局变量，那怕是语句块或是函数里。 变量前加local关键字的是局部变量。 控制语句while循环local i = 0 local max = 10 while i &lt;= max do print(i) i = i +1 end if-elselocal function main() local age = 140 local sex = &#39;Male&#39; if age == 40 and sex ==&quot;Male&quot; then print(&quot; 男人四十一枝花 &quot;) elseif age &gt; 60 and sex ~=&quot;Female&quot; then print(&quot;old man without country!&quot;) elseif age &lt; 20 then io.write(&quot;too young, too naive!\n&quot;) else print(&quot;Your age is &quot;..age) end end -- 调用 main() for循环sum = 0 for i = 100, 1, -2 do sum = sum + i end 函数1. function myPower(x,y) return y+x end power2 = myPower(2,3) print(power2) 2. function newCounter() local i = 0 return function() -- anonymous function i = i + 1 return i end end c1 = newCounter() print(c1()) --&gt; 1 print(c1()) --&gt; 2 print(c1()) 返回值name, age,bGay = &quot;yiming&quot;, 37, false, &quot;yimingl@hotmail.com&quot; print(name,age,bGay) function isMyGirl(name) return name == &#39;xiao6&#39; , name end local bol,name = isMyGirl(&#39;xiao6&#39;) print(name,bol) Tablekey，value的键值对 类似 map lucy = {name=&#39;xiao6&#39;,age=18,height=165.5} xiao6.age=35 print(xiao6.name,xiao6.age,xiao6.height) print(xiao6) 数组arr = {&quot;string&quot;, 100, &quot;xiao6&quot;,function() print(&quot;memeda&quot;) return 1 end} print(arr[4]()) 遍历for k, v in pairs(arr) do print(k, v) end 面向对象成员函数person = {name=&#39;xiao6&#39;,age = 18} function person.eat(food) print(person.name ..&quot; eating &quot;..food) end person.eat(&quot;xxoo&quot;)]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo4]]></title>
    <url>%2F2019%2F08%2F25%2FDubbo4%2F</url>
    <content type="text"><![CDATA[Dubbo 04服务化最佳实践分包建议将服务接口、服务模型、服务异常等均放在 API 包中，因为服务模型和异常也是 API 的一部分，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。 如果需要，也可以考虑在 API 包中放置一份 Spring 的引用配置，这样使用方只需在 Spring 加载过程中引用此配置即可。配置建议放在模块的包目录下，以免冲突，如：com/alibaba/china/xxx/dubbo-reference.xml。 Maven 聚合项目改造公用的接口、工具类、实体类等抽取出API项目独立维护更新 每次更新后需要install 粒度服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。 服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。 不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。 版本每个接口都应定义版本号，为后续不兼容升级提供可能，如： &lt;dubbo:service interface=&quot;com.xxx.XxxService&quot; version=&quot;1.0&quot; /&gt;。 建议使用两位版本号，因为第三位版本号通常表示兼容升级，只有不兼容时才需要变更服务版本。 当不兼容时，先升级一半提供者为新版本，再将消费者全部升为新版本，然后将剩下的一半提供者升为新版本。 兼容性服务接口增加方法，或服务模型增加字段，可向后兼容，删除方法或删除字段，将不兼容，枚举类型新增字段也不兼容，需通过变更版本号升级。 枚举值如果是完备集，可以用 Enum，比如：ENABLE, DISABLE。 如果是业务种类，以后明显会有类型增加，不建议用 Enum，可以用 String 代替。 如果是在返回值中用了 Enum，并新增了 Enum 值，建议先升级服务消费方，这样服务提供方不会返回新值。 如果是在传入参数中用了 Enum，并新增了 Enum 值，建议先升级服务提供方，这样服务消费方不会传入新值。 序列化服务参数及返回值建议使用 POJO 对象，即通过 setter, getter 方法表示属性的对象。 服务参数及返回值不建议使用接口，因为数据模型抽象的意义不大，并且序列化需要接口实现类的元信息，并不能起到隐藏实现的意图。 服务参数及返回值都必须是传值调用，而不能是传引用调用，消费方和提供方的参数或返回值引用并不是同一个，只是值相同，Dubbo 不支持引用远程对象。 异常建议使用异常汇报错误，而不是返回错误码，异常信息能携带更多信息，并且语义更友好。 如果担心性能问题，在必要时，可以通过 override 掉异常类的 fillInStackTrace() 方法为空方法，使其不拷贝栈信息。 查询方法不建议抛出 checked 异常，否则调用方在查询时将过多的 try...catch，并且不能进行有效处理。 服务提供方不应将 DAO 或 SQL 等异常抛给消费方，应在服务实现中对消费方不关心的异常进行包装，否则可能出现消费方无法反序列化相应异常。 调用不要只是因为是 Dubbo 调用，而把调用 try...catch 起来。try...catch 应该加上合适的回滚边界上。 Provider 端需要对输入参数进行校验。如有性能上的考虑，服务实现者可以考虑在 API 包上加上服务 Stub 类来完成检验。 在 Provider 端尽量多配置 Consumer 端属性原因如下： 作服务的提供方，比服务消费方更清楚服务的性能参数，如调用的超时时间、合理的重试次数等 在 Provider 端配置后，Consumer 端不配置则会使用 Provider 端的配置，即 Provider 端的配置可以作为 Consumer 的缺省值 [1]。否则，Consumer 会使用 Consumer 端的全局设置，这对于 Provider 是不可控的，并且往往是不合理的 Provider 端尽量多配置 Consumer 端的属性，让 Provider 的实现者一开始就思考 Provider 端的服务特点和服务质量等问题。 建议在 Provider 端配置的 Consumer 端属性 timeout：方法调用的超时时间 retries：失败重试次数，缺省是 2 loadbalance：负载均衡算法，缺省是随机 random + 权重。还可以配置轮询 roundrobin、最不活跃优先 leastactive 和一致性哈希 consistenthash 等 actives：消费者端的最大并发调用限制，即当 Consumer 对一个服务的并发调用到上限后，新调用会阻塞直到超时，在方法上配置 dubbo:method 则针对该方法进行并发限制，在接口上配置 dubbo:service，则针对该服务进行并发限制 executes服务提供方可以使用的最大线程数 在 Provider 端配置合理的 Provider 端属性建议在 Provider 端配置的 Provider 端属性有： threads：服务线程池大小 executes：一个服务提供者并行执行请求上限，即当 Provider 对一个服务的并发调用达到上限后，新调用会阻塞，此时 Consumer 可能会超时。在方法上配置 dubbo:method 则针对该方法进行并发限制，在接口上配置 dubbo:service，则针对该服务进行并发限制 项目中多个模块间公共依赖的版本号、scope的控制 配置 Dubbo 缓存文件提供者列表缓存文件： &lt;dubbo:registry file=”${user.home}/output/dubbo.cache” /&gt; dubbo.registry.file=c:/output/dubbo.cache 注意： 可以根据需要调整缓存文件的路径，保证这个文件不会在发布过程中被清除； 如果有多个应用进程，请注意不要使用同一个文件，避免内容被覆盖； 该文件会缓存注册中心列表和服务提供者列表。配置缓存文件后，应用重启过程中，若注册中心不可用，应用会从该缓存文件读取服务提供者列表，进一步保证应用可靠性。 启动检查Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题，默认 check=&quot;true&quot;。 可以通过 check=&quot;false&quot; 关闭检查，比如，测试时，有些服务不关心，或者出现了循环依赖，必须有一方先启动。 另外，如果你的 Spring 容器是懒加载的，或者通过 API 编程延迟引用服务，请关闭 check，否则服务临时不可用时，会抛出异常，拿到 null 引用，如果 check=&quot;false&quot;，总是会返回引用，当服务恢复时，能自动连上。 示例通过 spring 配置文件关闭某个服务的启动时检查 (没有提供者时报错)： &lt;dubbo:reference interface=&quot;com.foo.BarService&quot; check=&quot;false&quot; /&gt; 关闭所有服务的启动时检查 (没有提供者时报错)： &lt;dubbo:consumer check=&quot;false&quot; /&gt; 关闭注册中心启动时检查 (注册订阅失败时报错)： &lt;dubbo:registry check=&quot;false&quot; /&gt; 通过 dubbo.propertiesdubbo.reference.com.foo.BarService.check=false dubbo.reference.check=false dubbo.consumer.check=false dubbo.registry.check=false 通过 -D 参数java -Ddubbo.reference.com.foo.BarService.check=false java -Ddubbo.reference.check=false java -Ddubbo.consumer.check=false java -Ddubbo.registry.check=false 配置的含义dubbo.reference.check=false，强制改变所有 reference 的 check 值，就算配置中有声明，也会被覆盖。 dubbo.consumer.check=false，是设置 check 的缺省值，如果配置中有显式的声明，如：&lt;dubbo:reference check=&quot;true&quot;/&gt;，不会受影响。 dubbo.registry.check=false，前面两个都是指订阅成功，但提供者列表是否为空是否报错，如果注册订阅失败时，也允许启动，需使用此选项，将在后台定时重试。 metrics当我们需要为某个系统某个服务做监控、做统计，就需要用到Metrics。 延迟暴露@Service(version = “1.0.0” ,timeout = 10000, interfaceClass = IAccountService.class,delay = 1000000) Telnet治理服务显示服务ls ls: 显示服务列表 ls -l: 显示服务详细信息列表 ls XxxService: 显示服务的方法列表 ls -l XxxService: 显示服务的方法详细信息列表 ps ps: 显示服务端口列表 ps -l: 显示服务地址列表 ps 20880: 显示端口上的连接信息 ps -l 20880: 显示端口上的连接详细信息 服务调用引入fastjson依赖 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.51&lt;/version&gt; &lt;/dependency&gt; invoke com.mashibing.springboot.service.IRoleService:1.0.0.findById(8) Telnet命令扩展http://dubbo.apache.org/zh-cn/docs/dev/impls/telnet-handler.html QOS 模块手动上下线相关参数说明QoS提供了一些启动参数，来对启动进行配置，他们主要包括： 参数 说明 默认值 qosEnable 是否启动QoS true qosPort 启动QoS绑定的端口 22222 qosAcceptForeignIp 是否允许远程访问 false 注意，从2.6.4/2.7.0开始，qosAcceptForeignIp默认配置改为false，如果qosAcceptForeignIp设置为true，有可能带来安全风险，请仔细评估后再打开。 新版本的 telnet 端口 与 dubbo 协议的端口是不同的端口，默认为 22222，可通过配置文件dubbo.properties修改: dubbo.application.qos.port=33333 或者通过设置 JVM 参数: -Ddubbo.application.qos.port=33333 默认情况下，dubbo 接收任何主机发起的命令，可通过配置文件dubbo.properties 修改: dubbo.application.qos.accept.foreign.ip=false 或者通过设置 JVM 参数: -Ddubbo.application.qos.accept.foreign.ip=false]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo3]]></title>
    <url>%2F2019%2F08%2F25%2FDubbo3%2F</url>
    <content type="text"><![CDATA[Dubbo03restful风格的APIRepresentational State Transfer，资源表现层状态转换 根路径mashibing.com 协议http:// 版本v1 可以直接写在URL上，或者写在header中传递“Accept-Version: v2” @RequestMapping(headers = &quot;Accept-Version=v2&quot;,value = &quot;models&quot;,method = RequestMethod.GET) 用HTTP协议里的动词来实现资源的增删改查GET 用来获取资源， POST 用来新建资源（也可以用于更新资源）。 DELETE 用来删除资源。 UPDATE http://api.chesxs.com/v1/fence 更新围栏信息 用例单个资源http://mashibing.com/api/v1/Users/1 使用Get方法获取id是1的用户数据 正确：GET /model/models/{id} #获取单个资源 正确：POST /model/models #创建单个资源 正确：PUT /model/models/{id} #更新单个资源 正确：DELETE /model/models/{id} #删除单个资源 正确：PATCH /model/models/{id} #更新单个资源（只传差异） 正确：GET /model/configRuleFile #获取单个资源（如果仅有一个值时，应采用单数方式） 返回结果： 如果指定的资源并不存在，那么应该返回404 Not Found状态，否则应该返回200 OK状态码 资源集合对于资源集合，支持以下URL 正确： GET /model/models #获取资源列表 正确： GET /model/models?ids={ids} #批量获取资源列表 正确： DELETE /model/models?ids={ids} #批量删除资源列表 返回结果： 如果列表为空，则应该空数组 响应结果 响应状态码 含义 成功 200 调用成功 201 创建成功 204 执行成功，但无返回值 失败 400 无效请求 401 没有登录 403 没有权限 404 请求的资源不存在 500 服务内部错误 swagger（丝袜哥）Swagger是一个简单但功能强大的API表达工具。它具有地球上最大的API工具生态系统，数以千计的开发人员，使用几乎所有的现代编程语言，都在支持和使用Swagger。使用Swagger生成API，我们可以得到交互式文档，自动生成代码的SDK以及API的发现特性等。 OpenAPIOpenAPI规范是Linux基金会的一个项目，试图通过定义一种用来描述API格式或API定义的语言，来规范RESTful服务开发过程。OpenAPI规范帮助我们描述一个API的基本信息 比如： 有关该API的一般性描述 可用路径（/资源） 在每个路径上的可用操作（获取/提交…） 每个操作的输入/输出格式 根据OpenAPI规范编写的二进制文本文件，能够像代码一样用任何VCS工具管理起来一旦编写完成，API文档可以作为： 需求和系统特性描述的根据 前后台查询、讨论、自测的基础 部分或者全部代码自动生成的根据 其他重要的作用，比如开放平台开发者的手册… 资源官网https://swagger.io/ 在线编辑器http://editor.swagger.io/ 编写API文档我们可以选择使用JSON或者YAML的语言格式来编写API文档 swagger: &#39;2.0&#39; info: version: 1.0.0 title: mashibing.com api description: 马老师的官网接口 contact: name: yiming url: http://mashibing.com email: 888@qqq.com license: name: MIT url: http://opensource.org/licenses/MIT schemes: - http host: mashibing.com basePath: /api/v1 paths: /user/{userid}: get: summary: 获取一个用户 description: 根据id获取用户信息 parameters: - name: userid in: path required: true description: 用户id type: string responses: 200: description: OK /user: get: summary: 返回List 包含所有用户 description: 返回List 包含所有用户 parameters: - name: pageSize in: query description: 每页显示多少 type: integer - name: pageNum in: query description: 当前第几页 type: integer responses: 200: description: OK schema: type: array items: required: - username properties: username: type: string password: type: string 整合SpringBoot官方依赖&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt; &lt;artifactId&gt;jackson-core-asl&lt;/artifactId&gt; &lt;version&gt;1.9.13&lt;/version&gt; &lt;/dependency&gt; 第三方https://github.com/SpringForAll/spring-boot-starter-swagger 依赖引入&lt;dependency&gt; &lt;groupId&gt;com.spring4all&lt;/groupId&gt; &lt;artifactId&gt;swagger-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.9.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; 启用注解http://localhost:803//v2/api-docs http://localhost:8080/swagger-ui.html 分组swagger.docket.controller.title=group-controller swagger.docket.controller.base-package=com.mashibing.springboot.controller swagger.docket.restcontroller.title=group-restcontroller swagger.docket.restcontroller.base-package=com.mashibing.springboot.controller.rest 实体模型@ApiModelProperty(value = &quot;权限id&quot;, name = &quot;id&quot;,dataType = &quot;integer&quot;,required = true,example = &quot;1&quot;) private Integer id; 接口方法@ApiOperation(value = &quot;获取所有权限&quot;) @RequestMapping(value = &quot;list&quot;,method = RequestMethod.GET) public List&lt;Permission&gt; list() { return permissionSrv.findAll(); } @ApiOperation(value = &quot;添加权限&quot;) @RequestMapping(&quot;update&quot;) public RespStat update(@ApiParam(name=&quot;permission&quot;,required = true, example = &quot;{json}&quot;,value = &quot;权限对象&quot;) @RequestBody Permission permission) { System.out.println(&quot;permission:&quot; + ToStringBuilder.reflectionToString(permission)); permissionSrv.update(permission); return RespStat.build(200); } 接口类描述 @Api(value = &quot;用户权限管理&quot;,tags={&quot;用户操作接口&quot;})]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo2]]></title>
    <url>%2F2019%2F08%2F25%2FDubbo2%2F</url>
    <content type="text"><![CDATA[Dubbo 02 微信开发Dubbo Adminhttps://github.com/apache/dubbo-admin 原系统微服务改造mvc层排除数据源检查Application 入口程序添加 @SpringBootApplication(exclude = {DataSourceAutoConfiguration.class}) 新增微信接口微服务功能：微信登录 前置条件：微信开放平台 https://open.weixin.qq.com/ 可以获取snsapi_login 开发测试环境：公众号 公众号（公众平台）获取的scope只包括两种：snsapi_base 和snsapi_userinfo 环境搭建获取测试账号https://mp.weixin.qq.com 注册登录后使用测试账号开发 反向代理服务器主要用于开发中内网穿透 http://www.ngrok.cc/ http://www.natapp.cc/ API微信公众平台开发者文档https://mp.weixin.qq.com/wiki?t=resource/res_main&amp;id=mp1445241432 微信开放平台（公众号第三方平台开发）https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&amp;t=resource/res_list&amp;verify=1&amp;lang=zh_CN 微信小程序开发文档https://developers.weixin.qq.com/miniprogram/dev/framework/ 微信商户服务中心https://mp.weixin.qq.com/cgi-bin/readtemplate?t=business/faq_tmpl&amp;lang=zh_CN 微信支付商户平台开发者文档https://pay.weixin.qq.com/wiki/doc/api/index.html 微信支付H5https://pay.weixin.qq.com/wiki/doc/api/H5.php?chapter=15_1 微信支付代扣费https://pay.weixin.qq.com/wiki/doc/api/pap.php?chapter=17_1 微信支付单品优惠https://pay.weixin.qq.com/wiki/doc/api/danpin.php?chapter=9_201&amp;index=3 开发框架https://github.com/liyiorg/weixin-popular TokenAPI access_token 获取 MediaAPI 多媒体上传下载(临时素材) MaterialAPI 永久素材 MenuAPI 菜单、个性化菜单 MessageAPI 信息发送（客服消息、群发消息、模板消息） PayMchAPI 支付订单、红包、企业付款、委托代扣、代扣费(商户平台版)、分账 QrcodeAPI 二维码 SnsAPI 网签授权 UserAPI 用户管理、分组、标签、黑名单 ShorturlAPI 长链接转短链接 TicketAPI JSAPI ticket ComponentAPI 第三方平台开发 CallbackipAPI 获取微信服务器IP地址 ClearQuotaAPI 接口调用频次清零 PoiAPI 微信门店 @Moyq5 (贡献) CardAPI 微信卡券 @Moyq5 (贡献)Shak earoundAPI 微信摇一摇周边 @Moyq5 (贡献) DatacubeAPI 数据统计 @Moyq5 (贡献) CustomserviceAPI 客服功能 @ConciseA (贡献) WxaAPI 微信小程序 WxopenAPI 微信小程序 CommentAPI 文章评论留言 OpenAPI 微信开放平台帐号管理 BizwifiAPI 微信连WiFi ScanAPI 微信扫一扫 SemanticAPI 微信智能 &lt;dependency&gt; &lt;groupId&gt;com.github.liyiorg&lt;/groupId&gt; &lt;artifactId&gt;weixin-popular&lt;/artifactId&gt; &lt;version&gt;2.8.28&lt;/version&gt; &lt;/dependency&gt; 入口层 -&gt; 域名与高并发在入口层 加入CDN技术可以提高用户响应时间 让系统能够承受更高并发，分发请求 尤其对 全网加速（海外用户）效果明显 DNSdomain name system DNS是应用层协议，事实上他是为其他应用层协议工作的，包括不限于HTTP和SMTP以及FTP，用于将用户提供的主机名解析为ip地址。 dns集群 CDN 微信开发私服验证菜单管理创建菜单 { &quot;button&quot;: [ { &quot;type&quot;: &quot;click&quot;, &quot;name&quot;: &quot;今日歌曲&quot;, &quot;key&quot;: &quot;V1001_TODAY_MUSIC&quot; }, { &quot;name&quot;: &quot;菜单&quot;, &quot;sub_button&quot;: [ { &quot;type&quot;: &quot;view&quot;, &quot;name&quot;: &quot;搜索&quot;, &quot;url&quot;: &quot;http://www.soso.com/&quot; }, { &quot;type&quot;: &quot;miniprogram&quot;, &quot;name&quot;: &quot;wxa&quot;, &quot;url&quot;: &quot;http://mp.weixin.qq.com&quot;, &quot;appid&quot;: &quot;wx286b93c14bbf93aa&quot;, &quot;pagepath&quot;: &quot;pages/lunar/index&quot; }, { &quot;type&quot;: &quot;click&quot;, &quot;name&quot;: &quot;赞一下我们&quot;, &quot;key&quot;: &quot;V1001_GOOD&quot; } ] } ] } 消息回复文本 XMLTextMessage xmlTextMessage = new XMLTextMessage(eventMessage.getFromUserName(), eventMessage.getToUserName(), &quot;hi&quot;); xmlTextMessage.outputStreamWrite(outputStream); 图 String mediaId= &quot;YiHQtRD_fDKEG3-yTOwiGWlqv56-SUW5vfEDeEuAKx9a78337LKlSUmI4T-Cj8ij&quot;; XMLImageMessage xmlImageMessage = new XMLImageMessage(eventMessage.getFromUserName(),eventMessage.getToUserName(),mediaId); xmlImageMessage.outputStreamWrite(outputStream); 连接 XMLTextMessage xmlTextMessage2 = new XMLTextMessage(eventMessage.getFromUserName(), eventMessage.getToUserName(), &quot;请先&lt;a href=&#39;&quot;+wxConf.getAppDomain()+&quot;/h5/account/register&#39;&gt;完善一下信息&lt;/a&gt;&quot;); TemplateMessage msg = new TemplateMessage(); msg.setTouser(&quot;oStlBwHto08mKRIVUod5IHyevJyE&quot;); msg.setUrl(&quot;http://baidu.com&quot;); msg.setTemplate_id(&quot;gj4jA7HoS-1bmGyBK8VedBBQAXAboRJfWxUpbA8HlvM&quot;); LinkedHashMap&lt;String, TemplateMessageItem&gt; items = new LinkedHashMap&lt;&gt;(); // 填充模板内容 items.put(&quot;content&quot;, new TemplateMessageItem(&quot; 宝宝，你好。&quot;, &quot;#000000&quot;)); msg.setData(items); // 发送提醒 MessageAPI.messageTemplateSend(TokenManager.getToken(wxConf.getAppID()), msg);]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础之concurrent包]]></title>
    <url>%2F2019%2F08%2F16%2FJava%E5%9F%BA%E7%A1%80%E4%B9%8Bconcurrent%E5%8C%85%2F</url>
    <content type="text"><![CDATA[concurrent包ExecutorsInterfaces. Executor is a simple standardized interface for defining custom thread-like subsystems, including thread pools, asynchronous I/O, and lightweight task frameworks. Depending on which concrete Executor class is being used, tasks may execute in a newly created thread, an existing task-execution thread, or the thread calling execute, and may execute sequentially or concurrently. ExecutorService provides a more complete asynchronous task execution framework. An ExecutorService manages queuing and scheduling of tasks, and allows controlled shutdown. The ScheduledExecutorService subinterface and associated interfaces add support for delayed and periodic task execution. ExecutorServices provide methods arranging asynchronous execution of any function expressed as Callable, the result-bearing analog of Runnable. A Future returns the results of a function, allows determination of whether execution has completed, and provides a means to cancel execution. A RunnableFuture is a Future that possesses a run method that upon execution, sets its results.Implementations. Classes ThreadPoolExecutor and ScheduledThreadPoolExecutor provide tunable, flexible thread pools. The Executors class provides factory methods for the most common kinds and configurations of Executors, as well as a few utility methods for using them. Other utilities based on Executors include the concrete class FutureTask providing a common extensible implementation of Futures, and ExecutorCompletionService, that assists in coordinating the processing of groups of asynchronous tasks.Class ForkJoinPool provides an Executor primarily designed for processing instances of ForkJoinTask and its subclasses. These classes employ a work-stealing scheduler that attains high throughput for tasks conforming to restrictions that often hold in computation-intensive parallel processing. 接口。 Executor是一个简单的标准化接口，用于定义自定义线程类子系统，包括线程池，异步I / O和轻量级任务框架。根据正在使用的具体Executor类，任务可以在新创建的线程，现有任务执行线程或调用execute的线程中执行，并且可以顺序执行或同时执行。 ExecutorService提供了更完整的异步任务执行框架。 ExecutorService管理任务的排队和调度，并允许受控关闭。 ScheduledExecutorService子接口和关联的接口添加了对延迟和定期任务执行的支持。 ExecutorServices提供了安排异步执行任何函数的方法，这些函数表示为Callable，它是Runnable的结果模拟。 Future返回函数的结果，允许确定执行是否已完成，并提供取消执行的方法。 RunnableFuture是一个拥有run方法的Future，在执行时设置其结果。实现。类ThreadPoolExecutor和ScheduledThreadPoolExecutor提供可调节的灵活线程池。 Executors类为Executors的最常见种类和配置提供工厂方法，以及一些使用它们的实用方法。基于Executors的其他实用程序包括提供Futures的常见可扩展实现的具体类FutureTask和ExecutorCompletionService，它们协助协调异步任务组的处理。 ForkJoinPool类提供了一个Executor，主要用于处理ForkJoinTask及其子类的实例。这些类采用了一种工作窃取调度程序，可以获得符合计算密集型并行处理中常常存在的限制的任务的高吞吐量。 QueuesThe ConcurrentLinkedQueue class supplies an efficient scalable thread-safe non-blocking FIFO queue. The ConcurrentLinkedDeque class is similar, but additionally supports the java.util.Deque interface.Five implementations in java.util.concurrent support the extended BlockingQueue interface, that defines blocking versions of put and take: LinkedBlockingQueue, ArrayBlockingQueue, SynchronousQueue, PriorityBlockingQueue, and DelayQueue. The different classes cover the most common usage contexts for producer-consumer, messaging, parallel tasking, and related concurrent designs.Extended interface TransferQueue, and implementation LinkedTransferQueue introduce a synchronous transfer method (along with related features) in which a producer may optionally block awaiting its consumer.The BlockingDeque interface extends BlockingQueue to support both FIFO and LIFO (stack-based) operations. Class LinkedBlockingDeque provides an implementation. ConcurrentLinkedQueue类提供高效的可扩展线程安全非阻塞FIFO队列。 ConcurrentLinkedDeque类类似，但另外支持java.util.Deque接口。 java.util.concurrent中的五个实现支持扩展的BlockingQueue接口，该接口定义了put和take的阻塞版本：LinkedBlockingQueue，ArrayBlockingQueue，SynchronousQueue，PriorityBlockingQueue和DelayQueue。不同的类涵盖了生产者 - 消费者，消息传递，并行任务和相关并发设计的最常见使用上下文。扩展接口TransferQueue和实现LinkedTransferQueue引入了同步传输方法（以及相关功能），其中生产者可以选择阻止等待其消费者。 BlockingDeque接口扩展了BlockingQueue，以支持FIFO和LIFO（基于堆栈）操作。 LinkedBlockingDeque类提供了一个实现。 TimingThe TimeUnit class provides multiple granularities (including nanoseconds) for specifying and controlling time-out based operations. Most classes in the package contain operations based on time-outs in addition to indefinite waits. In all cases that time-outs are used, the time-out specifies the minimum time that the method should wait before indicating that it timed-out. Implementations make a “best effort” to detect time-outs as soon as possible after they occur. However, an indefinite amount of time may elapse between a time-out being detected and a thread actually executing again after that time-out. All methods that accept timeout parameters treat values less than or equal to zero to mean not to wait at all. To wait “forever”, you can use a value of Long.MAX_VALUE. TimeUnit类提供多个粒度（包括纳秒），用于指定和控制基于超时的操作。除了无限期等待之外，程序包中的大多数类都包含基于超时的操作。在使用超时的所有情况下，超时指定方法在指示超时之前应等待的最短时间。实施工作可以“尽最大努力”在发生超时后尽快检测到超时。然而，在检测到超时和在超时之后再次实际执行的线程之间可能经过不确定的时间量。所有接受超时参数的方法都会将值小于或等于零，以表示根本不等待。要等待“永远”，您可以使用Long.MAX_VALUE值。 SynchronizersFive classes aid common special-purpose synchronization idioms.Semaphore is a classic concurrency tool.CountDownLatch is a very simple yet very common utility for blocking until a given number of signals, events, or conditions hold.A CyclicBarrier is a resettable multiway synchronization point useful in some styles of parallel programming.A Phaser provides a more flexible form of barrier that may be used to control phased computation among multiple threads.An Exchanger allows two threads to exchange objects at a rendezvous point, and is useful in several pipeline designs. 五个类帮助常见的专用同步习语。 Semaphore是一种经典的并发工具。 CountDownLatch是一个非常简单但非常常见的实用程序，用于阻塞，直到给定数量的信号，事件或条件成立。 CyclicBarrier是一种可重置的多路同步点，在某些并行编程风格中很有用。 Phaser提供了更灵活的屏障形式，可用于控制多个线程之间的分阶段计算。 Exchanger允许两个线程在集合点交换对象，并且在多个管道设计中很有用。 Concurrent CollectionsBesides Queues, this package supplies Collection implementations designed for use in multithreaded contexts: ConcurrentHashMap, ConcurrentSkipListMap, ConcurrentSkipListSet, CopyOnWriteArrayList, and CopyOnWriteArraySet. When many threads are expected to access a given collection, a ConcurrentHashMap is normally preferable to a synchronized HashMap, and a ConcurrentSkipListMap is normally preferable to a synchronized TreeMap. A CopyOnWriteArrayList is preferable to a synchronized ArrayList when the expected number of reads and traversals greatly outnumber the number of updates to a list.The “Concurrent” prefix used with some classes in this package is a shorthand indicating several differences from similar “synchronized” classes. For example java.util.Hashtable and Collections.synchronizedMap(new HashMap()) are synchronized. But ConcurrentHashMap is “concurrent”. A concurrent collection is thread-safe, but not governed by a single exclusion lock. In the particular case of ConcurrentHashMap, it safely permits any number of concurrent reads as well as a tunable number of concurrent writes. “Synchronized” classes can be useful when you need to prevent all access to a collection via a single lock, at the expense of poorer scalability. In other cases in which multiple threads are expected to access a common collection, “concurrent” versions are normally preferable. And unsynchronized collections are preferable when either collections are unshared, or are accessible only when holding other locks.Most concurrent Collection implementations (including most Queues) also differ from the usual java.util conventions in that their Iterators and Spliterators provide weakly consistent rather than fast-fail traversal:they may proceed concurrently with other operationsthey will never throw ConcurrentModificationExceptionthey are guaranteed to traverse elements as they existed upon construction exactly once, and may (but are not guaranteed to) reflect any modifications subsequent to construction. 除了Queues之外，这个包还提供了设计用于多线程上下文的Collection实现：ConcurrentHashMap，ConcurrentSkipListMap，ConcurrentSkipListSet，CopyOnWriteArrayList和CopyOnWriteArraySet。当期望许多线程访问给定集合时，ConcurrentHashMap通常优于同步HashMap，并且ConcurrentSkipListMap通常优于同步TreeMap。当预期的读取和遍历次数远远超过列表的更新次数时，CopyOnWriteArrayList优于同步的ArrayList。与此包中的某些类一起使用的“Concurrent”前缀是一个简写，表示与类似“synchronized”类的几个不同之处。例如，java.util.Hashtable和Collections.synchronizedMap（new HashMap（））是同步的。但ConcurrentHashMap是“并发”的。并发集合是线程安全的，但不受单个排除锁的控制。在ConcurrentHashMap的特定情况下，它可以安全地允许任意数量的并发读取以及可调数量的并发写入。当您需要通过单个锁来阻止对集合的所有访问时，“同步”类可能很有用，但代价是可扩展性较差。在期望多个线程访问公共集合的其他情况下，通常优选“并发”版本。当任何集合未被共享时，或者只有在持有其他锁时才可访问非同步集合。大多数并发Collection实现（包括大多数队列）也不同于通常的java.util约定，因为它们的迭代器和Spliterator提供弱一致而不是快速失败的遍历：它们可以与其他操作同时进行，它们永远不会抛出ConcurrentModificationException它们是保证的在施工时只存在一次的横向元素，并且可以（但不保证）反映施工后的任何修改。 Memory Consistency PropertiesChapter 17 of the Java Language Specification defines the happens-before relation on memory operations such as reads and writes of shared variables. The results of a write by one thread are guaranteed to be visible to a read by another thread only if the write operation happens-before the read operation. The synchronized and volatile constructs, as well as the Thread.start() and Thread.join() methods, can form happens-before relationships. In particular:Each action in a thread happens-before every action in that thread that comes later in the program’s order.An unlock (synchronized block or method exit) of a monitor happens-before every subsequent lock (synchronized block or method entry) of that same monitor. And because the happens-before relation is transitive, all actions of a thread prior to unlocking happen-before all actions subsequent to any thread locking that monitor.A write to a volatile field happens-before every subsequent read of that same field. Writes and reads of volatile fields have similar memory consistency effects as entering and exiting monitors, but do not entail mutual exclusion locking.A call to start on a thread happens-before any action in the started thread.All actions in a thread happen-before any other thread successfully returns from a join on that thread.The methods of all classes in java.util.concurrent and its subpackages extend these guarantees to higher-level synchronization. In particular:Actions in a thread prior to placing an object into any concurrent collection happen-before actions subsequent to the access or removal of that element from the collection in another thread.Actions in a thread prior to the submission of a Runnable to an Executor happen-before its execution begins. Similarly for Callables submitted to an ExecutorService.Actions taken by the asynchronous computation represented by a Future happen-before actions subsequent to the retrieval of the result via Future.get() in another thread.Actions prior to “releasing” synchronizer methods such as Lock.unlock, Semaphore.release, and CountDownLatch.countDown happen-before actions subsequent to a successful “acquiring” method such as Lock.lock, Semaphore.acquire, Condition.await, and CountDownLatch.await on the same synchronizer object in another thread.For each pair of threads that successfully exchange objects via an Exchanger, actions prior to the exchange() in each thread happen-before those subsequent to the corresponding exchange() in another thread.Actions prior to calling CyclicBarrier.await and Phaser.awaitAdvance (as well as its variants) happen-before actions performed by the barrier action, and actions performed by the barrier action happen-before actions subsequent to a successful return from the corresponding await in other threads. Java语言规范的第17章定义了内存操作的先发生关系，例如共享变量的读写。只有在读取操作之前发生写入操作时，一个线程的写入结果才能保证对另一个线程的读取可见。 synchronized和volatile构造以及Thread.start（）和Thread.join（）方法可以形成先发生关系。特别是：线程中的每个动作都发生在该线程中的每个动作之前，该动作在程序的顺序中稍后出现。监视器的解锁（同步块或方法退出）发生在同一监视器的每个后续锁定（同步块或方法入口）之前。并且由于之前发生的关系是可传递的，因此在解锁之前线程的所有操作都会发生 - 在任何线程锁定该监视器之后的所有操作之前。在每次后续读取同一字段之前，会发生对易失性字段的写入。易失性字段的写入和读取具有与进入和退出监视器类似的内存一致性效果，但不需要互斥锁定。在启动线程中的任何操作之前发生对线程启动的调用。线程中的所有操作都发生在任何其他线程从该线程上的连接成功返回之前。 java.util.concurrent及其子包中所有类的方法将这些保证扩展到更高级别的同步。特别是：在将对象放入任何并发集合之前的线程中的操作发生在从另一个线程中的集合访问或移除该元素之后的操作之前。在向执行程序提交Runnable之前，线程中的操作发生在执行开始之前。类似地，提交给ExecutorService的Callables也是如此。由Future表示的异步计算所采取的操作发生在通过另一个线程中的Future.get（）检索结果之后的操作之前。 “释放”同步器方法（如Lock.unlock，Semaphore.release和CountDownLatch.countDown）之前的操作发生在成功“获取”方法（例如Lock.lock，Semaphore.acquire，Condition.await和CountDownLatch）之后的操作之前。 .await在另一个线程中的同一个同步器对象上。对于通过Exchanger成功交换对象的每对线程，每个线程中的exchange（）之前的操作发生在另一个线程中相应的exchange（）之后的操作之前。调用CyclicBarrier.await和Phaser.awaitAdvance（及其变体）之前的操作发生在屏障操作执行的操作之前，屏障操作执行的操作发生在从其他相应的await成功返回之后的操作之前发生线程。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo初识]]></title>
    <url>%2F2019%2F08%2F16%2FDubbo%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Dubbo 01架构模型传统架构 All in One测试麻烦，微小修改 全都得重新测 单体架构也称之为单体系统或者是单体应用。就是一种把系统中所有的功能、模块耦合在一个应用中的架构方式。其优点为：项目易于管理、部署简单。缺点：测试成本高、可伸缩性差、可靠性差、迭代困难、跨语言程度差、团队协作难 聚合项目划分 单项目容易 因为某个功能导致整体oom 拆分完 咋实现 SOA 架构: Service-Oriented Architecture面向服务的架构（SOA）是一个组件模型，它将应用程序拆分成不同功能单元（称为服务）通过这些服务之间定义良好的接口和契约联系起来。接口是采用中立的方式进行定义的，它应该独立于实现服务的硬件平台、操作系统和编程语言。这使得构建在各种各样的系统中的服务可以以一种统一和通用的方式进行交互。 在没有实施SOA的年代，从我们研发的角度来看，只能在代码级别复用，即Ctrl +V。SOA出现，我们开始走向了模块、业务线的复用。 SOA年代的典型实现： SOAP协议，CXF框架，XML传输 xsd，数据校验 SOA架构伴随着软件研发行业20年的发展，在最初的时候，大型it公司内部系统规模越来越大，IT系统越来越复杂，All in One单体架构的思想导致公司内项目业务和数据相互隔离，形成了孤岛。 最初，我们使用数据库作为项目之间数据交互和中转的平台，现在我们有了消息中间件。 最初，我们使用XML完成系统之间解耦与相互关联，现在我们有了RPC，Restful 最初，我们使用业务维度划分整体项目结构， 最初，我们多项目节点维护一个共享数据中心，现在我们做冗余存储，闭环数据，保证高效运行及数据最终一致性 最初，SOA思想指导指导我们把所有的IT系统汇总成一个大的整体，按照业务维度划分服务，集中化管理 现在我们拆分抽象服务使其可以多系统复用相同的功能模块。 基于dubbo RPC的微服务式架构RPC远程过程调用 : Remote Procedure Call Protocol 远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。 原来的RPC也有其他几种比如DCOM，CORBA，RMI（Java）等 RMI——Remote Method Invoke：调用远程的方法。“方法”一般是附属于某个对象上的，所以通常RMI指对在远程的计算机上的某个对象，进行其方法函数的调用。 RPC——Remote Procedure Call：远程过程调用。指的是对网络上另外一个计算机上的，某段特定的函数代码的调用。 传输协议 RPC，可以基于TCP协议，也可以基于HTTP协议 HTTP，基于HTTP协议 传输效率 RPC，使用自定义的TCP协议，可以让请求报文体积更小，或者使用HTTP2协议，也可以很好的减少报文的体积，提高传输效率 HTTP，如果是基于HTTP1.1的协议，请求中会包含很多无用的内容，如果是基于HTTP2.0，那么简单的封装以下是可以作为一个RPC来使用的，这时标准RPC框架更多的是服务治理 性能消耗 RPC，可以基于thrift实现高效的二进制传输 HTTP，大部分是通过json来实现的，字节大小和序列化耗时都比thrift要更消耗性能 负载均衡 RPC，基本都自带了负载均衡策略 HTTP，需要配置Nginx，HAProxy来实现 服务治理 RPC，能做到自动通知，不影响上游 HTTP，需要事先通知，修改Nginx/HAProxy配置 RPC主要用于公司内部的服务调用，性能消耗低，传输效率高，服务治理方便。HTTP主要用于对外的异构环境，浏览器接口调用，APP接口调用，第三方接口调用等。 概念Dubbo介绍Apache Dubbo |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成。Dubbo框架，是基于容器运行的。容器是Spring。 官方网站 : http://dubbo.apache.org/ 阿里巴巴已经将dubbo框架捐献给了Apache软件基金会 角色registry注册中心. 是用于发布和订阅服务的一个平台.用于替代SOA结构体系框架中的ESB服务总线的。 发布开发服务端代码完毕后, 将服务信息发布出去. 实现一个服务的公开. 订阅客户端程序,从注册中心下载服务内容 这个过程是订阅. 订阅服务的时候, 会将发布的服务所有信息,一次性下载到客户端. 客户端也可以自定义, 修改部分服务配置信息. 如: 超时的时长, 调用的重试次数等. consumer服务的消费者, 就是服务的客户端. 消费者必须使用Dubbo技术开发部分代码. 基本上都是配置文件定义. provider服务的提供者, 就是服务端. 服务端必须使用Dubbo技术开发部分代码. 以配置文件为主. container容器. Dubbo技术的服务端(Provider), 在启动执行的时候, 必须依赖容器才能正常启动. 默认依赖的就是spring容器. 且Dubbo技术不能脱离spring框架. 在2.5.3版本的dubbo中, 默认依赖的是spring2.5版本技术. 可以选用spring4.5以下版本. 在2.5.7版本的dubbo中, 默认依赖的是spring4.3.10版本技术. 可以选择任意的spring版本. monitor dubbo admin监控中心. 是Dubbo提供的一个jar工程. 主要功能是监控服务端(Provider)和消费端(Consumer)的使用数据的. 如: 服务端是什么,有多少接口,多少方法, 调用次数, 压力信息等. 客户端有多少, 调用过哪些服务端, 调用了多少次等. 执行流程 start: 启动Spring容器时,自动启动Dubbo的Provider register: Dubbo的Provider在启动后自动会去注册中心注册内容.注册的内容包括: Provider的 IP Provider 的端口. Provider 对外提供的接口列表.哪些方法.哪些接口类 Dubbo 的版本. 访问Provider的协议. subscribe: 订阅.当Consumer启动时,自动去Registry获取到所已注册的服务的信息. notify: 通知.当Provider的信息发生变化时, 自动由Registry向Consumer推送通知. invoke: 调用. Consumer 调用Provider中方法 同步请求.消耗一定性能.但是必须是同步请求,因为需要接收调用方法后的结果. count:次数. 每隔2分钟,provoider和consumer自动向Monitor发送访问次数.Monitor进行统计. 协议Dubbo协议(官方推荐协议)优点： 采用NIO复用单一长连接，并使用线程池并发处理请求，减少握手和加大并发效率，性能较好（推荐使用） 缺点： 大文件上传时,可能出现问题(不使用Dubbo文件上传) RMI(Remote Method Invocation)协议优点: JDK自带的能力。可与原生RMI互操作，基于TCP协议 缺点: 偶尔连接失败. Hessian协议优点: 可与原生Hessian互操作，基于HTTP协议 缺点: 需hessian.jar支持，http短连接的开销大 注册中心Zookeeper(官方推荐)优点: 支持分布式.很多周边产品. 缺点: 受限于Zookeeper软件的稳定性.Zookeeper专门分布式辅助软件,稳定较优 Multicast优点: 去中心化,不需要单独安装软件. 缺点: 2.2.1 Provider和Consumer和Registry不能跨机房(路由) Redis优点: 支持集群,性能高 缺点: 要求服务器时间同步.否则可能出现集群失败问题. Simple优点: 标准RPC服务.没有兼容问题 缺点: 不支持集群. 组件选型及成熟度http://dubbo.apache.org/zh-cn/docs/user/maturity.html 功能成熟度 Feature Maturity Strength Problem Advise User 并发控制 Tested 并发控制 试用 连接控制 Tested 连接数控制 试用 直连提供者 Tested 点对点直连服务提供方，用于测试 测试环境使用 Alibaba 分组聚合 Tested 分组聚合返回值，用于菜单聚合等服务 特殊场景使用 可用于生产环境 参数验证 Tested 参数验证，JSR303验证框架集成 对性能有影响 试用 LaiWang 结果缓存 Tested 结果缓存，用于加速请求 试用 泛化引用 Stable 泛化调用，无需业务接口类进行远程调用，用于测试平台，开放网关桥接等 可用于生产环境 Alibaba 泛化实现 Stable 泛化实现，无需业务接口类实现任意接口，用于Mock平台 可用于生产环境 Alibaba 回声测试 Tested 回声测试 试用 隐式传参 Stable 附加参数 可用于生产环境 异步调用 Tested 不可靠异步调用 试用 本地调用 Tested 本地调用 试用 参数回调 Tested 参数回调 特殊场景使用 试用 Registry 事件通知 Tested 事件通知，在远程调用执行前后触发 试用 本地存根 Stable 在客户端执行部分逻辑 可用于生产环境 Alibaba 本地伪装 Stable 伪造返回结果，可在失败时执行，或直接执行，用于服务降级 需注册中心支持 可用于生产环境 Alibaba 延迟暴露 Stable 延迟暴露服务，用于等待应用加载warmup数据，或等待spring加载完成 可用于生产环境 Alibaba 延迟连接 Tested 延迟建立连接，调用时建立 试用 Registry 粘滞连接 Tested 粘滞连接，总是向同一个提供方发起请求，除非此提供方挂掉，再切换到另一台 试用 Registry 令牌验证 Tested 令牌验证，用于服务授权 需注册中心支持 试用 路由规则 Tested 动态决定调用关系 需注册中心支持 试用 配置规则 Tested 动态下发配置，实现功能的开关 需注册中心支持 试用 访问日志 Tested 访问日志，用于记录调用信息 本地存储，影响性能，受磁盘大小限制 试用 分布式事务 Research JTA/XA三阶段提交事务 不稳定 不可用 策略成熟度 Feature Maturity Strength Problem Advise User Zookeeper注册中心 Stable 支持基于网络的集群方式，有广泛周边开源产品，建议使用dubbo-2.3.3以上版本（推荐使用） 依赖于Zookeeper的稳定性 可用于生产环境 Redis注册中心 Stable 支持基于客户端双写的集群方式，性能高 要求服务器时间同步，用于检查心跳过期脏数据 可用于生产环境 Multicast注册中心 Tested 去中心化，不需要安装注册中心 依赖于网络拓扑和路由，跨机房有风险 小规模应用或开发测试环境 Simple注册中心 Tested Dogfooding，注册中心本身也是一个标准的RPC服务 没有集群支持，可能单点故障 试用 Feature Maturity Strength Problem Advise User Simple监控中心 Stable 支持JFreeChart统计报表 没有集群支持，可能单点故障，但故障后不影响RPC运行 可用于生产环境 Feature Maturity Strength Problem Advise User Dubbo协议 Stable 采用NIO复用单一长连接，并使用线程池并发处理请求，减少握手和加大并发效率，性能较好（推荐使用） 在大文件传输时，单一连接会成为瓶颈 可用于生产环境 Alibaba Rmi协议 Stable 可与原生RMI互操作，基于TCP协议 偶尔会连接失败，需重建Stub 可用于生产环境 Alibaba Hessian协议 Stable 可与原生Hessian互操作，基于HTTP协议 需hessian.jar支持，http短连接的开销大 可用于生产环境 Feature Maturity Strength Problem Advise User Netty Transporter Stable JBoss的NIO框架，性能较好（推荐使用） 一次请求派发两种事件，需屏蔽无用事件 可用于生产环境 Alibaba Mina Transporter Stable 老牌NIO框架，稳定 待发送消息队列派发不及时，大压力下，会出现FullGC 可用于生产环境 Alibaba Grizzly Transporter Tested Sun的NIO框架，应用于GlassFish服务器中 线程池不可扩展，Filter不能拦截下一Filter 试用 Feature Maturity Strength Problem Advise User Hessian Serialization Stable 性能较好，多语言支持（推荐使用） Hessian的各版本兼容性不好，可能和应用使用的Hessian冲突，Dubbo内嵌了hessian3.2.1的源码 可用于生产环境 Alibaba Dubbo Serialization Tested 通过不传送POJO的类元信息，在大量POJO传输时，性能较好 当参数对象增加字段时，需外部文件声明 试用 Json Serialization Tested 纯文本，可跨语言解析，缺省采用FastJson解析 性能较差 试用 Java Serialization Stable Java原生支持 性能较差 可用于生产环境 Feature Maturity Strength Problem Advise User Javassist ProxyFactory Stable 通过字节码生成代替反射，性能比较好（推荐使用） 依赖于javassist.jar包，占用JVM的Perm内存，Perm可能要设大一些：java -XX:PermSize=128m 可用于生产环境 Alibaba Jdk ProxyFactory Stable JDK原生支持 性能较差 可用于生产环境 Feature Maturity Strength Problem Advise User Failover Cluster Stable 失败自动切换，当出现失败，重试其它服务器，通常用于读操作（推荐使用） 重试会带来更长延迟 可用于生产环境 Alibaba Failfast Cluster Stable 快速失败，只发起一次调用，失败立即报错,通常用于非幂等性的写操作 如果有机器正在重启，可能会出现调用失败 可用于生产环境 Alibaba Failsafe Cluster Stable 失败安全，出现异常时，直接忽略，通常用于写入审计日志等操作 调用信息丢失 可用于生产环境 Monitor Failback Cluster Tested 失败自动恢复，后台记录失败请求，定时重发，通常用于消息通知操作 不可靠，重启丢失 可用于生产环境 Registry Forking Cluster Tested 并行调用多个服务器，只要一个成功即返回，通常用于实时性要求较高的读操作 需要浪费更多服务资源 可用于生产环境 Broadcast Cluster Tested 广播调用所有提供者，逐个调用，任意一台报错则报错，通常用于更新提供方本地状态 速度慢，任意一台报错则报错 可用于生产环境 Feature Maturity Strength Problem Advise User Random LoadBalance Stable 随机，按权重设置随机概率（推荐使用） 在一个截面上碰撞的概率高，重试时，可能出现瞬间压力不均 可用于生产环境 Alibaba RoundRobin LoadBalance Stable 轮询，按公约后的权重设置轮询比率 存在慢的机器累积请求问题，极端情况可能产生雪崩 可用于生产环境 LeastActive LoadBalance Stable 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差，使慢的机器收到更少请求 不支持权重，在容量规划时，不能通过权重把压力导向一台机器压测容量 可用于生产环境 ConsistentHash LoadBalance Stable 一致性Hash，相同参数的请求总是发到同一提供者，当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动 压力分摊不均 可用于生产环境 Feature Maturity Strength Problem Advise User 条件路由规则 Stable 基于条件表达式的路由规则，功能简单易用 有些复杂多分支条件情况，规则很难描述 可用于生产环境 Alibaba 脚本路由规则 Tested 基于脚本引擎的路由规则，功能强大 没有运行沙箱，脚本能力过于强大，可能成为后门 试用 Feature Maturity Strength Problem Advise User Spring Container Stable 自动加载META-INF/spring目录下的所有Spring配置 可用于生产环境 Alibaba Jetty Container Stable 启动一个内嵌Jetty，用于汇报状态 大量访问页面时，会影响服务器的线程和内存 可用于生产环境 Alibaba Log4j Container Stable 自动配置log4j的配置，在多进程启动时，自动给日志文件按进程分目录 用户不能控制log4j的配置，不灵活 可用于生产环境 Alibaba zookeeper安装 安装jdk rpm -ivh jdk-8u131-linux-x64.rpm java –version 检查是否成功 安装Zookeeper下载 http://zookeeper.apache.org/ 并上传解压缩 配置zookeeper环境变量修改文件 vi /etc/profile 追加内容 在path后追加/usr/local/zookeeper/bin 注意中间间隔是：冒号 export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/zo okeeper/bin source /etc/profile 重新加载配置 修改zoo.cfg重命名zoo_sample.cfg 为zoo.cfg 默认加载配置文件会找zoo.cfg这个文件 修改配置文件 vi /usr/local/zookeeper/conf/zoo.cfg 创建数据存放目录 Mkdir /data/zookeeper 创建Myid文件，并写入服务器编号 注意：这里写入myid文件的编号和接下来要配置的服务器列表编号一一对应，每台服务器配置的编号也应该不一样。 Myid文件里只有一个数字 1 创建好的这个目录用于存储zookeeper产生的数据 修改datadir为刚才我们创建的目录 dataDir=/data/zookeeper 在最后面加入集群服务器列表 server.1=192.168.2.51:2888:3888 server.2=cm02:2888:3888 server.3=cm03:2888:3888 配置的服务器集群个数建议是奇数的 半数以上节点存活，就可以对外提供服务 其中 server.x 这里的数字编号 就是我们的myid里写入的数字 Cm01是主机名或ip地址 接下来是对外通讯端口和内部选举端口 启动zkServer.sh start 命令启动一台zookeeper服务器 没报错的话 使用jps看一下进程 QuorumPeerMain是zookeeper的主进程 通过status命令可以查看服务器运行状态 注意：当我们使用集群模式启动zookeeper的时候，由于我们只启动了一台服务器，集群总共3台，没有满足zookeeper半数以上节点运行原则，所以服务虽然起来了，但是没有办法对外提供服务。 这时我们需要启动第二台服务器 Dubbo Hello World环境SpringBoot + dubbo Pom.xml 依赖&lt;!-- Aapche Dubbo --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${dubbo.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;${dubbo.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.curator/curator-framework --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.14&lt;/version&gt; &lt;/dependency&gt; 服务提供方 provider配置文件server.port=8081 spring.application.name=DemoProvider dubbo.scan.base-packages=com.msb.db1.service dubbo.protocol.name=dubbo dubbo.protocol.port=666 dubbo.protocol.host=192.168.101.106 dubbo.registry.address=zookeeper://192.168.150.13:2181 服务接口public interface DemoService { String sayHello(String name); } 接口实现import org.apache.dubbo.config.annotation.Service; import org.springframework.stereotype.Component; @Service(version = &quot;1.0.0&quot; ,timeout = 10000, interfaceClass = DemoService.class) @Component public class DemoServiceImpl implements DemoService { @Override public String sayHello(String name) { // TODO Auto-generated method stub System.out.println(&quot;来啦~~~！&quot;); return &quot;hello:&quot; + name; } } 服务消费方 customer配置spring.application.name=DemoCustomer dubbo.scan.base-packages=com.msb.db1.service dubbo.registry.address=zookeeper://192.168.150.13:2181 自动注入@Reference(version = &quot;1.0.0&quot;) DemoService serv; 接口public interface DemoService { String sayHello(String name); } 本地存根（Stub） 服务端的骨架对象(Skeleton)]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastDFS集群]]></title>
    <url>%2F2019%2F08%2F16%2FFastDFS%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[FastDFS 集群 克隆虚拟机VMware修改mac 修改 ip地址 rm -f /etc/udev/rules.d/70-persistent-net.rules reboot Tracker集群搭建克隆出来之前已经装好的两台虚拟机做Tracker节点 tarcker 节点 ip 131、132 启动两个Tracker节点 /etc/init.d/fdfs_trackerd start 查看端口 netstat -unltp |grep fdfs 启动日志 tail -100f trackerd.log Storage集群搭建复制4台虚拟机 第一组 ip： 181、182 对应 group1 第二组 ip： 191、192 对应 group2 修改配置文件/etc/fdfs/storage.conf 修改tracker_server 的ip地址，多个 tracker 直接添加多条配置 tracker_server=192.168.150.131:22122 tracker_server=192.168.150.132:22122 启动Storage服务 /etc/init.d/fdfs_storaged start fdfsMonitor /usr/bin/fdfs_monitor /etc/fdfs/storage.conf 查看端口：netstat -unltp | grep fdfs 使用FastDFS中的Monitor查看：在所有的Storage启动成功后执行下述命令 /usr/bin/fdfs_monitor /etc/fdfs/storage.conf 测试FastDfs集群测试文件上传 fdfs_upload_file /etc/fdfs/client.conf tmp.sh 修改tracker.conf文件中group的负载策略 修改配置文件中的store_lookup，这个属性的可选值有0,1,2。分别代表： # 0: 轮询 # 1: 指定卷，通过卷名指定访问某一个卷 # 2: 访问空闲磁盘空间较大的。是默认的负载策略。 Nginx集群搭建FastDFS 通过 Tracker 服务器,将文件放在 Storage 服务器存储，但是同组存储服务器之间需要进入文件复制，有同步延迟的问题。假设 Tracker 服务器将文件上传到了 S1，上传成功后文件 ID已经返回给客户端。 此时 FastDFS 存储集群机制会将这个文件同步到同组存储 S2，在文件还没有复制完成的情况下，客户端如果用这个文件 ID 在 S2 上取文件,就会出现文件无法访问的错误。 而 fastdfs-nginx-module 可以重定向文件连接到源服务器（S1）取文件,避免客户端由于复制延迟导致的文件无法访问错误。 Storage节点Nginx反向代理在每个Storage节点安装配置Nginx 修改配置文件mod_fastdfs.confconnect_timeout=10 #181、182 对应 group 1 #191、192 对应 group 2 group_name= tracker_server=192.168.150.131:22122 tracker_server=192.168.150.132:22122 group_count = 2 # group settings for group #1 # since v1.14 # when support multi-group, uncomment following section [group1] group_name=group1 storage_server_port=23000 store_path_count=1 store_path0=/var/data/fastdfs-storage/store #store_path1=/home/yuqing/fastdfs1 # group settings for group #2 # since v1.14 # when support multi-group, uncomment following section as neccessary [group2] group_name=group2 storage_server_port=23000 store_path_count=1 store_path0=/var/data/fastdfs-storage/store 检查Nginx配置文件端口80必须和Storage服务器中的/etc/fdfs/storage.conf配置文件中的http.server_port=80一致。 listen 80; location ~ /group([0-9])/M00 { # add_header Content-Disposition &quot;attachment;filename=$arg_attname&quot;; ngx_fastdfs_module; } 测试访问文件 报错 ERROR - file: /root/fastdfs-nginx-module/src//common.c, line: 709, expect parameter token or ts in url, uri: / group1/M00/00/00/wKiWtV1H4eKAKE4YAAABAE3E6HQ3627.gif 检查防盗链系统 vi /etc/fdfs/http.conf http.anti_steal.check_token= Tracker节点 负载均衡反向代理在每个Tracker节点安装配置Nginx 在 tracker 上安装的 nginx 主要为了提供 http 访问的反向代理、负载均衡以及缓存服务。 Nginx缓存传并解压ngx_cache_purge-2.3.tar.gzcd /root tar -zxf ngx_cache_purge-2.3.tar.gz 编译安装nginx./configure --prefix=/usr/local/tengine --add-module=/root/ngx_cache_purge-2.3 make make install 配置nginxngx_cache_purge模块的作用：用于清除指定url的缓存 下载地址：http://labs.frickle.com/files/ngx_cache_purge-2.3.tar.gz Nginx.conf http { include mime.types; default_type application/octet-stream; #log_format main &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; # &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; # &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;; #access_log logs/access.log main; #access_log &quot;pipe:rollback logs/access_log interval=1d baknum=7 maxsize=2G&quot; main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #设置缓存 server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 300m; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; #设置缓存存储路径、存储方式、分配内存大小、磁盘最大空间、缓存期限 #levels=1:2 表示缓存文件有两级目录 1表示第一级目录名为1位数，2表示第二级目录名为2位数 proxy_cache_path /var/data/cache/nginx/proxy_cache levels=1:2 #keys_zone 缓存区域名字，分配200m空间，最大缓存1g,有效期30天 keys_zone=http-cache:200m max_size=1g inactive=30d; proxy_temp_path /var/data/cache/nginx/proxy_cache/tmp; #设置 group1 的服务器 upstream fdfs_group1 { server 192.168.150.181:80 weight=1 max_fails=2 fail_timeout=30s; server 192.168.150.18:80 weight=1 max_fails=2 fail_timeout=30s; } #设置 group2 的服务器 upstream fdfs_group2 { server 192.168.150.191:80 weight=1 max_fails=2 fail_timeout=30s; server 192.168.150.192:80 weight=1 max_fails=2 fail_timeout=30s; } #gzip on; listen 80; #charset koi8-r; #access_log logs/host.access.log main; #access_log &quot;pipe:rollback logs/host.access_log interval=1d baknum=7 maxsize=2G&quot; main; server_name localhost; location /group1/M00 { proxy_next_upstream http_502 http_504 error timeout invalid_header; proxy_cache http-cache; proxy_cache_valid 200 304 12h; proxy_cache_key $uri$is_args$args; proxy_pass http://fdfs_group1; expires 30d; } location /group2/M00 { proxy_next_upstream http_502 http_504 error timeout invalid_header; proxy_cache http-cache; proxy_cache_valid 200 304 12h; proxy_cache_key $uri$is_args$args; proxy_pass http://fdfs_group2; expires 30d; } #设置清除缓存的访问权限 location ~/purge(/.*) { allow 127.0.0.1; allow 192.168.2.0/24; deny all; proxy_cache_purge http-cache $1$is_args$args; } purge命令清除静态缓存http://域名+purge+静态资源相对路径 来清除静态资源缓存 添加Nginx模块,保留原有模块查看nginx编译安装时的命令，安装了哪些模块 命令/usr/local/nginx/sbin/nginx -V 编译 ./configure --prefix=/usr/local/tengine --add-module=/root/ngx_c --以前的模块 ache_purge-2.3 make 注意make完 不要make install 拷贝obj下的 Nginx文件到 已安装好的sbin目录下 覆盖即可 cp ./nginx /usr/local/tengine/sbin/ 高可用keepalived安装上传并解压keepalived-1.2.18.tar.gz cd /root/ tar -zxf keepalived-1.2.18.tar.gz 编译并安装Keepalived cd /root/keepalived-1.2.18 ./configure --prefix=/usr/local/keepalived make &amp;&amp; make install Keepalived安装成Linux服务mkdir /etc/keepalived cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/ cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ ln -s /usr/local/sbin/keepalived /usr/sbin/ ln -s /usr/local/keepalived/sbin/keepalived /sbin/ 设置开机启动 chkconfig keepalived on 修改keepalived.confts1 ! Configuration File for keepalived global_defs { ## keepalived 自带的邮件提醒需要开启 sendmail 服务。建议用独立的监控或第三方 SMTP router_id NK1 ## 标识本节点的字条串，通常为 hostname } ## keepalived 会定时执行脚本并对脚本执行的结果进行分析，动态调整 vrrp_instance 的优先级。如果脚本执行结果为 0，并且 weight 配置的值大于 0，则优先级相应的增加。如果脚本执行结果非 0，并且 weight配置的值小于 0，则优先级相应的减少。其他情况，维持原本配置的优先级，即配置文件中 priority 对应的值。 vrrp_script chk_nginx { script &quot;/etc/keepalived/nginx_check.sh&quot; ## 检测 nginx 状态的脚本路径 interval 2 ## 检测时间间隔 weight -20 ## 如果条件成立，权重-20 } ## 定义虚拟路由，VI_1 为虚拟路由的标示符，自己定义名称 vrrp_instance VI_1 { state MASTER ## 主节点为 MASTER，对应的备份节点为 BACKUP interface eth1 ## 绑定虚拟 IP 的网络接口，与本机 IP 地址所在的网络接口相同，我的是 eth1 virtual_router_id 51 ## 虚拟路由的 ID 号，两个节点设置必须一样，可选 IP 最后一段使用, 相同的 VRID 为一个组，他将决定多播的 MAC 地址 mcast_src_ip 192.168.2.117 ## 本机 IP 地址 priority 100 ## 节点优先级，值范围 0-254，MASTER 要比BACKUP 高 nopreempt ## 优先级高的设置 nopreempt 解决异常恢复后再次抢占的问题 advert_int 1 ## 组播信息发送间隔，两个节点设置必须一样，默认 1s ## 设置验证信息，两个节点必须一致 authentication { auth_type PASS auth_pass 1111 ## 真实生产，按需求对应该过来 } ## 将 track_script 块加入instance 配置块 track_script { chk_nginx ## 执行 Nginx 监控的服务 } ## 虚拟 IP 池, 两个节点设置必须一样 virtual_ipaddress { 192.168.150.138/24 dev eth0 label eth0:2 } } ts2 ! Configuration File for keepalived global_defs { # notification_email { # acassen@firewall.loc # failover@firewall.loc # sysadmin@firewall.loc # } # notification_email_from Alexandre.Cassen@firewall.loc # smtp_server 192.168.200.1 # smtp_connect_timeout 30 router_id ts2 } vrrp_script chk_nginx { script &quot;/etc/keepalived/nginx_check.sh&quot; ## 检测 nginx 状态的脚本路径 interval 2 ## 检测时间间隔 weight -20 ## 如果条件成立，权重-20 } vrrp_instance VI_1 { state BACKUP interface eth0 virtual_router_id 51 priority 90 advert_int 1 } vrrp_script chk_nginx { script &quot;/etc/keepalived/nginx_check.sh&quot; ## 检测 nginx 状态的脚本路径 router_id ts2 } vrrp_script chk_nginx { script &quot;/etc/keepalived/nginx_check.sh&quot; ## 检测 nginx 状态的脚本路径 interval 2 ## 检测时间间隔 weight -20 ## 如果条件成立，权重-20 } vrrp_instance VI_1 { state BACKUP interface eth0 virtual_router_id 51 priority 90 advert_int 1 mcast_src_ip 192.168.150.132 track_script { chk_nginx } authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.150.138/24 dev eth0 label eth0:2 } } 状态检查脚本编写nginx状态检查脚本/etc/keepalived/nginx_check.sh 在Keepalived配置中已用。脚本要求：如果 nginx 停止运行，尝试启动，如果无法启动则杀死本机的 keepalived 进程， keepalied将虚拟 ip 绑定到 BACKUP 机器上。 vi /etc/keepalived/nginx_check.sh 内容 #!/bin/bash A=`ps -C nginx –no-header |wc -l` if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived fi fi 权限 chmod +x /etc/keepalived/nginx_check.sh 启动 service keepalived start 下载文件@RequestMapping(&quot;/down&quot;) @ResponseBody public ResponseEntity&lt;byte[]&gt; down(HttpServletResponse resp) { DownloadByteArray cb = new DownloadByteArray(); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_OCTET_STREAM); headers.setContentDispositionFormData(&quot;attachment&quot;, &quot;aaa.xx&quot;); byte[] bs = fc.downloadFile(&quot;group1&quot;, &quot;M00/00/00/wKiWDV0vAb-AcOaYABf1Yhcsfws9181.xx&quot;, cb); return new ResponseEntity&lt;&gt;(bs,headers,HttpStatus.OK); }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastDFS]]></title>
    <url>%2F2019%2F08%2F16%2FFastDFS%2F</url>
    <content type="text"><![CDATA[FastDFS介绍技术论坛： http://bbs.chinaunix.net/forum-240-1.html FAQ：http://bbs.chinaunix.net/thread-1920470-1-1.html 资源地址： https://sourceforge.net/projects/fastdfs/ 源码资源： https://github.com/happyfish100 FastDFS是一个开源的轻量级分布式文件系统，它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。 FastDFS为互联网量身定制，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。 FastDFS服务端有两个角色：跟踪器（tracker）和存储节点（storage）。跟踪器主要做调度工作，在访问上起负载均衡的作用。 存储节点存储文件，完成文件管理的所有功能：就是这样的存储、同步和提供存取接口，FastDFS同时对文件的metadata进行管理。所谓文件的meta data就是文件的相关属性，以键值对（key value）方式表示，如：width=1024，其中的key为width，value为1024。文件metadata是文件属性列表，可以包含多个键值对。 跟踪器和存储节点都可以由一台或多台服务器构成。跟踪器和存储节点中的服务器均可以随时增加或下线而不会影响线上服务。其中跟踪器中的所有服务器都是对等的，可以根据服务器的压力情况随时增加或减少。 为了支持大容量，存储节点（服务器）采用了分卷（或分组）的组织方式。存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了冗余备份和负载均衡的作用。 在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。 当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。 FastDFS中的文件标识分为两个部分：卷名和文件名，二者缺一不可。 web项目架构 架构 tracker Server： 主节点，跟踪服务器，主要做调度工作，在访问上起负载均衡的作用。 记录storage server的状态，是连接Client和Storage server的枢纽。 FastDFS集群中的Tracker server可以有多台，Trackerserver之间是相互平等关系同时提供服务 Trackerserver不存在单点故障。客户端请求Trackerserver采用轮询方式，如果请求的tracker无法提供服务则换另一个tracker。 Storage Server： 存储服务器，文件和meta data都保存到存储服务器上 storage集群由一个或多个组构成，集群存储总容量为集群中所有组的存储容量之和。 一个组由一台或多台存储服务器组成，组内的Storage server之间是平等关系 不同组的Storageserver之间不会相互通信，同组内的Storageserver之间会相互连接进行文件同步，从而保证同组内每个storage上的文件完全一致的。 一个组的存储容量为该组内存储服务器容量最小的那个，由此可见组内存储服务器的软硬件配置最好是一致的。 Storage server会连接集群中所有的Tracker server，定时向他们报告自己的状态，包括磁盘剩余空间、文件同步状况、文件上传下载次数等统计信息。 group：组，也称为卷。同组内服务器上的文件是完全相同的 文件标识：包括两部分：组名和文件名（包含路径） meta data：文件相关属性，键值对（Key Value Pair）方式，如：width=1024,heigth=768 上传流程 client询问tracker上传到的storage，不需要附加参数； tracker返回一台可用的storage； client直接和storage通讯完成文件上传 内部机制如下： 1、选择tracker server 当集群中不止一个tracker server时，由于tracker之间是完全对等的关系，客户端在upload文件时可以任意选择一个trakcer。选择存储的group当tracker接收到upload file的请求时，会为该文件分配一个可以存储该文件的group，支持如下选择group的规则： 1、Round robin，所有的group间轮询 2、Specified group，指定某一个确定的group 3、Load balance，剩余存储空间多多group优先 2、选择storage server 当选定group后，tracker会在group内选择一个storage server给客户端，支持如下选择storage的规则： 1、Round robin，在group内的所有storage间轮询 2、First server ordered by ip，按ip排序 3、First server ordered by priority，按优先级排序（优先级在storage上配置） 3、选择storage path 当分配好storage server后，客户端将向storage发送写文件请求，storage将会为文件分配一个数据存储目录，支持如下规则： 1、Round robin，多个存储目录间轮询 2、剩余存储空间最多的优先 4、生成Fileid 选定存储目录之后，storage会为文件生一个Fileid，由storage server ip、文件创建时间、文件大小、文件crc32和一个随机数拼接而成，然后将这个二进制串进行base64编码，转换为可打印的字符串。选择两级目录当选定存储目录之后，storage会为文件分配一个fileid，每个存储目录下有两级256*256的子目录，storage会按文件fileid进行两次hash（猜测），路由到其中一个子目录，然后将文件以fileid为文件名存储到该子目录下。 5、生成文件名 当文件存储到某个子目录后，即认为该文件存储成功，接下来会为该文件生成一个文件名，文件名由group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。 下载流程 client询问tracker下载文件的storage，参数为文件标识（组名和文件名）； tracker返回一台可用的storage； client直接和storage通讯完成文件下载。 binlog每个storage写文件后，同时会写一份binlog，binlog里不包含文件数据，只包含文件名等元信息，这份binlog用于后台同步，storage会记录向group内其他storage同步的进度，以便重启后能接上次的进度继续同步；进度以时间戳的方式进行记录，所以最好能保证集群内所有server的时钟保持同步。 FastDFS和其他文件存储的简单对比 指标 FastDFS NFS 集中存储设备 如NetApp ,NAS 线性扩容性 高 差 差 文件高并发访问性能 高 差 一般 文件访问方式 专有API POSIX 支持POSIX 硬件成本 较低 中等 高 相同内容文件只保存一份 支持 不支持 不支持 指标 FastDFS mogileFS 系统简洁性 简洁 只有两个角色：tracker和storage 一般 有三个角色：tracker、storage和存储文件信息的mysql db 系统性能 很高（没有使用数据库，文件同步直接点对点，不经过tracker中转） 高（使用mysql来存储文件索引等信息，文件同步通过tracker调度和中转） 系统稳定性 高（C语言开发，可以支持高并发和高负载） 一般（Perl语言开发，高并发和高负载支持一般） 软RAID方式 分组（组内冗余），灵活性较大 动态冗余，灵活性一般 通信协议 专有协议 下载文件支持HTTP HTTP 技术文档 较详细 较少 文件附加属性（meta data） 支持 不支持 相同内容文件只保存一份 支持 不支持 下载文件时支持文件偏移量 支持 不支持 单机安装准备linux服务器或虚拟机 Tracker 和 Storage 安装在一台机器上 版本FastDFS 5.08版本 安装FastDFS依赖FastDFS是C语言开发的应用。安装必须使用make、cmake和gcc编译器。 yum install -y make cmake gcc gcc-c++ 安装FastDFS核心库libfastcommon是从FastDFS 和FastDHT 中提取出来的公共C函数库 上传文件后解压缩unzip libfastcommon-master.zip -d /usr/local/fastdfs 编译安装libfastmon没有提供make命令安装文件。使用的是shell脚本执行编译和安装。 shell脚本为make.sh 编译 sh make.sh 安装 sh make.sh install 有固定的默认安装位置。在/usr/lib64和/usr/include/fastcommon两个目录中。 创建软连接因为 FastDFS 主程序设置的 lib 目录是/usr/local/lib，所以需要创建软链接 ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so FastDFS主程序安装上传并解压缩tar -zxvf FastDFS_v5.08.tar.gz -C /usr/local/fastdfs 编译安装./make.sh ./make.sh install 安装后，FastDFS主程序所在位置： /usr/bin - 可执行文件所在位置。 /etc/fdfs - 配置文件所在位置。 /usr/lib64 - 主程序代码所在位置 /usr/include/fastdfs - 包含的一些插件组所在位置 服务配置程序脚本在/etc/init.d/目录中，脚本文件是 fdfs-storaged和fdfs-trackerd 配置文件配置文件在/etc/fdfs/目录中 tracker.conf.sample - 跟踪器服务配置文件模板 storage.conf.sample - 存储服务器配置文件模板 client.conf.sample - FastDFS提供的命令行客户端配置文件模板。可以通过命令行测试FastDFS有效性。 Tracker 服务修改配置文件复制一份模板配置文件 cd /etc/fdfs cp tracker.conf.sample tracker.conf 打开 tracker.conf 修改 base_path 路径，base_path FastDFSTracker启动后使用的根目录，用来存放Tracker data和logs。 base_path=/home/yuqing/fastdfs -&gt; base_path=/var/data/fastdfs-tracker（自定义目录） 配置中的路径需要先创建好才能启动服务 mkdir -p /var/data/fastdfs-tracker 启动Tracker/etc/init.d/fdfs_trackerd start 启动成功后，配置文件中base_path指向的目录中出现FastDFS服务相关数据目录（data目录、logs目录） 查看服务状态ps -ef | grep fdfs 停止服务/etc/init.d/fdfs_trackerd stop 重启服务/etc/init.d/fdfs_trackerd restart 启动 Storage配置文件cd /etc/fdfs cp storage.conf.sample storage.conf mkdir -p /var/data/fastdfs-storage/base mkdir -p /var/data/fastdfs-storage/store base_path=/home/yuqing/fastdfs -&gt; base_path=/var/data/fastdfs-storage/base（自定义目录） store_path0=/home/yuqing/fastdfs -&gt; store_path0=/var/data/fastdfs-storage/store（自定义目录） tracker_server=192.168.150.11:22122 -&gt; tracker_server=tracker服务IP:22122 base_path - 基础路径。用于保存storage server基础数据内容和日志内容的目录。 store_path0 - 存储路径。是用于保存FastDFS中存储文件的目录，就是要创建256*256个子目录的位置。base_path和store_path0可以使用同一个目录。 tracker_server - 跟踪服务器位置。就是跟踪服务器的ip和端口。 启动服务要求tracker服务必须已启动 /etc/init.d/fdfs_storaged start 启动成功后，配置文件中base_path指向的目录中出现FastDFS服务相关数据目录（data目录、logs目录） 配置文件中的store_path0指向的目录中同样出现FastDFS存储相关数据录（data目录） 其中$store_path0/data/目录中默认创建若干子孙目录（两级目录层级总计256*256个目录），是用于存储具体文件数据的。 Storage服务器启动比较慢，因为第一次启动的时候，需要创建256*256个目录。 查看服务状态/etc/init.d/fdfs_storaged status 停止服务/etc/init.d/fdfs_storaged stop 重启服务/etc/init.d/fdfs_storaged restart Client修改配置文件cd /etc/fdfs cp client.conf.sample client.conf client.conf配置文件中主要描述客户端的行为，需要进行下述修改： vi /etc/fdfs/client.conf base_path=/home/yuqing/fastdfs -&gt; base_path=/fastdfs/client （自定义目录） tracker_server=192.168.150.11:22122 -&gt; tracker_server=tracker服务IP:22122 base_path - 就是客户端命令行执行过程时临时数据存储位置。 创建自定义目录 mkdir -p /fastdfs/client 上传文件/usr/local/bin/fdfs_upload_file /etc/fdfs/client.conf /要上传的文件 [root@node03 data]# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /root/install.log group1/M00/00/00/wKiWDV0xfqWAFe1OAAAib-i5DLU637.log 上传结束后，返回group1/M00/00/00/xxxxxxxxxx.xxx，检查storage服务结点中的$store_path0/data/00/00/目录中是否有上传的文件（一般情况上传的文件按顺序保存在$store_path0/data/00/00/目录中，不能完全保证）。 上传文件结果：group1/M00/00/00/wKiWDV0xfqWAFe1OAAAib-i5DLU637.log 组名：group1文件上传后所在的storage组名称，在文件上传成功后有storage服务器返回，需要客户端自行保存。 虚拟磁盘路径：M00 storage配置的虚拟路径，与磁盘选项store_path*对应。如果配置了store_path0则是M00，如果配置了store_path1则是M01，以此类推。 数据两级目录：/00/00 storage服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件。 文件名：wKiWDV0xfqWAFe1OAAAib-i5DLU637.log 删除文件/usr/bin/fdfs_delete_file /etc/fdfs/client.conf group1/M00/00/00/wKiWDV0xfqWAFe1OAAAib-i5DLU637.log Nginx组件如果FastDFS中保存的是图片信息。希望在WEB应用中可以直接访问FastDFS中的图片进行显示。如果操作？ 安装Nginx是为了WEB应用中可以使用HTTP协议直接访问Storage服务中存储的文件。在storage结点所在服务器安装Nginx组件。 需要安装两部分内容。 Nginx应用，在安装nginx应用的时候，同时要在nginx中增加一个FastDFS的组件。 fastdfs-nginx-module模块上传并解压 tar -zxf fastdfs-nginx-module_v1.16.tar.gz 修改配置vi /usr/local/fastdfs/fastdfs-nginx-module/src/config CORE_INCS=&quot;$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/&quot; 编译安装Nginx./configure --prefix=/usr/local/tengine --add-module=/root/fastdfs-nginx-module/src/ make &amp;&amp; make install 配置fastdfs-nginx-module拷贝配置文件cp /root/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/ 修改配置文件 mod_fastdfs.conftracker_server=192.168.2.109:22122 url_have_group_name = true store_path0=/var/data/fastdfs-storage/store 拷贝http服务需要的配置复制FastDFS安装包中的两个配置文件（http.conf和mime.types）到/etc/fdfs目录中 创建网络访问存储服务的软连接在上传文件到FastDFS后，FastDFS会返回group1/M00/00/00/xxxxxxxxxx.xxx。其中group1是卷名，在mod_fastdfs.conf配置文件中已配置了url_have_group_name，以保证URL解析正确。 而其中的M00是FastDFS保存数据时使用的虚拟目录，需要将这个虚拟目录定位到真实数据目录上。 ln -s /var/data/fastdfs-storage/store/data/ /var/data/fastdfs-storage/store/data/M00 修改nginx配置文件location ~ /group([0-9])/M00 { ngx_fastdfs_module; } http://192.168.150.11/group1/M00/00/00/wKiWC10xxc6AfHCKAAAib-i5DLU543_big.log 文件名add_header Content-Disposition &quot;attachment;filename=$arg_attname&quot;; JavaApihttps://github.com/tobato/FastDFS_Client RAID 配置 fdfs: so-timeout: 1500 connect-timeout: 600 tracker-list: - 192.168.150.13:22122 上传文件// 元数据 Set&lt;MetaData&gt; metaDataSet = new HashSet&lt;MetaData&gt;(); metaDataSet.add(new MetaData(&quot;Author&quot;, &quot;yimingge&quot;)); metaDataSet.add(new MetaData(&quot;CreateDate&quot;, &quot;2016-01-05&quot;)); try { StorePath uploadFile = null; uploadFile = fc.uploadFile(filename.getInputStream(), filename.getSize(), getFileExtName(filename.getOriginalFilename()), metaDataSet); account.setPassword(password); account.setLocation(uploadFile.getPath()); } catch (FileNotFoundException e) { e.printStackTrace(); } 获取文件后缀 private String getFileExtName(String name) { return (name.substring(name.lastIndexOf(&quot;.&quot;)+1)); } 或 FilenameUtils.getExtension 返回结果带group uploadFile.getFullPath() ： group1/M00/00/00/wKiWDV0u7ZKALKtNAAADP9sEx2w432.sql 不带group uploadFile.getPath() ： M00/00/00/wKiWDV0u7ZKALKtNAAADP9sEx2w432.sql 缩略图配置 thumb-image: width: 150 height: 150 uploadFile = fc.uploadImageAndCrtThumbImage(filename.getInputStream(), filename.getSize(), FilenameUtils.getExtension(filename.getOriginalFilename()), metaDataSet); 下载文件@RequestMapping(&quot;/down&quot;) @ResponseBody public ResponseEntity&lt;byte[]&gt; down(HttpServletResponse resp) { DownloadByteArray cb = new DownloadByteArray(); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_OCTET_STREAM); headers.setContentDispositionFormData(&quot;attachment&quot;, &quot;aaa.xx&quot;); byte[] bs = fc.downloadFile(&quot;group1&quot;, &quot;M00/00/00/wKiWDV0vAb-AcOaYABf1Yhcsfws9181.xx&quot;, cb); return new ResponseEntity&lt;&gt;(bs,headers,HttpStatus.OK); }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveJDBC]]></title>
    <url>%2F2019%2F08%2F12%2FHiveJDBC%2F</url>
    <content type="text"><![CDATA[Hive远程数据库模式安装安装hive的步骤:1、解压安装2、修改环境变量vi /etc/profile export HIVE_HOME=/opt/bigdata/hive-2.3.4 将bin目录添加到PATH路径中 3、修改配置文件，进入到/opt/bigdata/hive-2.3.4/conf//修改文件名称，必须修改，文件名称必须是hive-site.xml mv hive-default.xml.template hive-site.xml //增加配置： 进入到文件之后，将文件原有的配置删除，但是保留最后一行， 从&lt;configuration&gt;&lt;/configuration&gt;，将光标移动到&lt;configuration&gt;这一行， 在vi的末行模式中输入以下命令 :.,$-1d //增加如下配置信息： &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; 4、添加MySQL的驱动包拷贝到lib目录5、执行初始化元数据数据库的步骤schematool -dbType mysql -initSchema 6、执行hive启动对应的服务7、执行相应的hive SQL的基本操作hive远程元数据服务模式安装安装步骤:1、选择两台虚拟机，node03作为服务端，node04作为客户端2、分别在Node03和node04上解压hive的安装包，或者在从node02上远程拷贝hive的安装包到Node03和node043、node03修改hive-site.xml配置：&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_remote/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; 4、node04修改hive-site.xml配置：&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_remote/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node03:9083&lt;/value&gt; &lt;/property&gt; 5、在node03服务端执行元数据库的初始化操作，schematool -dbType mysql -initSchema6、在node03上执行hive –service metastore,启动hive的元数据服务，是阻塞式窗口7、在node04上执行hive，进入到hive的cli窗口linux安装mysql安装步骤:1、在linux上使用yum的方式安装软件yum install mysql-server -y 2、启动mysql的服务service mysqld start 3、将mysql服务设置成开机启动（有些同学关闭虚拟机之后，下次重新启动的时候可能忘记开启服务，因此将mysql服务设置完开机启动，不会占用太多的开机时间） chkconfig mysqld on 4、进入到mysql的命令行，敲击以下命令mysql 5、mysql命令行命令(安装完mysql之后需要修改mysql的登录权限)--切换数据库 use mysql --查看表 show tables --查看表的数据 select host,user,password from user; --插入权限数据 grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option --删除本机的用户访问权限（可以执行也可以不执行） delete from user where host!=&#39;%&#39; --刷新权限或者重启mysqld的服务 service mysqld restart;--（重启mysql服务） flush privileges;--(刷新权限) linux安装mysql安装步骤1、在linux上使用yum的方式安装软件yum install mysql-server -y 2、启动mysql的服务service mysqld start 3、将mysql服务设置成开机启动（有些同学关闭虚拟机之后，下次重新启动的时候可能忘记开启服务，因此将mysql服务设置完开机启动，不会占用太多的开机时间）chkconfig mysqld on 4、进入到mysql的命令行，敲击以下命令mysql 5、mysql命令行命令(安装完mysql之后需要修改mysql的登录权限)--切换数据库 use mysql --查看表 show tables --查看表的数据 select host,user,password from user; --插入权限数据 grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option --删除本机的用户访问权限（可以执行也可以不执行） delete from user where host!=&#39;%&#39; --刷新权限或者重启mysqld的服务 service mysqld restart;--（重启mysql服务） flush privileges;--(刷新权限) linux切换yum源切换教程1、需要提前安装wget命令yum install wget -y 2、切换到yum的安装目录/etc/yum.repos.d/ 3、将所有的已经存在的文件添加备份​ 1、给文件该名称添加.bak​ 2、创建backup目录，将所有的文件移动进去 4、打开镜像网站 https://mirrors.aliyun.comwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 5、清除yum的已有缓存yum clean all 6、生成yum的缓存yum makecache 注意：yum缓存这块说的多一点，yum的安装方式其实也是rpm包的安装方式，但是rpm包在安装的时候需要很多rpm包的依赖，因此需要将rpm包的依赖关系下载到本地，防止每次频繁的向镜像网站发送请求，因此在第6步骤的时候需要将rpm的依赖关系下载到本地，放置到缓存中]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive优化]]></title>
    <url>%2F2019%2F08%2F12%2FHive%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Hive优化Hive的存储层依托于HDFS，Hive的计算层依托于MapReduce，一般Hive的执行效率主要取决于SQL语句的执行效率，因此，Hive的优化的核心思想是MapReduce的优化。 1、查看Hive执行计划（小白慎用）Hive的SQL语句在执行之前需要将SQL语句转换成MapReduce任务，因此需要了解具体的转换过程，可以在SQL语句中输入如下命令查看具体的执行计划。 --查看执行计划，添加extended关键字可以查看更加详细的执行计划 explain [extended] query 2、Hive的抓取策略Hive的某些SQL语句需要转换成MapReduce的操作，某些SQL语句就不需要转换成MapReduce操作，但是同学们需要注意，理论上来说，所有的SQL语句都需要转换成MapReduce操作，只不过Hive在转换SQL语句的过程中会做部分优化，使某些简单的操作不再需要转换成MapReduce，例如： （1）select 仅支持本表字段​（2）where仅对本表字段做条件过滤 --查看Hive的数据抓取策略 Set hive.fetch.task.conversion=none/more; 3、Hive本地模式类似于MapReduce的操作，Hive的运行也分为本地模式和集群模式，在开发阶段可以选择使用本地执行，提高SQL语句的执行效率，验证SQL语句是否正确。 --设置本地模式 set hive.exec.mode.local.auto=true; 注意：要想使用Hive的本地模式，加载数据文件大小不能超过128M,如果超过128M,就算设置了本地模式，也会按照集群模式运行。 --设置读取数据量的大小限制 set hive.exec.mode.local.auto.inputbytes.max=128M 4、Hive并行模式在SQL语句足够复杂的情况下，可能在一个SQL语句中包含多个子查询语句，且多个子查询语句之间没有任何依赖关系，此时，可以Hive运行的并行度 --设置Hive SQL的并行度 set hive.exec.parallel=true; 注意：Hive的并行度并不是无限增加的，在一次SQL计算中，可以通过以下参数来设置并行的job的个数 --设置一次SQL计算允许并行执行的job个数的最大值 set hive.exec.parallel.thread.number 5、Hive严格模式Hive中为了提高SQL语句的执行效率，可以设置严格模式，充分利用Hive的某些特点。 -- 设置Hive的严格模式 set hive.mapred.mode=strict; 注意：当设置严格模式之后，会有如下限制：（1）对于分区表，必须添加where对于分区字段的条件过滤（2）order by语句必须包含limit输出限制​（3）限制执行笛卡尔积的查询 6、Hive排序在编写SQL语句的过程中，很多情况下需要对数据进行排序操作，Hive中支持多种排序操作适合不同的应用场景。 1、Order By - 对于查询结果做全排序，只允许有一个reduce处理​（当数据量较大时，应慎用。严格模式下，必须结合limit来使用）​2、Sort By - 对于单个reduce的数据进行排序​3、Distribute By - 分区排序，经常和Sort By结合使用​4、Cluster By - 相当于 Sort By + Distribute By（Cluster By不能通过asc、desc的方式指定排序规则；可通过 distribute by column sort by column asc|desc 的方式） 7、Hive join1、Hive 在多个表的join操作时尽可能多的使用相同的连接键，这样在转换MR任务时会转换成少的MR的任务。 2、手动Map join:在map端完成join操作 --SQL方式，在SQL语句中添加MapJoin标记（mapjoin hint） SELECT /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value FROM smallTable JOIN bigTable ON smallTable.key = bigTable.key; 3、开启自动的Map Join --通过修改以下配置启用自动的mapjoin： set hive.auto.convert.join = true; --（该参数为true时，Hive自动对左边的表统计量，如果是小表就加入内存，即对小表使用Map join） --相关配置参数： hive.mapjoin.smalltable.filesize; --（大表小表判断的阈值，如果表的大小小于该值则会被加载到内存中运行） hive.ignore.mapjoin.hint； --（默认值：true；是否忽略mapjoin hint 即mapjoin标记） 4、大表join大表​（1）空key过滤：有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。​（2）空key转换：有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上 8、Map-Side聚合Hive的某些SQL操作可以实现map端的聚合，类似于MR的combine操作 --通过设置以下参数开启在Map端的聚合： set hive.map.aggr=true; --相关配置参数： --map端group by执行聚合时处理的多少行数据（默认：100000） hive.groupby.mapaggr.checkinterval： --进行聚合的最小比例（预先对100000条数据做聚合，若聚合之后的数据量/100000的值大于该配置0.5，则不会聚合） hive.map.aggr.hash.min.reduction： --map端聚合使用的内存的最大值 hive.map.aggr.hash.percentmemory： --是否对GroupBy产生的数据倾斜做优化，默认为false hive.groupby.skewindata 9、合并小文件Hive在操作的时候，如果文件数目小，容易在文件存储端造成压力，给hdfs造成压力，影响效率 --设置合并属性 --是否合并map输出文件： set hive.merge.mapfiles=true --是否合并reduce输出文件： set hive.merge.mapredfiles=true; --合并文件的大小： set hive.merge.size.per.task=256*1000*1000 10、合理设置Map以及Reduce的数量--Map数量相关的参数 --一个split的最大值，即每个map处理文件的最大值 set mapred.max.split.size --一个节点上split的最小值 set mapred.min.split.size.per.node --一个机架上split的最小值 set mapred.min.split.size.per.rack --Reduce数量相关的参数 --强制指定reduce任务的数量 set mapred.reduce.tasks --每个reduce任务处理的数据量 set hive.exec.reducers.bytes.per.reducer --每个任务最大的reduce数 set hive.exec.reducers.max 11、JVM重用/* 适用场景： 1、小文件个数过多 2、task个数过多 缺点： 设置开启之后，task插槽会一直占用资源，不论是否有task运行，直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！ */ set mapred.job.reuse.jvm.num.tasks=n;--（n为task插槽个数） 压缩和存储1、 Hadoop压缩配置1) MR支持的压缩编码 压缩格式 工具 算法 文件扩展名 是否可切分 DEFAULT 无 DEFAULT .deflate 否 Gzip gzip DEFAULT .gz 否 bzip2 bzip2 bzip2 .bz2 是 LZO lzop LZO .lzo 否 LZ4 无 LZ4 .lz4 否 Snappy 无 Snappy .snappy 否 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec LZ4 org.apache.hadoop.io.compress.Lz4Codec Snappy org.apache.hadoop.io.compress.SnappyCodec 2) 压缩配置参数要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）： 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 3) 开启Map输出阶段压缩开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：案例实操： --1）开启hive中间传输数据压缩功能 hive (default)&gt;set hive.exec.compress.intermediate=true; --2）开启mapreduce中map输出压缩功能 hive (default)&gt;set mapreduce.map.output.compress=true; --3）设置mapreduce中map输出数据的压缩方式 hive (default)&gt;set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec; --4）执行查询语句 hive (default)&gt; select count(*) from aaaa; 4) 开启Reduce输出阶段压缩当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。 案例实操： --1）开启hive最终输出数据压缩功能 hive (default)&gt;set hive.exec.compress.output=true; --2）开启mapreduce最终输出数据压缩 hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true; --3）设置mapreduce最终数据输出压缩方式 hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; --4）设置mapreduce最终数据输出压缩为块压缩 hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK; --5）测试一下输出结果是否是压缩文件 hive (default)&gt; insert overwrite local directory &#39;/root/data&#39; select * from aaaa; 2、文件存储格式Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。 1) 列式存储和行式存储 上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。 行存储的特点： 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列存储的特点： 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的； ORC和PARQUET是基于列式存储的。 2) TEXTFILE格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。 3) ORC格式Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。 可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer： 1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。3）Stripe Footer：存的是各个Stream的类型，长度等信息。 每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。 4) PARQUET格式Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。 Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。 通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。 上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。 5) 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比。​存储文件的压缩比测试： --1）TextFile --（1）创建表，存储数据格式为TEXTFILE create table log_text (track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by &#39;\t&#39;stored as textfile ; --（2）向表中加载数据 hive (default)&gt; load data local inpath &#39;/root/log&#39; into table log_text ; --（3）查看表中数据大小 dfs -du -h /user/hive/warehouse/log_text; 18.1 M /user/hive/warehouse/log_text/log.data --2）ORC --（1）创建表，存储数据格式为ORC create table log_orc(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by &#39;\t&#39;stored as orc ; --（2）向表中加载数据 insert into table log_orc select * from log_text ; --（3）查看表中数据大小 dfs -du -h /user/hive/warehouse/log_orc/ ; 2.8 M /user/hive/warehouse/log_orc/000000_0 --3）Parquet --（1）创建表，存储数据格式为parquet create table log_parquet(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by &#39;\t&#39;stored as parquet ; --（2）向表中加载数据 insert into table log_parquet select * from log_text ; --（3）查看表中数据大小 dfs -du -h /user/hive/warehouse/log_parquet/ ; 13.1 M /user/hive/warehouse/log_parquet/000000_0 --存储文件的压缩比总结： ORC &gt; Parquet &gt; textFile 3、存储和压缩结合官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC​ORC存储方式的压缩： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0) --1）创建一个非压缩的的ORC存储方式 --（1）建表语句 create table log_orc_none(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by &#39;\t&#39;stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;); --（2）插入数据 insert into table log_orc_none select * from log_text ; --（3）查看插入后数据 dfs -du -h /user/hive/warehouse/log_orc_none/ ; 7.7 M /user/hive/warehouse/log_orc_none/000000_0 --2）创建一个SNAPPY压缩的ORC存储方式 --（1）建表语句 create table log_orc_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by &#39;\t&#39;stored as orc tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); --（2）插入数据 insert into table log_orc_snappy select * from log_text ; --（3）查看插入后数据 dfs -du -h /user/hive/warehouse/log_orc_snappy/ ; 3.8 M /user/hive/warehouse/log_orc_snappy/000000_0 --3）上一节中默认创建的ORC存储方式，导入数据后的大小为 2.8 M /user/hive/warehouse/log_orc/000000_0 --总结 比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。 hive—high Avaliable hive的搭建方式有三种，分别是 1、Local/Embedded Metastore Database (Derby)​2、Remote Metastore Database​3、Remote Metastore Server 一般情况下，我们在学习的时候直接使用hive –service metastore的方式启动服务端，使用hive的方式直接访问登录客户端，除了这种方式之外，hive提供了hiveserver2的服务端启动方式，提供了beeline和jdbc的支持，并且官网也提出，一般在生产环境中，使用hiveserver2的方式比较多，如图： 使用hiveserver2的优点如下： 1、在应用端不需要部署hadoop和hive的客户端​2、hiveserver2不用直接将hdfs和metastore暴露给用户​3、有HA机制，解决应用端的并发和负载问题​4、jdbc的连接方式，可以使用任何语言，方便与应用进行数据交互本文档主要介绍如何进行hive的HA的搭建：如何进行搭建，参照之前hadoop的HA，使用zookeeper完成HA 1、环境如下: Node01 Node02 Node03 Node04 Namenode 1 1 Journalnode 1 1 1 Datanode 1 1 1 Zkfc 1 1 zookeeper 1 1 1 resourcemanager 1 1 1 nodemanager 1 1 1 Hiveserver2 1 beeline 1 2、node02—hive-site.xml &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt; &lt;value&gt;hiveserver2_zk&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node02&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10001&lt;/value&gt; &lt;/property&gt; 3、node4—hive-site.xml &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt; &lt;value&gt;hiveserver2_zk&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node04&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10001&lt;/value&gt; &lt;/property&gt; 4、使用jdbc或者beeline两种方式进行访问 1） beeline !connect jdbc:hive2://node01,node02,node03/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2_zk root 123 2）jdbc public class HiveJdbcClient2 { private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws SQLException { try { Class.forName(driverName); } catch (ClassNotFoundException e) { e.printStackTrace(); } Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://node01,node02,node03/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2_zk&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = conn.createStatement(); String sql = &quot;select * from tbl&quot;; ResultSet res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1)); } } }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive函数]]></title>
    <url>%2F2019%2F08%2F12%2FHive%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Hive函数Hive中提供了非常丰富的运算符和内置函数支撑，具体操作如下： 1.内置运算符1.1关系运算符 运算符 类型 说明 A = B 所有原始类型 如果A与B相等,返回TRUE,否则返回FALSE A == B 无 失败，因为无效的语法。 SQL使用”=”，不使用”==”。 A &lt;&gt; B 所有原始类型 如果A不等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 A &lt; B 所有原始类型 如果A小于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 A &lt;= B 所有原始类型 如果A小于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 A &gt; B 所有原始类型 如果A大于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 A &gt;= B 所有原始类型 如果A大于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 A IS NULL 所有类型 如果A值为”NULL”，返回TRUE,否则返回FALSE A IS NOT NULL 所有类型 如果A值不为”NULL”，返回TRUE,否则返回FALSE A LIKE B 字符串 如 果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过sql进行匹配，如果相符返回TRUE，不符返回FALSE。B字符串中 的””代表任一字符，”%”则代表多个任意字符。例如： (‘foobar’ like ‘foo’)返回FALSE，（ ‘foobar’ like ‘foo _ _’或者 ‘foobar’ like ‘foo%’)则返回TURE A RLIKE B 字符串 如 果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过java进行匹配，如果相符返回TRUE，不符返回FALSE。例如：（ ‘foobar’ rlike ‘foo’）返回FALSE，（’foobar’ rlike ‘^f.*r$’ ）返回TRUE。 A REGEXP B 字符串 与RLIKE相同。 1.2算术运算符 运算符 类型 说明 A + B 所有数字类型 A和B相加。结果的与操作数值有共同类型。例如每一个整数是一个浮点数，浮点数包含整数。所以，一个浮点数和一个整数相加结果也是一个浮点数。 A – B 所有数字类型 A和B相减。结果的与操作数值有共同类型。 A * B 所有数字类型 A和B相乘，结果的与操作数值有共同类型。需要说明的是，如果乘法造成溢出，将选择更高的类型。 A / B 所有数字类型 A和B相除，结果是一个double（双精度）类型的结果。 A % B 所有数字类型 A除以B余数与操作数值有共同类型。 A &amp; B 所有数字类型 运算符查看两个参数的二进制表示法的值，并执行按位”与”操作。两个表达式的一位均为1时，则结果的该位为 1。否则，结果的该位为 0。 A\ B 所有数字类型 运算符查看两个参数的二进制表示法的值，并执行按位”或”操作。只要任一表达式的一位为 1，则结果的该位为 1。否则，结果的该位为 0。 A ^ B 所有数字类型 运算符查看两个参数的二进制表示法的值，并执行按位”异或”操作。当且仅当只有一个表达式的某位上为 1 时，结果的该位才为 1。否则结果的该位为 0。 ~A 所有数字类型 对一个表达式执行按位”非”（取反）。 1.3逻辑运算符 运算符 类型 说明 A AND B 布尔值 A和B同时正确时,返回TRUE,否则FALSE。如果A或B值为NULL，返回NULL。 A &amp;&amp; B 布尔值 与”A AND B”相同 A OR B 布尔值 A或B正确,或两者同时正确返返回TRUE,否则FALSE。如果A和B值同时为NULL，返回NULL。 A \ B 布尔值 与”A OR B”相同 NOT A 布尔值 如果A为NULL或错误的时候返回TURE，否则返回FALSE。 ! A 布尔值 与”NOT A”相同 1.4复杂类型函数 函数 类型 说明 map (key1, value1, key2, value2, …) 通过指定的键/值对，创建一个map。 struct (val1, val2, val3, …) 通过指定的字段值，创建一个结构。结构字段名称将COL1，COL2，… array (val1, val2, …) 通过指定的元素，创建一个数组。 1.5对复杂类型函数操作 函数 类型 说明 A[n] A是一个数组，n为int型 返回数组A的第n个元素，第一个元素的索引为0。如果A数组为[‘foo’,’bar’]，则A[0]返回’foo’和A[1]返回”bar”。 M[key] M是Map&lt;K, V&gt;，关键K型 返回关键值对应的值，例如mapM为 {‘f’ -&gt; ‘foo’, ‘b’ -&gt; ‘bar’, ‘all’ -&gt; ‘foobar’}，则M[‘all’] 返回’foobar’。 S.x S为struct 返回结构x字符串在结构S中的存储位置。如 foobar {int foo, int bar} foobar.foo的领域中存储的整数。 2.内置函数2.1数学函数 返回类型 函数 说明 BIGINT round(double a) 四舍五入 DOUBLE round(double a, int d) 小数部分d位之后数字四舍五入，例如round(21.263,2),返回21.26 BIGINT floor(double a) 对给定数据进行向下舍入最接近的整数。例如floor(21.2),返回21。 BIGINT ceil(double a), ceiling(double a) 将参数向上舍入为最接近的整数。例如ceil(21.2),返回23. double rand(), rand(int seed) 返回大于或等于0且小于1的平均分布随机数（依重新计算而变） double exp(double a) 返回e的n次方 double ln(double a) 返回给定数值的自然对数 double log10(double a) 返回给定数值的以10为底自然对数 double log2(double a) 返回给定数值的以2为底自然对数 double log(double base, double a) 返回给定底数及指数返回自然对数 double pow(double a, double p) power(double a, double p) 返回某数的乘幂 double sqrt(double a) 返回数值的平方根 string bin(BIGINT a) 返回二进制格式 string hex(BIGINT a) hex(string a) 将整数或字符转换为十六进制格式 string unhex(string a) 十六进制字符转换由数字表示的字符。 string conv(BIGINT num, int from_base, int to_base) 将 指定数值，由原来的度量体系转换为指定的试题体系。例如CONV(‘a’,16,2),返回。参考：’1010′ http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv double abs(double a) 取绝对值 int double pmod(int a, int b) pmod(double a, double b) 返回a除b的余数的绝对值 double sin(double a) 返回给定角度的正弦值 double asin(double a) 返回x的反正弦，即是X。如果X是在-1到1的正弦值，返回NULL。 double cos(double a) 返回余弦 double acos(double a) 返回X的反余弦，即余弦是X，，如果-1&lt;= A &lt;= 1，否则返回null. int double positive(int a) positive(double a) 返回A的值，例如positive(2)，返回2。 int double negative(int a) negative(double a) 返回A的相反数，例如negative(2),返回-2。 2.2收集函数 返回类型 函数 说明 int size(Map&lt;K.V&gt;) 返回的map类型的元素的数量 int size(Array) 返回数组类型的元素数量 2.3类型转换函数 返回类型 函数 说明 指定 “type” cast(expr as ) 类型转换。例如将字符”1″转换为整数:cast(’1′ as bigint)，如果转换失败返回NULL。 2.4日期函数 返回类型 函数 说明 string from_unixtime(bigint unixtime[, string format]) UNIX_TIMESTAMP参数表示返回一个值’YYYY- MM – DD HH：MM：SS’或YYYYMMDDHHMMSS.uuuuuu格式，这取决于是否是在一个字符串或数字语境中使用的功能。该值表示在当前的时区。 bigint unix_timestamp() 如果不带参数的调用，返回一个Unix时间戳（从’1970- 01 – 0100:00:00′到现在的UTC秒数）为无符号整数。 bigint unix_timestamp(string date) 指定日期参数调用UNIX_TIMESTAMP（），它返回参数值’1970- 01 – 0100:00:00′到指定日期的秒数。 bigint unix_timestamp(string date, string pattern) 指定时间输入格式，返回到1970年秒数：unix_timestamp(’2009-03-20′, ‘yyyy-MM-dd’) = 1237532400 string to_date(string timestamp) 返回时间中的年月日： to_date(“1970-01-01 00:00:00″) = “1970-01-01″ string to_dates(string date) 给定一个日期date，返回一个天数（0年以来的天数） int year(string date) 返回指定时间的年份，范围在1000到9999，或为”零”日期的0。 int month(string date) 返回指定时间的月份，范围为1至12月，或0一个月的一部分，如’0000-00-00′或’2008-00-00′的日期。 int day(string date) dayofmonth(date) 返回指定时间的日期 int hour(string date) 返回指定时间的小时，范围为0到23。 int minute(string date) 返回指定时间的分钟，范围为0到59。 int second(string date) 返回指定时间的秒，范围为0到59。 int weekofyear(string date) 返回指定日期所在一年中的星期号，范围为0到53。 int datediff(string enddate, string startdate) 两个时间参数的日期之差。 int date_add(string startdate, int days) 给定时间，在此基础上加上指定的时间段。 int date_sub(string startdate, int days) 给定时间，在此基础上减去指定的时间段。 2.5条件函数 返回类型 函数 说明 T if(boolean testCondition, T valueTrue, T valueFalseOrNull) 判断是否满足条件，如果满足返回一个值，如果不满足则返回另一个值。 T COALESCE(T v1, T v2, …) 返回一组数据中，第一个不为NULL的值，如果均为NULL,返回NULL。 T CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END 当a=b时,返回c；当a=d时，返回e，否则返回f。 T CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END 当值为a时返回b,当值为c时返回d。否则返回e。 2.6字符函数 返回类型 函数 说明 int length(string A) 返回字符串的长度 string reverse(string A) 返回倒序字符串 string concat(string A, string B…) 连接多个字符串，合并为一个字符串，可以接受任意数量的输入字符串 string concat_ws(string SEP, string A, string B…) 链接多个字符串，字符串之间以指定的分隔符分开。 string substr(string A, int start) substring(string A, int start) 从文本字符串中指定的起始位置后的字符。 string substr(string A, int start, int len) substring(string A, int start, int len) 从文本字符串中指定的位置指定长度的字符。 string upper(string A) ucase(string A) 将文本字符串转换成字母全部大写形式 string lower(string A) lcase(string A) 将文本字符串转换成字母全部小写形式 string trim(string A) 删除字符串两端的空格，字符之间的空格保留 string ltrim(string A) 删除字符串左边的空格，其他的空格保留 string rtrim(string A) 删除字符串右边的空格，其他的空格保留 string regexp_replace(string A, string B, string C) 字符串A中的B字符被C字符替代 string regexp_extract(string subject, string pattern, int index) 通过下标返回正则表达式指定的部分。regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) returns ‘bar.’ string parse_url(string urlString, string partToExtract [, string keyToExtract]) 返回URL指定的部分。parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1′, ‘HOST’) 返回：’facebook.com’ string get_json_object(string json_string, string path) select a.timestamp, get_json_object(a.appevents, ‘$.eventid’), get_json_object(a.appenvets, ‘$.eventname’) from log a; string space(int n) 返回指定数量的空格 string repeat(string str, int n) 重复N次字符串 int ascii(string str) 返回字符串中首字符的数字值 string lpad(string str, int len, string pad) 返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从左侧填补。 string rpad(string str, int len, string pad) 返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从右侧填补。 array split(string str, string pat) 将字符串转换为数组。 int find_in_set(string str, string strList) 返回字符串str第一次在strlist出现的位置。如果任一参数为NULL,返回NULL；如果第一个参数包含逗号，返回0。 array&lt;array&gt; sentences(string str, string lang, string locale) 将字符串中内容按语句分组，每个单词间以逗号分隔，最后返回数组。 例如sentences(‘Hello there! How are you?’) 返回：( (“Hello”, “there”), (“How”, “are”, “you”) ) array&lt;struct&lt;string,double&gt;&gt; ngrams(array&lt;array&gt;, int N, int K, int pf) SELECT ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter; array&lt;struct&lt;string,double&gt;&gt; context_ngrams(array&lt;array&gt;, array, int K, int pf) SELECT context_ngrams(sentences(lower(tweet)), array(null,null), 100, [, 1000]) FROM twitter; 3.内置的聚合函数（UDAF） 返回类型 函数 说明 bigint count(*) , count(expr), count(DISTINCT expr[, expr_., expr_.]) 返回记录条数。 double sum(col), sum(DISTINCT col) 求和 double avg(col), avg(DISTINCT col) 求平均值 double min(col) 返回指定列中最小值 double max(col) 返回指定列中最大值 double var_pop(col) 返回指定列的方差 double var_samp(col) 返回指定列的样本方差 double stddev_pop(col) 返回指定列的偏差 double stddev_samp(col) 返回指定列的样本偏差 double covar_pop(col1, col2) 两列数值协方差 double covar_samp(col1, col2) 两列数值样本协方差 double corr(col1, col2) 返回两列数值的相关系数 double percentile(col, p) 返回数值区域的百分比数值点。0&lt;=P&lt;=1,否则返回NULL,不支持浮点型数值。 array percentile(col, array(p~1,,\ [, p,,2,,]…)) 返回数值区域的一组百分比值分别对应的数值点。0&lt;=P&lt;=1,否则返回NULL,不支持浮点型数值。 double percentile_approx(col, p[, B]) Returns an approximate p^th^ percentile of a numeric column (including floating point types) in the group. The B parameter controls approximation accuracy at the cost of memory. Higher values yield better approximations, and the default is 10,000. When the number of distinct values in col is smaller than B, this gives an exact percentile value. array percentile_approx(col, array(p~1,, [, p,,2_]…) [, B]) Same as above, but accepts and returns an array of percentile values instead of a single one. array&lt;struct{‘x’,’y’}&gt; histogram_numeric(col, b) Computes a histogram of a numeric column in the group using b non-uniformly spaced bins. The output is an array of size b of double-valued (x,y) coordinates that represent the bin centers and heights array collect_set(col) 返回无重复记录 4.内置表生成函数（UDTF） 返回类型 函数 说明 数组 explode(array a) 数组一条记录中有多个参数，将参数拆分，每个参数生成一列。 json_tuple get_json_object 语句：select a.timestamp, get_json_object(a.appevents, ‘$.eventid’), get_json_object(a.appenvets, ‘$.eventname’) from log a; json_tuple语句: select a.timestamp, b.* from log a lateral view json_tuple(a.appevent, ‘eventid’, ‘eventname’) b as f1, f2 5.自定义函数自定义函数包括三种UDF、UDAF、UDTFUDF(User-Defined-Function) ：一进一出UDAF(User- Defined Aggregation Funcation) ：聚集函数，多进一出。Count/max/minUDTF(User-Defined Table-Generating Functions) :一进多出，如explore() 5.1 UDF 开发1、UDF函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容。2、编写UDF函数的时候需要注意一下几点：a）自定义UDF需要继承org.apache.hadoop.hive.ql.UDF。b）需要实现evaluate函数，evaluate函数支持重载。3、步骤(1)将jar包上传到虚拟机中：a）把程序打包放到目标机器上去；b）进入hive客户端，添加jar包：hive&gt;add jar /run/jar/udf_test.jar;c）创建临时函数：hive&gt;CREATE TEMPORARY FUNCTION add_example AS ‘hive.udf.Add’;d）查询HQL语句： SELECT add_example(8, 9) FROM scores; ​SELECT add_example(scores.math, scores.art) FROM scores; ​SELECT add_example(6, 7, 8, 6.8) FROM scores; e）销毁临时函数：hive&gt; DROP TEMPORARY FUNCTION add_example;​注意：此种方式创建的函数属于临时函数，当关闭了当前会话之后，函数会无法使用，因为jar的引用没有了，无法找到对应的java文件进行处理，因此不推荐使用。(2)将jar包上传到hdfs集群中： a）把程序打包上传到hdfs的某个目录下​b）创建函数：hive&gt;CREATE FUNCTION add_example AS ‘hive.udf.Add’ using jar “hdfs://mycluster/jar/udf_test.jar”;d）查询HQL语句： SELECT add_example(8, 9) FROM scores; ​SELECT add_example(scores.math, scores.art) FROM scores; ​SELECT add_example(6, 7, 8, 6.8) FROM scores; e）销毁临时函数：hive&gt; DROP FUNCTION add_example; Hive参数操作和运行方式1、Hive参数操作1、hive参数介绍hive当中的参数、变量都是以命名空间开头的，详情如下表所示： 命名空间 读写权限 含义 hiveconf 可读写 hive-site.xml当中的各配置变量例：hive –hiveconf hive.cli.print.header=true system 可读写 系统变量，包含JVM运行参数等例：system:user.name=root env 只读 环境变量例：env：JAVA_HOME hivevar 可读写 例：hive -d val=key hive的变量可以通过${}方式进行引用，其中system、env下的变量必须以前缀开头 2、hive参数的设置方式1、在${HIVE_HOME}/conf/hive-site.xml文件中添加参数设置​注意：永久生效，所有的hive会话都会加载对应的配置​2、在启动hive cli时，通过–hiveconf key=value的方式进行设置​例如：hive –hiveconf hive.cli.print.header=true​注意：只在当前会话有效，退出会话之后参数失效​3、在进入到cli之后，通过set命令设置​例如：set hive.cli.print.header=true; --在hive cli控制台可以通过set对hive中的参数进行查询设置 --set设置 set hive.cli.print.header=true; --set查看 set hive.cli.print.header --set查看全部属性 set 4、hive参数初始化设置​在当前用户的家目录下创建.hiverc文件，在当前文件中设置hive参数的命令，每次进入hive cli的时候，都会加载.hiverc的文件，执行文件中的命令。​注意：在当前用户的家目录下还会存在.hivehistory文件，此文件中保存了hive cli中执行的所有命令 2、hive运行方式1、hive运行方式分类（1）命令行方式或者控制台模式​（2）脚本运行方式（实际生产环境中用最多）（3）JDBC方式：hiveserver2​（4）web GUI接口（hwi、hue等） 2、hive命令行模式详解（1）在命令行中可以直接输入SQL语句，例如：select * from table_name​（2）在命令行中可以与HDFS交互，例如：dfs ls /​（3）在命令行中可以与linux交互，例如：! pwd或者! ls /​注意：与linux交互的时候必须要加! 3、hive脚本运行方式--hive直接执行sql命令，可以写一个sql语句，也可以使用;分割写多个sql语句 hive -e &quot;&quot; --hive执行sql命令，将sql语句执行的结果重定向到某一个文件中 hive -e &quot;&quot;&gt;aaa --hive静默输出模式，输出的结果中不包含ok，time token等关键字 hive -S -e &quot;&quot;&gt;aaa --hive可以直接读取文件中的sql命令，进行执行 hive -f file --hive可以从文件中读取命令，并且执行初始化操作 hive -i /home/my/hive-init.sql --在hive的命令行中也可以执行外部文件中的命令 hive&gt; source file (在hive cli中运行) 4、hive JDBC访问方式，之前讲过，不再赘述5、Hive GUI方式​ Hive动态分区和分桶1、Hive动态分区1、hive的动态分区介绍hive的静态分区需要用户在插入数据的时候必须手动指定hive的分区字段值，但是这样的话会导致用户的操作复杂度提高，而且在使用的时候会导致数据只能插入到某一个指定分区，无法让数据散列分布，因此更好的方式是当数据在进行插入的时候，根据数据的某一个字段或某几个字段值动态的将数据插入到不同的目录中，此时，引入动态分区。 2、hive的动态分区配置--hive设置hive动态分区开启 set hive.exec.dynamic.partition=true; 默认：true --hive的动态分区模式 set hive.exec.dynamic.partition.mode=nostrict; 默认：strict（至少有一个分区列是静态分区） --每一个执行mr节点上，允许创建的动态分区的最大数量(100) set hive.exec.max.dynamic.partitions.pernode; --所有执行mr节点上，允许创建的所有动态分区的最大数量(1000) set hive.exec.max.dynamic.partitions; --所有的mr job允许创建的文件的最大数量(100000) set hive.exec.max.created.files; 3、hive动态分区语法--Hive extension (dynamic partition inserts): INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; 2、Hive分桶1、Hive分桶的介绍Bucketed tables are fantastic in that they allow much more efficient sampling than do non-bucketed tables, and they may later allow for time saving operations such as mapside joins. However, the bucketing specified at table creation is not enforced when the table is written to, and so it is possible for the table&#39;s metadata to advertise properties which are not upheld by the table&#39;s actual layout. This should obviously be avoided. Here is how to do it right. 注意：​ 1、Hive分桶表是对列值取hash值得方式，将不同数据放到不同文件中存储​ 2、对于hive中每一个表、分区都可以进一步进行分桶​ 3、由列的hash值除以桶的个数来决定每条数据划分在哪个桶中 2、Hive分桶的配置--设置hive支持分桶 set hive.enforce.bucketing=true; 3、Hive分桶的抽样查询--案例 select * from bucket_table tablesample(bucket 1 out of 4 on columns) --TABLESAMPLE语法： TABLESAMPLE(BUCKET x OUT OF y) x：表示从哪个bucket开始抽取数据 y：必须为该表总bucket数的倍数或因子 Hive的视图和索引1、Hive Lateral View1、基本介绍Lateral View用于和UDTF函数（explode、split）结合来使用。​首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。主要解决在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题。​语法：​ LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias) 2、案例select count(distinct(myCol1)), count(distinct(myCol2)) from psn2 LATERAL VIEW explode(likes) myTable1 AS myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; 2、Hive视图1、Hive视图基本介绍Hive 中的视图和RDBMS中视图的概念一致，都是一组数据的逻辑表示，本质上就是一条SELECT语句的结果集。视图是纯粹的逻辑对象，没有关联的存储(Hive 3.0.0引入的物化视图除外)，当查询引用视图时，Hive可以将视图的定义与查询结合起来，例如将查询中的过滤器推送到视图中。 2、Hive视图特点1、不支持物化视图​2、只能查询，不能做加载数据操作​3、视图的创建，只是保存一份元数据，查询视图时才执行对应的子查询​4、view定义中若包含了ORDER BY/LIMIT语句，当查询视图时也进行ORDER BY/LIMIT语句操作，view当中 定义的优先级更高5、view支持迭代视图 3、Hive视图语法--创建视图： CREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], ...) ] [COMMENT view_comment] [TBLPROPERTIES (property_name = property_value, ...)] AS SELECT ... ; --查询视图： select colums from view; --删除视图： DROP VIEW [IF EXISTS] [db_name.]view_name; 3、Hive索引1、hive索引为了提高数据的检索效率，可以使用hive的索引 2、hive基本操作--创建索引： create index t1_index on table psn2(name) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table; --as：指定索引器； --in table：指定索引表，若不指定默认生成在default__psn2_t1_index__表中 create index t1_index on table psn2(name) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild; --查询索引 show index on psn2; --重建索引（建立索引之后必须重建索引才能生效） ALTER INDEX t1_index ON psn2 REBUILD; --删除索引 DROP INDEX IF EXISTS t1_index ON psn2; Hive权限管理1、hive授权模型介绍 （1）Storage Based Authorization in the Metastore Server 基于存储的授权 - 可以对Metastore中的元数据进行保护，但是没有提供更加细粒度的访问控制（例如：列级别、行级别）。（2）SQL Standards Based Authorization in HiveServer2 基于SQL标准的Hive授权 - 完全兼容SQL的授权模型，推荐使用该模式。（3）Default Hive Authorization (Legacy Mode) hive默认授权 - 设计目的仅仅只是为了防止用户产生误操作，而不是防止恶意用户访问未经授权的数据。 2、基于SQL标准的hiveserver2授权模式（1）完全兼容SQL的授权模型（2）除支持对于用户的授权认证，还支持角色role的授权认证 1、role可理解为是一组权限的集合，通过role为用户授权 2、一个用户可以具有一个或多个角色，默认包含另种角色：public、admin 3、基于SQL标准的hiveserver2授权模式的限制1、启用当前认证方式之后，dfs, add, delete, compile, and reset等命令被禁用。2、通过set命令设置hive configuration的方式被限制某些用户使用。（可通过修改配置文件hive-site.xml中hive.security.authorization.sqlstd.confwhitelist进行配置）3、添加、删除函数以及宏的操作，仅为具有admin的用户开放。​4、用户自定义函数（开放支持永久的自定义函数），可通过具有admin角色的用户创建，其他用户都可以使用。5、Transform功能被禁用。 4、详细配置&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.users.in.admin.role&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator&lt;/value&gt; &lt;/property&gt; 5、Hive权限管理命令--角色的添加、删除、查看、设置： -- 创建角色 CREATE ROLE role_name; -- 删除角色 DROP ROLE role_name; -- 设置角色 SET ROLE (role_name|ALL|NONE); -- 查看当前具有的角色 SHOW CURRENT ROLES; -- 查看所有存在的角色 SHOW ROLES; 6、Hive权限分配图 Action Select Insert Update Delete Owership Admin URL Privilege(RWX Permission + Ownership) ALTER DATABASE Y ALTER INDEX PROPERTIES Y ALTER INDEX REBUILD Y ALTER PARTITION LOCATION Y Y (for new partition location) ALTER TABLE (all of them except the ones above) Y ALTER TABLE ADD PARTITION Y Y (for partition location) ALTER TABLE DROP PARTITION Y ALTER TABLE LOCATION Y Y (for new location) ALTER VIEW PROPERTIES Y ALTER VIEW RENAME Y ANALYZE TABLE Y Y CREATE DATABASE Y (if custom location specified) CREATE FUNCTION Y CREATE INDEX Y (of table) CREATE MACRO Y CREATE TABLE Y (of database) Y (for create external table – the location) CREATE TABLE AS SELECT Y (of input) Y (of database) CREATE VIEW Y + G DELETE Y DESCRIBE TABLE Y DROP DATABASE Y DROP FUNCTION Y DROP INDEX Y DROP MACRO Y DROP TABLE Y DROP VIEW Y DROP VIEW PROPERTIES Y EXPLAIN Y INSERT Y Y (for OVERWRITE) LOAD Y (output) Y (output) Y (input location) MSCK (metastore check) Y SELECT Y SHOW COLUMNS Y SHOW CREATE TABLE Y+G SHOW PARTITIONS Y SHOW TABLE PROPERTIES Y SHOW TABLE STATUS Y TRUNCATE TABLE Y UPDATE Y]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveSQL操作]]></title>
    <url>%2F2019%2F08%2F12%2FHiveSQL%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Hive基本SQL操作Hive DDL（数据库定义语言）1、数据库的基本操作--展示所有数据库 show databases; --切换数据库 use database_name; /* 创建数据库 CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; */ create database test; /* 删除数据库 DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; */ drop database database_name; 注意：当进入hive的命令行开始编写SQL语句的时候，如果没有任何相关的数据库操作，那么默认情况下，所有的表存在于default数据库，在hdfs上的展示形式是将此数据库的表保存在hive的默认路径下，如果创建了数据库，那么会在hive的默认路径下生成一个database_name.db的文件夹，此数据库的所有表会保存在database_name.db的目录下。 2、数据库表的基本操作/* 创建表的操作 基本语法： CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 复杂数据类型 data_type : primitive_type | array_type | map_type | struct_type | union_type -- (Note: Available in Hive 0.7.0 and later) 基本数据类型 primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later) array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt; union_type : UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later) 行格式规范 row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 文件基本类型 file_format: : SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | JSONFILE -- (Note: Available in Hive 4.0.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname 表约束 constraint_specification: : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE */ --创建普通hive表（不包含行定义格式） create table psn ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) --创建自定义行格式的hive表 create table psn2 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &#39;,&#39; collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39;; --创建默认分隔符的hive表（^A、^B、^C） create table psn3 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &#39;\001&#39; collection items terminated by &#39;\002&#39; map keys terminated by &#39;\003&#39;; --或者 create table psn3 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) --创建hive的外部表(需要添加external和location的关键字) create external table psn4 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &#39;,&#39; collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39; location &#39;/data&#39;; /* 在之前创建的表都属于hive的内部表（psn,psn2,psn3）,而psn4属于hive的外部表， 内部表跟外部表的区别： 1、hive内部表创建的时候数据存储在hive的默认存储目录中，外部表在创建的时候需要制定额外的目录 2、hive内部表删除的时候，会将元数据和数据都删除，而外部表只会删除元数据，不会删除数据 应用场景: 内部表:需要先创建表，然后向表中添加数据，适合做中间表的存储 外部表：可以先创建表，再添加数据，也可以先有数据，再创建表，本质上是将hdfs的某一个目录的数据跟hive的表关联映射起来，因此适合原始数据的存储，不会因为误操作将数据给删除掉 */ /* hive的分区表: hive默认将表的数据保存在某一个hdfs的存储目录下，当需要检索符合条件的某一部分数据的时候，需要全量遍历数据，io量比较大，效率比较低，因此可以采用分而治之的思想，将符合某些条件的数据放置在某一个目录，此时检索的时候只需要搜索指定目录即可，不需要全量遍历数据。 */ --创建单分区表 create table psn5 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) partitioned by(gender string) row format delimited fields terminated by &#39;,&#39; collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39;; --创建多分区表 create table psn6 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) partitioned by(gender string,age int) row format delimited fields terminated by &#39;,&#39; collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39;; /* 注意： 1、当创建完分区表之后，在保存数据的时候，会在hdfs目录中看到分区列会成为一个目录，以多级目录的形式存在 2、当创建多分区表之后，插入数据的时候不可以只添加一个分区列，需要将所有的分区列都添加值 3、多分区表在添加分区列的值得时候，与顺序无关，与分区表的分区列的名称相关，按照名称就行匹配 */ --给分区表添加分区列的值 alter table table_name add partition(col_name=col_value) --删除分区列的值 alter table table_name drop partition(col_name=col_value) /* 注意: 1、添加分区列的值的时候，如果定义的是多分区表，那么必须给所有的分区列都赋值 2、删除分区列的值的时候，无论是单分区表还是多分区表，都可以将指定的分区进行删除 */ /* 修复分区:在使用hive外部表的时候，可以先将数据上传到hdfs的某一个目录中，然后再创建外部表建立映射关系，如果在上传数据的时候，参考分区表的形式也创建了多级目录，那么此时创建完表之后，是查询不到数据的，原因是分区的元数据没有保存在mysql中，因此需要修复分区，将元数据同步更新到mysql中，此时才可以查询到元数据。具体操作如下： */ --在hdfs创建目录并上传文件 hdfs dfs -mkdir /msb hdfs dfs -mkdir /msb/age=10 hdfs dfs -mkdir /msb/age=20 hdfs dfs -put /root/data/data /msb/age=10 hdfs dfs -put /root/data/data /msb/age=20 --创建外部表 create external table psn7 ( id int, name string, likes array&lt;string&gt;, address map&lt;string,string&gt; ) partitioned by(age int) row format delimited fields terminated by &#39;,&#39; collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39; location &#39;/msb&#39;; --查询结果（没有数据） select * from psn7; --修复分区 msck repair table psn7; --查询结果（有数据） select * from psn7; /* 问题：以上面的方式创建hive的分区表会存在问题，每次插入的数据都是人为指定分区列的值，我们更加希望能够根据记录中的某一个字段来判断将数据插入到哪一个分区目录下，此时利用我们上面的分区方式是无法完成操作的，需要使用动态分区来完成相关操作，现在学的知识点无法满足，后续讲解。 */ Hive DML1、插入数据1、Loading files into tables/* 记载数据文件到某一张表中 语法： LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT &#39;inputformat&#39; SERDE &#39;serde&#39;] (3.0 or later) */ --加载本地数据到hive表 load data local inpath &#39;/root/data/data&#39; into table psn;--(/root/data/data指的是本地 linux目录) --加载hdfs数据文件到hive表 load data inpath &#39;/data/data&#39; into table psn;--(/data/data指的是hdfs的目录) /* 注意： 1、load操作不会对数据做任何的转换修改操作 2、从本地linux load数据文件是复制文件的过程 3、从hdfs load数据文件是移动文件的过程 4、load操作也支持向分区表中load数据，只不过需要添加分区列的值 */ 2、Inserting data into Hive Tables from queries/* 从查询语句中获取数据插入某张表 语法： Standard syntax: INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; Hive extension (multiple inserts): FROM from_statement INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 [INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] [INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...; FROM from_statement INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] [INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] ...; Hive extension (dynamic partition inserts): INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; */ --注意：这种方式插入数据的时候需要预先创建好结果表 --从表中查询数据插入结果表 INSERT OVERWRITE TABLE psn9 SELECT id,name FROM psn --从表中获取部分列插入到新表中 from psn insert overwrite table psn9 select id,name insert into table psn10 select id 3、Writing data into the filesystem from queries/* 将查询到的结果插入到文件系统中 语法： Standard syntax: INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0) SELECT ... FROM ... Hive extension (multiple inserts): FROM from_statement INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1 [INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] (Note: Only available starting with Hive 0.13) */ --注意：路径千万不要填写根目录，会把所有的数据文件都覆盖 --将查询到的结果导入到hdfs文件系统中 insert overwrite directory &#39;/result&#39; select * from psn; --将查询的结果导入到本地文件系统中 insert overwrite local directory &#39;/result&#39; select * from psn; 4、Inserting values into tables from SQL/* 使用传统关系型数据库的方式插入数据，效率较低 语法： Standard Syntax: INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...] Where values_row is: ( value [, value ...] ) where a value is either null or any valid SQL literal */ --插入数据 insert into psn values(1,&#39;zhangsan&#39;) 2、数据更新和删除 在官网中我们明确看到hive中是支持Update和Delete操作的，但是实际上，是需要事务的支持的，Hive对于事务的支持有很多的限制，如下图所示： 因此，在使用hive的过程中，我们一般不会产生删除和更新的操作，如果你需要测试的话，参考下面如下配置： //在hive的hive-site.xml中添加如下配置： &lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.enforce.bucketing&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt; &lt;value&gt;nonstrict&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.txn.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.compactor.worker.threads&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; //操作语句 create table test_trancaction (user_id Int,name String) clustered by (user_id) into 3 buckets stored as orc TBLPROPERTIES (&#39;transactional&#39;=&#39;true&#39;); create table test_insert_test(id int,name string) row format delimited fields TERMINATED BY &#39;,&#39;; insert into test_trancaction select * from test_insert_test; update test_trancaction set name=&#39;jerrick_up&#39; where id=1; //数据文件 1,jerrick 2,tom 3,jerry 4,lily 5,hanmei 6,limlei 7,lucky HiveSerde目的： ​ Hive Serde用来做序列化和反序列化，构建在数据存储和执行引擎之间，对两者实现解耦。 应用场景： ​ 1、hive主要用来存储结构化数据，如果结构化数据存储的格式嵌套比较复杂的时候，可以使用serde的方式，利用正则表达式匹配的方法来读取数据，例如，表字段如下：id,name,map&lt;string,array&lt;map&lt;string,string&gt;&gt;&gt; ​ 2、当读取数据的时候，数据的某些特殊格式不希望显示在数据中，如： 192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] “GET /bg-upper.png HTTP/1.1” 304 - 不希望数据显示的时候包含[]或者””,此时可以考虑使用serde的方式 语法规则： row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] : SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)] 应用案例： 1、数据文件 192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 - 192.168.57.4 - - [29/Feb/2019:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 - ​ 2、基本操作： --创建表 CREATE TABLE logtbl ( host STRING, identity STRING, t_user STRING, time STRING, request STRING, referer STRING, agent STRING) ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39; WITH SERDEPROPERTIES (&quot;input.regex&quot; = &quot;([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \&quot;(.*)\&quot; (-|[0-9]*) (-|[0-9]*)&quot; )STORED AS TEXTFILE; --加载数据 load data local inpath &#39;/root/data/log&#39; into table logtbl; --查询操作 select * from logtbl; --数据显示如下（不包含[]和&quot;） 192.168.57.4 - - 29/Feb/2019:18:14:35 +0800 GET /bg-upper.png HTTP/1.1 304 - 192.168.57.4 - - 29/Feb/2019:18:14:35 +0800 GET /bg-nav.png HTTP/1.1 304 - 192.168.57.4 - - 29/Feb/2019:18:14:35 +0800 GET /asf-logo.png HTTP/1.1 304 - 192.168.57.4 - - 29/Feb/2019:18:14:35 +0800 GET /bg-button.png HTTP/1.1 304 - 192.168.57.4 - - 29/Feb/2019:18:14:35 +0800 GET /bg-middle.png HTTP/1.1 304 - HiveServer2基本概念介绍1、HiveServer2基本介绍 HiveServer2 (HS2) is a server interface that enables remote clients to execute queries against Hive and retrieve the results (a more detailed intro here). The current implementation, based on Thrift RPC, is an improved version of HiveServer and supports multi-client concurrency and authentication. It is designed to provide better support for open API clients like JDBC and ODBC. HiveServer2是一个服务接口，能够允许远程的客户端去执行SQL请求且得到检索结果。HiveServer2的实现，依托于Thrift RPC，是HiveServer的提高版本，它被设计用来提供更好的支持对于open API例如JDBC和ODBC。 HiveServer is an optional service that allows a remote client to submit requests to Hive, using a variety of programming languages, and retrieve results. HiveServer is built on Apache ThriftTM (http://thrift.apache.org/), therefore it is sometimes called the Thrift server although this can lead to confusion because a newer service named HiveServer2 is also built on Thrift. Since the introduction of HiveServer2, HiveServer has also been called HiveServer1. HiveServer是一个可选的服务，只允许一个远程的客户端去提交请求到hive中。（目前已被淘汰） 2、Beeline HiveServer2 supports a command shell Beeline that works with HiveServer2. It’s a JDBC client that is based on the SQLLine CLI 。 HiveServer2提供了一种新的命令行接口，可以提交执行SQL语句。 hiveserver2的搭建使用在搭建hiveserver2服务的时候需要修改hdfs的超级用户的管理权限，修改配置如下： --在hdfs集群的core-site.xml文件中添加如下配置文件 &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; --配置完成之后重新启动集群，或者在namenode的节点上执行如下命令 hdfs dfsadmin -fs hdfs://node01:8020 -refreshSuperUserGroupsConfiguration hdfs dfsadmin -fs hdfs://node02:8020 -refreshSuperUserGroupsConfiguration 1、独立hiveserver2模式1、将现有的所有hive的服务停止，不需要修改任何服务，在node03机器上执行hiveserver2或者hive –service hiveserver2的命令，开始启动hiveserver2的服务，hiveserver2的服务也是一个阻塞式窗口，当开启服务后，会开启一个10000的端口，对外提供服务。 2、在node04上使用beeline的方式进行登录 2、共享metastore server的hiveserver2模式搭建1、在node03上执行hive –service metastore启动元数据服务 2、在node04上执行hiveserver2或者hive –service hiveserver2两个命令其中一个都可以 3、在任意一台包含beeline脚本的虚拟机中执行beeline的命令进行连接 HiveServer2的访问方式1、beeline的访问方式（1）beeline -u jdbc:hive2://:/ -n name​（2）beeline进入到beeline的命令行 beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;db&gt; root 123 注意： ​ 1、使用beeline方式登录的时候，默认的用户名和密码是不验证的，也就是说随便写用户名和密码即可 ​ 2、使用第一种beeline的方式访问的时候，用户名和密码可以不输入 ​ 3、使用第二种beeline方式访问的时候，必须输入用户名和密码，用户名和密码是什么无所谓 2、jdbc的访问方式1、创建普通的java项目，将hive的jar包添加到classpath中，最精简的jar包如下： commons-lang-2.6.jar commons-logging-1.2.jar curator-client-2.7.1.jar curator-framework-2.7.1.jar guava-14.0.1.jar hive-exec-2.3.4.jar hive-jdbc-2.3.4.jar hive-jdbc-handler-2.3.4.jar hive-metastore-2.3.4.jar hive-service-2.3.4.jar hive-service-rpc-2.3.4.jar httpclient-4.4.jar httpcore-4.4.jar libfb303-0.9.3.jar libthrift-0.9.3.jar log4j-1.2-api-2.6.2.jar log4j-api-2.6.2.jar log4j-core-2.6.2.jar log4j-jul-2.5.jar log4j-slf4j-impl-2.6.2.jar log4j-web-2.6.2.jar zookeeper-3.4.6.jar 2、编辑如下代码： package com.mashibing; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; public class HiveJdbcClient { private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws SQLException { try { Class.forName(driverName); } catch (ClassNotFoundException e) { e.printStackTrace(); } Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://node04:10000/default&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = conn.createStatement(); String sql = &quot;select * from psn limit 5&quot;; ResultSet res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1) + &quot;-&quot; + res.getString(&quot;name&quot;)); } } } 运行之后，即可得到最终结果。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive基础知识]]></title>
    <url>%2F2019%2F08%2F12%2FHive%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[01 Hive的基本介绍Hive的官方介绍1、hive产生的原因a) 方便对文件及数据的元数据进行管理，提供统一的元数据管理方式b) 提供更加简单的方式来访问大规模的数据集，使用SQL语言进行数据分析 2、hive是什么？The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.​ Hive经常被大数据企业用作企业级数据仓库。 ​ Hive在使用过程中是使用SQL语句来进行数据分析，由SQL语句到具体的任务执行还需要经过解释器，编译器，优化器，执行器四部分才能完成。 ​ （1）解释器：调用语法解释器和语义分析器将SQL语句转换成对应的可执行的java代码或者业务代码 ​ （2）编译器：将对应的java代码转换成字节码文件或者jar包 ​ （3）优化器：从SQL语句到java代码的解析转化过程中需要调用优化器，进行相关策略的优化，实现最优的 查询性能 ​ （4）执行器：当业务代码转换完成之后，需要上传到MapReduce的集群中执行 3、数据仓库–Hive1、数据仓库基本概念​ 数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。 2、数据处理分类：OLAP与OLTP​ 数据处理大致可以分成两大类：联机事务处理OLTP（on-line transaction processing）、联机分析处理OLAP（On-Line Analytical Processing）。OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 3、OLTP​ OLTP，也叫联机事务处理（Online Transaction Processing），表示事务性非常高的系统，一般都是高可用的在线系统，以小的事务以及小的查询为主，评估其系统的时候，一般看其每秒执行的Transaction以及Execute SQL的数量。在这样的系统中，单个数据库每秒处理的Transaction往往超过几百个，或者是几千个，Select 语句的执行量每秒几千甚至几万个。典型的OLTP系统有电子商务系统、银行、证券等，如美国eBay的业务数据库，就是很典型的OLTP数据库。 4、OLAP​ OLAP（On-Line Analysis Processing）在线分析处理是一种共享多维信息的快速分析技术；OLAP利用多维数据库技术使用户从不同角度观察数据；OLAP用于支持复杂的分析操作，侧重于对管理人员的决策支持，可以满足分析人员快速、灵活地进行大数据复量的复杂查询的要求，并且以一种直观、易懂的形式呈现查询结果，辅助决策。 基本概念：​ 度量：数据度量的指标，数据的实际含义​ 维度：描述与业务主题相关的一组属性​ 事实：不同维度在某一取值下的度量 特点：​ (1)快速性：用户对OLAP的快速反应能力有很高的要求。系统应能在5秒内对用户的大部分分析要求做出反 应。​ (2)可分析性：OLAP系统应能处理与应用有关的任何逻辑分析和统计分析。 ​ (3)多维性：多维性是OLAP的关键属性。系统必须提供对数据的多维视图和分析,包括对层次维和多重层次 维的完全支持。​ (4)信息性：不论数据量有多大，也不管数据存储在何处，OLAP系统应能及时获得信息，并且管理大容量信 息。 分类：​ 按照存储方式分类： ​ ROLAP：关系型在线分析处理 ​ MOLAP：多维在线分析处理 ​ HOLAP：混合型在线分析处理 ​ 按照处理方式分类： ​ Server OLAP和Client OLAP 操作： ​ 钻取：在维的不同层次间的变化，从上层降到下一层，或者说将汇总数据拆分到更细节的数据，比如通过 对2019年第二季度的总销售数据进行钻取来查看2019年4,5,6,每个月的消费数据，再例如可以钻取 浙江省来查看杭州市、温州市、宁波市……这些城市的销售数据​ 上卷：钻取的逆操作，即从细粒度数据向更高汇总层的聚合，如将江苏省、上海市、浙江省的销售数据进 行汇总来查看江浙沪地区的销售数据​ 切片：选择维中特定的值进行分析，比如只选择电子产品的销售数据或者2019年第二季度的数据​ 切块：选择维中特定区间的数据或者某批特定值进行分析，比如选择2019年第一季度到第二季度的销售数 据或者是电子产品和日用品的销售数据​ 旋转：维的位置互换，就像是二维表的行列转换，比如通过旋转来实现产品维和地域维的互换 4、数据库与数据仓库的区别​ 1、数据库是对业务系统的支撑，性能要求高，相应的时间短，而数据仓库则对响应时间没有太多的要求，当然也是越快越好 ​ 2、数据库存储的是某一个产品线或者某个业务线的数据，数据仓库可以将多个数据源的数据经过统一的规则清洗之后进行集中统一管理 ​ 3、数据库中存储的数据可以修改，无法保存各个历史时刻的数据，数据仓库可以保存各个时间点的数据，形成时间拉链表，可以对各个历史时刻的数据做分析 ​ 4、数据库一次操作的数据量小，数据仓库操作的数据量大 ​ 5、数据库使用的是实体-关系（E-R）模型，数据仓库使用的是星型模型或者雪花模型 ​ 6、数据库是面向事务级别的操作，数据仓库是面向分析的操作 02 Hive的架构Hive的官方架构图1、Hive的架构图 2、Hive的服务（角色）1、用户访问接口​ CLI（Command Line Interface）：用户可以使用Hive自带的命令行接口执行Hive QL、设置参数等功能 ​ JDBC/ODBC：用户可以使用JDBC或者ODBC的方式在代码中操作Hive ​ Web GUI：浏览器接口，用户可以在浏览器中对Hive进行操作（2.2之后淘汰） 2、Thrift Server:​ Thrift服务运行客户端使用Java、C++、Ruby等多种语言，通过编程的方式远程访问Hive 3、Driver​ Hive Driver是Hive的核心，其中包含解释器、编译器、优化器等各个组件，完成从SQL语句到MapReduce任务的解析优化执行过程 4、metastore​ Hive的元数据存储服务，一般将数据存储在关系型数据库中，为了实现Hive元数据的持久化操作，Hive的安装包中自带了Derby内存数据库，但是在实际的生产环境中一般使用mysql来存储元数据 3、Hive的访问流程图 03 Hive的安装搭建Hive可以从源码中编译安装，也可以直接使用官网下载的安装包，在此处我们选择安装包解压安装的方式。 Hive中最最重要的角色就是metastore 因此按照metastore的管理共有四种hive的安装搭建方式：官网参考地址如下： https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration Hive安装分类：​ 1、Local/Embedded Metastore Database（Derby） ​ 2、Remote Metastore Database ​ 3、Local/Embedded Metastore Server ​ 4、Remote Metastore Server ​ 根据上述分类，可以简单归纳为以下三类 ​ 1、使用Hive自带的内存数据库Derby作为元数据存储 ​ 2、使用远程数据库mysql作为元数据存储 ​ 3、使用本地/远程元数据服务模式安装Hive 详细操作：​ 1、使用Hive自带的内存数据库Derby作为元数据存储 ​ 2、使用远程数据库mysql作为元数据存储 ) ​ 3、使用本地/远程元数据服务模式安装Hive]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring名词解释]]></title>
    <url>%2F2019%2F08%2F10%2FSpring%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[SpringServletRequest定义一个对象，以便向servlet提供客户机请求信息。servlet容器创建一个ServletRequest对象，并将其作为参数传递给servlet的服务方法。ServletRequest对象提供包括参数名称和值、属性和输入流在内的数据。扩展ServletRequest的接口可以提供额外的特定于协议的数据(例如，HTTP数据由javax.servlet.http.HttpServletRequest提供)。 ServletResponse定义一个对象来帮助servlet向客户机发送响应。servlet容器创建一个ServletResponse对象，并将其作为参数传递给servlet的服务方法。要在MIME主体响应中发送二进制数据，请使用getOutputStream返回的ServletOutputStream。要发送字符数据，请使用getWriter返回的PrintWriter对象。例如，要混合二进制和文本数据以创建多部分响应，可以使用ServletOutputStream并手动管理字符部分。MIME主体响应的字符集可以显式或隐式指定。指定响应主体的优先顺序为:使用setCharacterEncoding和setContentType显式地处理每个请求使用setLocale隐式地处理每个请求每个web应用程序通过部署描述符或ServletContext.setRequestCharacterEncoding(字符串)容器默认通过特定于供应商的配置iso - 8859 - 1必须在getWriter和提交要使用的字符编码的响应之前调用setCharacterEncoding、setContentType或setLocale方法。有关MIME的更多信息，请参阅Internet RFC 2045等RFC。SMTP和HTTP等协议定义了MIME的概要文件，这些标准还在不断发展。 HttpServletRequest扩展ServletRequest接口，为HTTP servlet提供请求信息。servlet容器创建一个HttpServletRequest对象，并将其作为参数传递给servlet的服务方法(doGet、doPost等)。 HttpServletResponse扩展ServletResponse接口，在发送响应时提供http特定的功能。例如，它有访问HTTP头文件和cookie的方法。servlet容器创建一个HttpServletResponse对象，并将其作为参数传递给servlet的服务方法(doGet、doPost等)。 HttpSession提供一种方法，可以跨多个页面请求或访问Web站点来标识用户，并存储有关该用户的信息。servlet容器使用这个接口在HTTP客户机和HTTP服务器之间创建会话。会话在来自用户的多个连接或页面请求之间持续存在指定的时间段。一个会话通常对应一个用户，该用户可能多次访问一个站点。服务器可以通过许多方式维护会话，比如使用cookie或重写url。这个接口允许servlet这样做 · 查看和操作有关会话的信息，如会话标识符、创建时间和最后访问时间 · 将对象绑定到会话，允许用户信息跨多个用户连接持久存储当应用程序在会话中存储对象或从会话中删除对象时，会话将检查该对象是否实现HttpSessionBindingListener。如果它这样做了，servlet将通知对象它已被绑定到会话或从会话解除绑定。绑定方法完成后发送通知。对于无效或过期的会话，将在会话无效或过期后发送通知。 当容器在分布式容器设置中的vm之间迁移会话时，将通知实现HttpSessionActivationListener接口的所有会话属性。servlet应该能够处理客户机不选择加入会话的情况，比如在有意关闭cookie时。在客户机加入会话之前，isNew返回true。如果客户机选择不加入会话，getSession将对每个请求返回一个不同的会话，isNew将始终返回true。会话信息的作用域仅限于当前web应用程序(ServletContext)，因此存储在一个上下文中的信息在另一个上下文中不会直接可见。 Cookie创建一个cookie，由servlet发送到Web浏览器的少量信息，由浏览器保存，然后发送回服务器。 cookie的值可以唯一标识客户端，因此cookie通常用于会话管理。 Cookie具有名称，单个值和可选属性，例如注释，路径和域限定符，最大年龄和版本号。某些Web浏览器在处理可选属性方面存在缺陷，因此请谨慎使用它们以提高servlet的互操作性。 servlet使用HttpServletResponse.addCookie方法将cookie发送到浏览器，该方法将字段添加到HTTP响应标头，以便一次一个地向浏览器发送cookie。浏览器预计每个Web服务器支持20个cookie，总共300个cookie，并且可能将cookie大小限制为每个4KB。浏览器通过向HTTP请求标头添加字段将cookie返回给servlet。可以使用HttpServletRequest.getCookies方法从请求中检索Cookie。多个cookie可能具有相同的名称但路径属性不同。 Cookie会影响使用它们的网页的缓存。HTTP 1.0不会缓存使用使用此类创建的cookie的页面。此类不支持使用HTTP 1.1定义的缓存控制。此类支持RFC 2109和RFC 6265规范。默认情况下，使用RFC 6265创建cookie。 Model特定于Java-5的接口，用于定义模型属性的持有者。主要用于向模型添加属性。允许以java.util.Map的形式访问整个模型。 MultipartFile在多部分请求中接收的上载文件的表示。文件内容存储在内存中或临时存储在磁盘上。在任何一种情况下，如果需要，用户负责将文件内容复制到会话级或持久性存储。临时存储将在请求处理结束时清除。 @Controller表示带注释的类是“控制器”（例如Web控制器）。此注释用作@Component的特化，允许通过类路径扫描自动检测实现类。它通常与基于org.springframework.web.bind.annotation.RequestMapping批注的带注释的处理程序方法结合使用。 @Component表示带注释的类是“组件”。当使用基于注释的配置和类路径扫描时，这些类被视为自动检测的候选者。其他类级注释也可以被认为是识别组件，通常是特殊类型的组件：例如， @Repository注释或AspectJ的@Aspect注释。 @Repository表示带注释的类是“存储库”，最初由域驱动设计（Evans，2003）定义为“用于封装模拟对象集合的存储，检索和搜索行为的机制”。实现传统Java EE模式（如“数据访问对象”）的团队也可以将此构造型应用于DAO类，但在此之前应注意理解数据访问对象和DDD样式存储库之间的区别。这个注释是一个通用的刻板印象，个别团队可能会缩小其语义并在适当时使用。当与PersistenceExceptionTranslationPostProcessor一起使用时，这样注释的类有资格进行Spring DataAccessException转换。带注释的类还阐明了它在整个应用程序体系结构中的作用，用于工具，方面等。从Spring 2.5开始，这个注释也用作@Component的特化，允许通过类路径自动检测实现类扫描。 @RequestMapping使用灵活方法签名将Web请求映射到请求处理类中的方法的注释。Spring MVC和Spring WebFlux都通过RequestMappingHandlerMapping和RequestMappingHandlerAdapter在它们各自的模块和包结构中支持这个注释。有关每个支持的处理程序方法参数和返回类型的确切列表，请使用下面的参考文档链接： · Spring MVC方法参数和返回值Spring · WebFlux方法参数和返回值注意：此批注可以在类和方法级别。在大多数情况下，在方法级别，应用程序将更喜欢使用HTTP方法特定变体之一@GetMapping，@PostMapping，@PutMapping，@DelaMapping或@PatchMapping。注意：使用控制器接口（例如，用于AOP代理）时，请确保始终将所有映射注释（例如@RequestMapping和@SessionAttributes）放在控制器接口而不是实现类上。 @RequestBody指示方法返回值的注释应绑定到Web响应主体。支持带注释的处理程序方法。从版本4.0开始，此注释也可以添加到类型级别，在这种情况下，它是继承的，不需要在方法级别添加。 @GetMapping用于将HTTP GET请求映射到特定处理程序方法的注释。具体来说，@ GetMapping是一个组合注释，充当@RequestMapping（method = RequestMethod.GET）的快捷方式。 @PostMapping用于将HTTP POST请求映射到特定处理程序方法的注释。具体来说，@ PostMapping是一个组合注释，充当@RequestMapping（method = RequestMethod.POST）的快捷方式。 @PutMapping用于将HTTP PUT请求映射到特定处理程序方法的注释。具体来说，@ PutMapping是一个组合注释，充当@RequestMapping（method = RequestMethod.PUT）的快捷方式。 @DeleteMapping用于将HTTP DELETE请求映射到特定处理程序方法的注释。具体来说，@ DeleteMapping是一个组合注释，充当@RequestMapping（method = RequestMethod.DELETE）的快捷方式。 @PickMapping用于将HTTP PATCH请求映射到特定处理程序方法的注释。具体来说，@ PickMapping是一个组合注释，充当@RequestMapping（method = RequestMethod.PATCH）的快捷方式。 RequestMethodJava5枚举HTTP请求方法。旨在与RequestMapping批注的RequestMapping.method（）属性一起使用。请注意，默认情况下，org.springframework.web.servlet.DispatcherServlet仅支持GET，HEAD，POST，PUT，PATCH和DELETE。 DispatcherServlet将使用默认的HttpServlet行为处理TRACE和OPTIONS，除非明确告知也要分派这些请求类型：检查“dispatchOptionsRequest”和“dispatchTraceRequest”属性，必要时将它们切换为“true”。 @Autowired将构造函数，字段，setter方法或配置方法标记为由Spring的依赖注入工具自动装配。这是JSR-330 javax.inject.Inject注释的替代方法，添加了必需与可选的语义。任何给定bean类只有一个构造函数（最大值）可以声明这个注释，并将’required’参数设置为true，表示构造函数在用作Spring bean时要自动装配。如果多个非必需构造函数声明了注释，则它们将被视为自动装配的候选者。将选择具有最大数量的依赖项的构造函数，这些构造函数可以通过匹配Spring容器中的bean来满足。如果不能满足任何候选者，则将使用主要/默认构造函数（如果存在）。如果一个类只声明一个构造函数开头，它将始终被使用，即使没有注释。带注释的构造函数不必是公共的。在调用任何配置方法之前，在构造bean之后立即注入字段。这样的配置字段不必是公共的。配置方法可以有任意名称和任意数量的参数;每个参数都将使用Spring容器中的匹配bean进行自动装配。 Bean属性设置器方法实际上只是这种通用配置方法的特例。这种配置方法不必是公开的。对于多参数构造函数或方法，’required’参数适用于所有参数。单个参数可以声明为Java-8样式的java.util.Optional，或者从Spring Framework 5.0开始，也可以在Kotlin中声明为@Nullable或非null参数类型，从而覆盖基本所需的语义。对于java.util.Collection或java.util.Map依赖类型，容器会自动装配与声明的值类型匹配的所有bean。为此目的，必须将映射键声明为String类型，并将其解析为相应的bean名称。这样一个容器提供的集合将被排序，考虑到目标组件的org.springframework.core.Ordered / org.springframework.core.annotation.Order值，否则遵循它们在容器中的注册顺序。或者，单个匹配的目标bean也可以是通常类型的Collection或Map本身，如此注入。请注意，实际注入是通过BeanPostProcessor执行的，而BeanPostProcessor又意味着您无法使用@Autowired将引用注入BeanPostProcessor或BeanFactoryPostProcessor类型。请参考javadoc获取AutowiredAnnotationBeanPostProcessor类（默认情况下，它会检查是否存在此批注）。 @Service表示带注释的类是“服务”，最初由域驱动设计（Evans，2003）定义为“作为模型中独立的接口提供的操作，没有封装状态”。也可能表明一个类是“业务服务门面”（在核心J2EE模式意义上）或类似的东西。这个注释是一个通用的刻板印象，个别团队可能会缩小其语义并在适当时使用。此注释用作@Component的特化，允许通过类路径扫描自动检测实现类。 @MapperScan使用Java Config时，使用此批注注册MyBatis映射器接口。它通过MapperScannerRegistrar执行与MapperScannerConfigurer相同的工作。配置示例： @Configuration @MapperScan(&quot;org.mybatis.spring.sample.mapper&quot;) public class AppConfig { @Bean public DataSource dataSource() { return new EmbeddedDatabaseBuilder().addScript(&quot;schema.sql&quot;).build(); } @Bean public DataSourceTransactionManager transactionManager() { return new DataSourceTransactionManager(dataSource()); } @Bean public SqlSessionFactory sqlSessionFactory() throws Exception { SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(dataSource()); return sessionFactory.getObject(); } } Junit@Before编写测试时，通常会发现多个测试需要在运行之前创建类似的对象。使用@Before注释public void方法会导致该方法在Test方法之前运行。 @Before超类方法将在当前类的方法之前运行，除非它们在当前类中被重写。没有定义其他排序。这是一个简单的例子： public class Example { List empty; @Before public void initialize（）{ empty = new ArrayList（）; } @Test public void size（）{ ... } @Test public void remove（）{ ... } } @Testannotation告诉JUnit它附加的公共void方法可以作为测试用例运行。要运行该方法，JUnit首先构造一个新的类实例，然后调用带注释的方法。 JUnit将报告测试引发的任何异常为失败。如果没有抛出异常，则假定测试成功。一个简单的测试如下所示： Test annotation支持两个可选参数。第一个是预期的，声明测试方法应该抛出异常。如果它不抛出异常或抛出与声明的异常不同的异常，则测试失败。 例如，以下测试成功： @Test（expected = IndexOutOfBoundsException.class） public void outOfBounds（）{ new ArrayList &lt;Object&gt;（）.get（1）; } 如果应验证异常的消息或其某个属性，则可以使用ExpectedException规则。有关异常测试的更多信息，请访问JUnit Wiki。第二个可选参数timeout会在测试时间超过指定的时钟时间（以毫秒为单位）时导致测试失败。以下测试失败： @Test（timeout = 100） public void infinity（）{ while（true）; } 警告：虽然超时对于捕获和终止无限循环很有用，但它不应被视为确定性的。以下测试可能会失败，也可能不会失败，具体取决于操作系统如何调度线程： @Test（timeout = 100） public void sleep100（）{ Thread.sleep（100）; } 线程安全警告：带有超时参数的测试方法在运行夹具的@Before和@After方法的线程以外的线程中运行。与没有超时参数的相同测试方法相比，这可能会对非线程安全的代码产生不同的行为。请考虑使用Timeout规则，以确保测试方法在与fixture的@Before和@After方法相同的线程上运行。 @After如果在Before方法中分配外部资源，则需要在测试运行后释放它们。使用@After注释公共void方法会导致该方法在Test方法之后运行。即使Before或Test方法抛出异常，也保证所有@After方法都能运行。在超类中声明的@After方法将在当前类的那些之后运行，除非它们在当前类中被重写。这是一个简单的例子： public class Example { File output; @Before public void createOutputFile（）{ output = new File（...）; } @Test public void something（）{ ... } @After public void deleteOutputFile（）{ output.delete（）; } }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase名词解释]]></title>
    <url>%2F2019%2F08%2F10%2FHbase%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[HbasePut用于执行单行的Put操作。要执行Put，请实例化包含要插入的行的Put对象，并为要插入的每个列执行添加或添加（如果设置时间戳）。 TableReducer扩展基本的Reducer类以添加所需的键和值输入/输出类。虽然输入键和值以及输出键可以是从前一个映射阶段输入的任何内容，但在使用TableOutputFormat类时，输出值必须是Put或Delete实例。此类由IdentityTableReducer扩展，但也可以进行子类化以实现类似的功能或所需的任何自定义代码。它具有将输出值强制为特定基本类型的优点。 AdminHBase的管理API。从Connection.getAdmin（）获取实例并在完成时调用close（）。 Admin可用于创建，删除，列出，启用和禁用以及以其他方式修改表，以及执行其他管理操作。 Table用于与单个HBase表通信。从Connection获取实例并在之后调用close（）。表可用于从表中获取，放置，删除或扫描数据。 TableName用于表示表名的不可变POJO类。其形式如下：：两个特殊名称空间： ​ 1.hbase - 系统名称空间，用于包含hbase内部表 ​ 2.default - 没有明确指定名称空间的表将自动落入此名称空间。即a）foo：bar，表示namespace = foo和qualifier = bar b）bar，表示namespace = default和qualifier = bar c）default：bar，表示namespace = default和qualifier = bar 在内部，在这个类中，我们缓存实例来限制对象的数量并使“等于”更快。我们尝试最小化创建的数组副本数量，以检查我们是否已经拥有此TableName的实例。代码未针对新实例创建进行优化，但已经过优化以检查是否存在。 TableDescriptorTableDescriptor包含有关HBase表的详细信息，例如所有列族的描述符，表是目录表，hbase：meta，如果表是只读的，则memstore的最大大小，当应该发生区域分割时，与之相关的协处理器等…… Bytes实用程序类，用于处理字节数组，与其他类型的转换，比较，哈希代码生成，HashMaps或HashSet的制造密钥，以及可用作映射或树中的键。 建表样例 public void createTable() throws IOException { if (admin.tableExists(tableName)) { admin.disableTable(tableName); admin.deleteTable(tableName); logger.info(&quot;delete table success&quot;); } TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(tableName); ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(&quot;cf&quot;.getBytes()); tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptorBuilder.build()); TableDescriptor build = tableDescriptorBuilder.build(); admin.createTable(build); logger.info(&quot;build table success&quot;); } 插入数据样例 public void put() throws IOException { Put put = new Put(Bytes.toBytes(&quot;1111&quot;)); put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;aqua&quot;)); put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(&quot;12&quot;)); put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;sex&quot;), Bytes.toBytes(&quot;girl&quot;)); table.put(put); logger.info(&quot;put data success&quot;); } Scan用于执行扫描操作。除实例化外，所有操作都与Get相同。可以定义可选的startRow和stopRow，而不是指定单个行。如果未指定行，则扫描程序将遍历所有行。要从Table的所有行获取所有列，请创建一个没有约束的实例;使用Scan（）构造函数。要将扫描约束到特定列族，请为扫描实例上的每个族调用addFamily。要获取特定列，请为要检索的每列调用addColumn。要仅检索特定版本时间戳范围内的列，请调用setTimeRange。要仅检索具有特定时间戳的列，请调用setTimestamp。要限制要返回的每列的版本数，请调用setMaxVersions。要限制每次调用next（）返回的最大值数，请调用setBatch。要添加过滤器，请调用setFilter。对于小型扫描，它在2.0.0中已弃用。现在我们在Scan对象中有一个setLimit（int）方法，用于告诉RS我们想要多少行。如果行返回达到限制，RS将自动关闭RegionScanner。我们还将在新实现中的openScanner中获取数据，这意味着我们也可以在一次rpc调用中完成扫描操作。我们还引入了setReadType（Scan.ReadType）方法。您可以使用此方法告诉RS显式使用pread。专家：要为此扫描显式禁用服务器端块缓存，请执行setCacheBlocks（boolean）。注意：用法会更改扫描实例。在内部，属性会在扫描运行时更新，如果启用，则会在Scan实例中累积指标。当你去克隆一个Scan实例或者你去重用一个创建的Scan实例时，请注意这种情况;更安全的是每次使用创建一个Scan实例。 Scan样例 public void scan() throws IOException { Scan scan = new Scan(); ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) { //使用scan进行查询 Cell cellName = result.getColumnLatestCell(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;name&quot;)); String name = Bytes.toString(CellUtil.cloneValue(cellName)); Cell cellAge = result.getColumnLatestCell(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;age&quot;)); String age = Bytes.toString(CellUtil.cloneValue(cellAge)); Cell cellSex = result.getColumnLatestCell(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;sex&quot;)); String sex = Bytes.toString(CellUtil.cloneValue(cellSex)); logger.info(name); logger.info(age); logger.info(sex); } } CellHBase中的存储单元由以下字段组成：1）行2）列族3）列限定符4）时间戳5）类型6）MVCC版本7）值唯一性由行，列族，列限定符的组合确定，时间戳和类型。自然比较器将对行，列族和列限定符执行按位比较。不那么直观，它会将更大的时间戳视为较小的值，目的是首先对较新的单元格进行排序。Cell实现Comparable ，只有在与同一个表中的其他键进行比较时才有意义。它使用CellComparator，它不适用于-ROOT-和hbase：meta表。将来，我们可以考虑添加一个布尔值isOnHeap（）方法和一个getValueBuffer（）方法，该方法可用于将值直接从堆外ByteBuffer传递到网络，而无需复制到堆上字节[]。历史记录：原始Cell实现（KeyValue）要求所有字段在同一个byte []中编码为连续字节，而此接口允许字段驻留在单独的byte []中。 Cell样例 public void get() throws IOException { Get get = new Get(Bytes.toBytes(&quot;1111&quot;)); //也可以直接添加列族,使用get.addFamily方法 get.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;name&quot;)); get.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;age&quot;)); get.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;sex&quot;)); Result result = table.get(get); logger.info(&quot;request:&quot; + result.toString()); Cell cellName = result.getColumnLatestCell(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;name&quot;)); Cell cellAge = result.getColumnLatestCell(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;age&quot;)); Cell cellSex = result.getColumnLatestCell(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;sex&quot;)); String age = Bytes.toString(CellUtil.cloneValue(cellAge)); String name = Bytes.toString(CellUtil.cloneValue(cellName)); String sex = Bytes.toString(CellUtil.cloneValue(cellSex)); logger.info(name + &quot;,&quot; + age + &quot;,&quot; + sex); }]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础]]></title>
    <url>%2F2019%2F08%2F10%2FJava%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[JAVA基础StringString类表示字符串。 Java程序中的所有字符串文字（例如“abc”）都实现为此类的实例。字符串是不变的;它们的值在创建后无法更改。字符串缓冲区支持可变字符串。因为String对象是不可变的，所以可以共享它们。例如： String str = &quot;abc&quot;; 相当于： 以下是一些如何使用字符串的示例： System.out.println(&quot;abc&quot;); String cde = &quot;cde&quot;; System.out.println(&quot;abc&quot; + cde); String c = &quot;abc&quot;.substring(2,3); String d = cde.substring(1, 2); String类包括用于检查序列的各个字符，用于比较字符串，搜索字符串，提取子字符串以及创建字符串副本的方法，其中所有字符都转换为大写或小写。案例映射基于Character类指定的Unicode标准版本。 Java语言为字符串连接运算符（+）提供特殊支持，并为其他对象转换为字符串。字符串连接是通过StringBuilder（或StringBuffer）类及其append方法实现的。字符串转换是通过方法toString实现的，由Object定义并由Java中的所有类继承。有关字符串连接和转换的其他信息，请参阅Gosling，Joy和Steele，Java语言规范。除非另有说明，否则将null参数传递给此类中的构造函数或方法将导致抛出NullPointerException。 String表示UTF-16格式的字符串，其中补充字符由代理项对表示（有关详细信息，请参阅Character类中的Unicode字符表示形式一节）。索引值是指char代码单元，因此补充字符在String中使用两个位置。 String类提供了处理Unicode代码点（即字符）的方法，以及处理Unicode代码单元（即char值）的方法。 以下是一些如何使用字符串的示例： String类包括用于检查序列的各个字符，用于比较字符串，搜索字符串，提取子字符串以及创建字符串副本的方法，其中所有字符都转换为大写或小写。案例映射基于Character类指定的Unicode标准版本。 Java语言为字符串连接运算符（+）提供特殊支持，并为其他对象转换为字符串。字符串连接是通过StringBuilder（或StringBuffer）类及其append方法实现的。字符串转换是通过方法toString实现的，由Object定义并由Java中的所有类继承。有关字符串连接和转换的其他信息，请参阅Gosling，Joy和Steele，Java语言规范。除非另有说明，否则将null参数传递给此类中的构造函数或方法将导致抛出NullPointerException。 String表示UTF-16格式的字符串，其中补充字符由代理项对表示（有关详细信息，请参阅Character类中的Unicode字符表示形式一节）。索引值是指char代码单元，因此补充字符在String中使用两个位置。 String类提供了处理Unicode代码点（即字符）的方法，以及处理Unicode代码单元（即char值）的方法。 Connection集合层次结构中的根接口。集合表示一组对象，称为其元素。有些集合允许重复元素而其他集合则不允许。有些是订购的，有些是无序的。JDK不提供此接口的任何直接实现：它提供了更具体的子接口（如Set和List）的实现。此接口通常用于传递集合并在需要最大通用性的情况下对其进行操作。包或多个集合（可能包含重复元素的无序集合）应直接实现此接口。所有通用Collection实现类（通常通过其子接口间接实现Collection）应提供两个“标准”构造函数：void（无参数）构造函数，它创建一个空集合，以及一个构造函数，其中一个参数类型Collection，使用与其参数相同的元素创建新集合。实际上，后一个构造函数允许用户复制任何集合，从而生成所需实现类型的等效集合。没有办法强制执行此约定（因为接口不能包含构造函数），但Java平台库中的所有通用Collection实现都符合。如果此集合不支持该操作，则指定此接口中包含的“破坏性”方法（即修改其操作集合的方法）将抛出UnsupportedOperationException。如果是这种情况，如果调用对集合没有影响，则这些方法可能（但不是必须）抛出UnsupportedOperationException。例如，如果要添加的集合为空，则可以（但不是必须）在不可修改的集合上调用addAll（Collection）方法抛出异常。某些集合实现对它们可能包含的元素有限制。例如，某些实现禁止null元素，并且一些实现对其元素的类型有限制。尝试添加不合格的元素会引发未经检查的异常，通常是NullPointerException或ClassCastException。尝试查询不合格元素的存在可能会引发异常，或者它可能只是返回false;一些实现将展示前一种行为，一些将展示后者。更一般地，尝试对不合格的元素进行操作，其完成不会导致将不合格的元素插入到集合中，可以在实现的选择中抛出异常或者它可以成功。此类异常在此接口的规范中标记为“可选”。由每个集合决定自己的同步策略。在实现没有更强的保证的情况下，未定义的行为可能是由于另一个线程正在变异的集合上的任何方法的调用引起的;这包括直接调用，将​​集合传递给可能执行调用的方法，以及使用现有迭代器来检查集合。 Collections Framework接口中的许多方法都是根据equals方法定义的。例如，contains（Object o）方法的规范说：“当且仅当此集合包含至少一个元素e时才返回true（o == null？e == null：o.equals（e）） “。不应将此规范解释为暗示使用非null参数o调用Collection.contains将导致对任何元素e调用o.equals（e）。实现可以自由地实现优化，从而避免等于调用，例如，首先比较两个元素的哈希码。（Object.hashCode（）规范保证具有不等哈希码的两个对象不能相等。）更一般地，各种集合框架接口的实现可以自由地利用底层Object方法的指定行为，只要实现者认为它是合适的。执行集合的递归遍历的某些集合操作可能会失败，而自引用实例的例外情况是集合直接或间接包含自身。这包括clone（），equals（），hashCode（）和toString（）方法。实现可以可选地处理自引用场景，但是大多数当前实现不这样做。此接口是Java Collections Framework的成员。 HashMap基于哈希表的Map接口实现。此实现提供所有可选的映射操作，并允许空值和空键。 （HashMap类大致相当于Hashtable，除了它是不同步的并且允许空值。）这个类不保证地图的顺序;特别是，它不保证订单会随着时间的推移保持不变。假设散列函数在桶之间正确地分散元素，该实现为基本操作（get和put）提供了恒定时间性能。对集合视图的迭代需要与HashMap实例的“容量”（桶的数量）加上其大小（键 - 值映射的数量）成比例的时间。因此，如果迭代性能很重要，则不要将初始容量设置得太高（或负载因子太低）非常重要。 HashMap的一个实例有两个影响其性能的参数：初始容量和负载因子。容量是哈希表中的桶数，初始容量只是创建哈希表时的容量。加载因子是在自动增加容量之前允许哈希表获取的完整程度的度量。当哈希表中的条目数超过加载因子和当前容量的乘积时，哈希表将被重新哈希（即，重建内部数据结构），以便哈希表具有大约两倍的桶数。作为一般规则，默认加载因子（.75）在时间和空间成本之间提供了良好的权衡。较高的值会减少空间开销，但会增加查找成本（反映在HashMap类的大多数操作中，包括get和put）。在设置其初始容量时，应考虑映射中的预期条目数及其加载因子，以便最小化重新散列操作的数量。如果初始容量大于最大条目数除以加载因子，则不会发生重新加载操作。如果要将多个映射存储在HashMap实例中，则使用足够大的容量创建映射将允许映射更有效地存储，而不是根据需要执行自动重新散列来扩展表。请注意，使用具有相同hashCode（）的许多键是减慢任何哈希表性能的可靠方法。为了改善影响，当键是Comparable时，此类可以使用键之间的比较顺序来帮助打破关系。请注意，此实现不同步。如果多个线程同时访问哈希映射，并且至少有一个线程在结构上修改了映射，则必须在外部进行同步。 （结构修改是添加或删除一个或多个映射的任何操作;仅更改与实例已包含的键关联的值不是结构修改。）这通常通过同步自然封装映射的某个对象来完成。 。如果不存在此类对象，则应使用Collections.synchronizedMap方法“包装”该映射。这最好在创建时完成，以防止对映射的意外不同步访问： Map m = Collections.synchronizedMap（new HashMap（...））; 所有这个类的“集合视图方法”返回的迭代器都是快速失败的：如果在创建迭代器之后的任何时候对映射进行结构修改，除了通过迭代器自己的remove方法之外，迭代器将抛出ConcurrentModificationException。因此，在并发修改的情况下，迭代器快速而干净地失败，而不是在未来的未确定时间冒着任意的，非确定性行为的风险。请注意，迭代器的快速失败行为无法得到保证，因为一般来说，在存在不同步的并发修改时，不可能做出任何硬性保证。失败快速迭代器会尽最大努力抛出ConcurrentModificationException。因此，编写依赖于此异常的程序以确保其正确性是错误的：迭代器的快速失败行为应该仅用于检测错误。此类是Java Collections Framework的成员。实施说明。此映射通常用作分区（分区）哈希表，但当分区变得太大时，它们会转换为 TreeNodes的分区，每个分区的结构与 java.util.TreeMap中的类似。大多数方法都尝试使用普通的bin，但在适用的情况下中继到TreeNode方法（只需检查 instanceof节点）。 TreeNodes的Bin可以像其他任何一样遍历和使用，但是当人口过多时还支持更快的查找。但是，由于正常使用中的绝大多数箱都没有人口过多，因此在表格方法的过程中可能会延迟检查树箱的存在。树容器（即，其元素都是TreeNodes的容器）主要由hashCode排序，但在tie的情况下，如果两个元素是相同的“C类实现Comparable ”，类型然后他们的compareTo方法用于排序。 （我们通过反射保守地检查泛型类型以验证 this - 请参阅methodsClassFor方法）。当密钥具有不同的哈希值或可订购时，树箱的复杂性在提供最坏情况的O（log n）操作时是值得的。因此，在hashCode（）方法的偶然或恶意用法下，性能会优雅地降级返回分布不佳的值，以及许多键共享hashCode的那些，只要它们也是 Comparable。 （如果这些都不适用，我们可能会浪费大约2倍的时间和空间而不采取任何预防措施。但是，唯一已知的案例源于糟糕的用户编程实践，这些实践已经很慢，这使得差别不大。）因为TreeNodes大约是常规节点大小的两倍，所以我们仅在bin包含足够的节点以保证使用时才使用它们（参见TREEIFY_THRESHOLD）。当它们变得太小（由于移除或调整大小）时，它们会转换回普通箱。在具有良好分布的用户hashCodes的用法中，树箱很少使用。理想情况下，在随机hashCodes下，bin中节点的频率遵循泊松分布（http://en.wikipedia.org/wiki/Poisson_distribution），其参数平均值约为0.5，默认大小调整阈值为0.75 ，虽然由于调整粒度而具有很大的差异。忽略方差，列表大小k的预期出现是（exp（-0.5） pow（0.5，k）/ factorial（k））。第一个值为： 0：0.60653066 1：0.30326533 2：0.07581633 3：0.01263606 4：0.00157952 5：0.00015795 6：0.00001316 7：0.00000094 8：0.00000006 更多：小于1/10百万树容器的根通常是它的第一个节点。但是，有时（目前仅在Iterator.remove上），根可能在其他地方，但可以在父链接之后恢复（方法TreeNode.root（））。所有适用的内部方法都接受哈希码作为参数（通常由公共方法提供），允许它们相互调用而无需重新计算用户hashCodes。 大多数内部方法也接受“tab”参数，即通常是当前表，但在调整大小或转换时可能是新的或旧的。当bin列表被树化，拆分或未解析时，我们将它们保持在相同的相对访问/遍历顺序（即字段 Node.next）中以更好地保留局部性，并略微简化处理拆分和遍历调用 iterator.remove。当在插入时使用比较器时，为了保持重新排序的总排序（或者在这里需要尽可能接近），我们将类和identityHashCodes比作 tie-breakers。普通vs树模式之间的使用和转换由于子类LinkedHashMap的存在而变得复杂。请参阅下面的钩子方法，定义为在插入时调用，删除和访问允许LinkedHashMap内部以其他方式保持独立于这些机制。 （这也要求将地图实例传递给可能创建新节点的一些实用程序方法。）类似并发编程的基于SSA的编码样式有助于避免在所有扭曲指针操作中出现锯齿错误。 synchronizedMap返回由指定映射支持的同步（线程安全）映射。为了保证串行访问，必须通过返回的映射完成对支持映射的所有访问。当迭代任何集合视图时，用户必须手动同步返回的映射：Map m = Collections.synchronizedMap（new HashMap（））; … Set s = m.keySet（）;//无需处于同步块…同步（m）{//在m上同步，而不是s！ Iterator i = s.iterator（）; //必须在同步块中（i.hasNext（））foo（i.next（））;不遵循此建议可能会导致非确定性行为。如果指定的映射是可序列化的，则返回的映射将是可序列化的。 List有序集合（也称为序列）。该接口的用户可以精确控制列表中每个元素的插入位置。用户可以通过整数索引（列表中的位置）访问元素，并搜索列表中的元素。与集合不同，列表通常允许重复元素。更正式地，列表通常允许元素对e1和e2成为e1.equals（e2），并且如果它们根本允许空元素，则它们通常允许多个空元素。通过在用户尝试插入运行时异常时抛出运行时异常，有人可能希望实现禁止重复的列表并不是不可想象，但我们希望这种用法很少见。 List接口在迭代器，add，remove，equals和hashCode方法的契约上放置了除Collection接口中指定的规则之外的其他规定。为方便起见，此处还包含其他继承方法的声明。 List接口提供了四种对列表元素进行位置（索引）访问的方法。列表（如Java数组）基于零。请注意，对于某些实现（例如，LinkedList类），这些操作可以与索引值成比例地执行。因此，如果调用者不知道实现，则迭代列表中的元素通常优选通过它进行索引。 List接口提供了一个特殊的迭代器，称为ListIterator，它允许元素插入和替换，以及Iterator接口提供的常规操作之外的双向访问。提供了一种方法来获得从列表中的指定位置开始的列表迭代器。 List接口提供了两种搜索指定对象的方法。从性能的角度来看，应谨慎使用这些方法。在许多实现中，它们将执行昂贵的线性搜索。 List接口提供了两种方法，可以有效地在列表中的任意点插入和删除多个元素。注意：虽然允许列表将自己包含为元素，但建议极其谨慎：equals和hashCode方法不再在这样的列表中很好地定义。某些列表实现对它们可能包含的元素有限制。例如，某些实现禁止null元素，并且一些实现对其元素的类型有限制。尝试添加不合格的元素会引发未经检查的异常，通常是NullPointerException或ClassCastException。尝试查询不合格元素的存在可能会引发异常，或者它可能只是返回false;一些实现将展示前一种行为，一些将展示后者。更一般地，在完成不会导致将不合格元素插入列表的不合格元素上尝试操作可能会引发异常，或者可能在实现的选择中成功。此类异常在此接口的规范中标记为“可选”。此接口是Java Collections Framework的成员。 Set不包含重复元素的集合。更正式地说，集合不包含元素对e1和e2，使得e1.equals（e2）和至多一个null元素。正如其名称所暗示的，该界面模拟数学集抽象。除了从Collection接口继承的那些之外，Set接口在所有构造函数的契约以及add，equals和hashCode方法的契约上放置了其他规定。为方便起见，此处还包含其他继承方法的声明。 （这些声明附带的规范是针对Set接口定制的，但它们不包含任何其他规定。）对于构造函数的附加规定，毫不奇怪，所有构造函数必须创建一个不包含重复元素的集合（如上所定义） ）。注意：如果将可变对象用作set元素，则必须非常小心。如果在对象是集合中的元素的同时以影响等于比较的方式更改对象的值，则不指定集合的​​行为。这种禁令的一个特例是，不允许集合将自身作为一个要素包含在内。某些集合实现对它们可能包含的元素有限制。例如，某些实现禁止null元素，并且一些实现对其元素的类型有限制。尝试添加不合格的元素会引发未经检查的异常，通常是NullPointerException或ClassCastException。尝试查询不合格元素的存在可能会引发异常，或者它可能只是返回false;一些实现将展示前一种行为，一些将展示后者。更一般地，尝试对不合格的元素进行操作，其完成不会导致将不合格的元素插入到集合中，可以在实现的选择中抛出异常或者它可以成功。此类异常在此接口的规范中标记为“可选”。此接口是Java Collections Framework的成员。 ArrayListList接口的可调整大小的数组实现。实现所有可选列表操作，并允许所有元素，包括null。除了实现List接口之外，此类还提供了一些方法来操作内部用于存储列表的数组的大小。 （这个类大致相当于Vector，除了它是不同步的。）size，isEmpty，get，set，iterator和listIterator操作在恒定时间内运行。添加操作以分摊的常量时间运行，即添加n个元素需要O（n）时间。所有其他操作都以线性时间运行（粗略地说）。与LinkedList实现相比，常数因子较低。每个ArrayList实例都有一个容量。容量是用于存储列表中元素的数组的大小。它始终至少与列表大小一样大。当元素添加到ArrayList时，其容量会自动增加。除了添加元素具有恒定的摊销时间成本这一事实之外，未指定增长策略的详细信息。在使用ensureCapacity操作添加大量元素之前，应用程序可以增加ArrayList实例的容量。这可能会减少增量重新分配的数量。请注意，此实现不同步。如果多个线程同时访问ArrayList实例，并且至少有一个线程在结构上修改了列表，则必须在外部进行同步。 （结构修改是添加或删除一个或多个元素的任何操作，或显式调整后备数组的大小;仅设置元素的值不是结构修改。）这通常通过同步一些自然封装的对象来实现。名单。如果不存在此类对象，则应使用Collections.synchronizedList方法“包装”该列表。这最好在创建时完成，以防止意外地不同步访问列表：List list = Collections.synchronizedList（new ArrayList（…））;此类的iterator和listIterator方法返回的迭代器是快速失败的：如果在创建迭代器之后的任何时候对列表进行结构修改，除了通过迭代器自己的remove或add方法之外，迭代器将抛出ConcurrentModificationException。因此，在并发修改的情况下，迭代器快速而干净地失败，而不是在未来的未确定时间冒着任意的，非确定性行为的风险。请注意，迭代器的快速失败行为无法得到保证，因为一般来说，在存在不同步的并发修改时，不可能做出任何硬性保证。失败快速迭代器会尽最大努力抛出ConcurrentModificationException。因此，编写依赖于此异常的程序以确保其正确性是错误的：迭代器的快速失败行为应该仅用于检测错误。此类是Java Collections Framework的成员。 Serializable实现java.io.Serializable接口的类启用了类的可序列化。未实现此接口的类将不会将其任何状态序列化或反序列化。可序列化类的所有子类型本身都是可序列化的。序列化接口没有方法或字段，仅用于标识可序列化的语义。为了允许序列化非可序列化类的子类型，子类型可以承担保存和恢复超类型的公共，受保护和（如果可访问）包字段的状态的责任。只有当它扩展的类具有可访问的no-arg构造函数来初始化类的状态时，子类型才可以承担此责任。如果不是这种情况，则声明类Serializable是错误的。将在运行时检测到错误。在反序列化期间，将使用类的public或protected no-arg构造函数初始化非可序列化类的字段。必须可以对可序列化的子类访问no-arg构造函数。可序列化子类的字段将从流中恢复。遍历图形时，可能会遇到不支持Serializable接口的对象。在这种情况下，将抛出NotSerializableException，并将标识非可序列化对象的类。在序列化和反序列化过程中需要特殊处理的类必须使用这些精确签名实现特殊方法： private void writeObject(java.io.ObjectOutputStream out) throws IOException private void readObject(java.io.ObjectInputStream in) throws IOException, ClassNotFoundException; private void readObjectNoData() throws ObjectStreamException; writeObject方法负责为其特定类编写对象的状态，以便相应的readObject方法可以恢复它。可以通过调用out.defaultWriteObject来调用保存Object字段的默认机制。该方法不需要关注属于其超类或子类的状态。通过使用writeObject方法或使用DataOutput支持的原始数据类型的方法将各个字段写入ObjectOutputStream来保存状态。 readObject方法负责从流中读取并恢复类字段。它可以调用in.defaultReadObject来调用恢复对象的非静态和非瞬态字段的默认机制。 defaultReadObject方法使用流中的信息来指定流中保存的对象的字段以及当前对象中相应命名的字段。这处理了类在演变为添加新字段时的情况。该方法不需要关注属于其超类或子类的状态。通过使用writeObject方法或使用DataOutput支持的原始数据类型的方法将各个字段写入ObjectOutputStream来保存状态。 readObjectNoData方法负责在序列化流未将给定类列为要反序列化的对象的超类的情况下初始化其特定类的对象的状态。如果接收方使用与发送方不同版本的反序列化实例的类，并且接收方的版本扩展了未由发送方版本扩展的类，则可能发生这种情况。如果序列化流已被篡改，也可能发生这种情况;因此，尽管存在“恶意”或不完整的源流，readObjectNoData仍可用于正确初始化反序列化对象。需要指定在将对象写入流时使用的备用对象的可序列化类应该使用确切的签名实现此特殊方法：ANY-ACCESS-MODIFIER对象writeReplace（）抛出ObjectStreamException;如果方法存在，则可以通过序列化调用此writeReplace方法，并且可以从要序列化的对象的类中定义的方法访问该方法。因此，该方法可以具有私有，受保护和包私有访问。对此方法的子类访问遵循java可访问性规则。从流中读取实例时需要指定替换的类应该使用精确签名实现此特殊方法。 ANY-ACCESS-MODIFIER对象readResolve（）抛出ObjectStreamException;此readResolve方法遵循与writeReplace相同的调用规则和可访问性规则。序列化运行时将每个可序列化类与版本号相关联，称为serialVersionUID，在反序列化期间使用该版本号来验证序列化对象的发送方和接收方是否已加载与该序列化兼容的该对象的类。如果接收者为具有与相应发送者类的serialVersionUID不同的对象加载了一个类，则反序列化将导致InvalidClassException。可序列化的类可以通过声明一个名为“serialVersionUID”的字段来显式声明它自己的serialVersionUID，该字段必须是static，final和long类型：ANY-ACCESS-MODIFIER static final long serialVersionUID = 42L;如果可序列化类未显式声明serialVersionUID，则序列化运行时将基于类的各个方面计算该类的默认serialVersionUID值，如Java（TM）对象序列化规范中所述。但是，强烈建议所有可序列化类显式声明serialVersionUID值，因为默认的serialVersionUID计算对类细节高度敏感，这些细节可能因编译器实现而异，因此在反序列化期间可能导致意外的InvalidClassExceptions。因此，为了保证跨不同java编译器实现的一致的serialVersionUID值，可序列化类必须声明显式的serialVersionUID值。强烈建议显式serialVersionUID声明尽可能使用private修饰符，因为此类声明仅适用于立即声明的类 - serialVersionUID字段作为继承成员无用。数组类不能声明显式的serialVersionUID，因此它们始终具有默认的计算值，但是对于数组类，不需要匹配serialVersionUID值。 writeObject方法负责为其特定类编写对象的状态，以便相应的readObject方法可以恢复它。可以通过调用out.defaultWriteObject来调用保存Object字段的默认机制。该方法不需要关注属于其超类或子类的状态。通过使用writeObject方法或使用DataOutput支持的原始数据类型的方法将各个字段写入ObjectOutputStream来保存状态。 readObject方法负责从流中读取并恢复类字段。它可以调用in.defaultReadObject来调用恢复对象的非静态和非瞬态字段的默认机制。 defaultReadObject方法使用流中的信息来指定流中保存的对象的字段以及当前对象中相应命名的字段。这处理了类在演变为添加新字段时的情况。该方法不需要关注属于其超类或子类的状态。通过使用writeObject方法或使用DataOutput支持的原始数据类型的方法将各个字段写入ObjectOutputStream来保存状态。 readObjectNoData方法负责在序列化流未将给定类列为要反序列化的对象的超类的情况下初始化其特定类的对象的状态。如果接收方使用与发送方不同版本的反序列化实例的类，并且接收方的版本扩展了未由发送方版本扩展的类，则可能发生这种情况。如果序列化流已被篡改，也可能发生这种情况;因此，尽管存在“恶意”或不完整的源流，readObjectNoData仍可用于正确初始化反序列化对象。需要指定在将对象写入流时使用的备用对象的可序列化类应该使用确切的签名实现此特殊方法：ANY-ACCESS-MODIFIER对象writeReplace（）抛出ObjectStreamException;如果方法存在，则可以通过序列化调用此writeReplace方法，并且可以从要序列化的对象的类中定义的方法访问该方法。因此，该方法可以具有私有，受保护和包私有访问。对此方法的子类访问遵循java可访问性规则。从流中读取实例时需要指定替换的类应该使用精确签名实现此特殊方法。 ANY-ACCESS-MODIFIER对象readResolve（）抛出ObjectStreamException;此readResolve方法遵循与writeReplace相同的调用规则和可访问性规则。序列化运行时将每个可序列化类与版本号相关联，称为serialVersionUID，在反序列化期间使用该版本号来验证序列化对象的发送方和接收方是否已加载与该序列化兼容的该对象的类。如果接收者为具有与相应发送者类的serialVersionUID不同的对象加载了一个类，则反序列化将导致InvalidClassException。可序列化的类可以通过声明一个名为“serialVersionUID”的字段来显式声明它自己的serialVersionUID，该字段必须是static，final和long类型：ANY-ACCESS-MODIFIER static final long serialVersionUID = 42L;如果可序列化类未显式声明serialVersionUID，则序列化运行时将基于类的各个方面计算该类的默认serialVersionUID值，如Java（TM）对象序列化规范中所述。但是，强烈建议所有可序列化类显式声明serialVersionUID值，因为默认的serialVersionUID计算对类细节高度敏感，这些细节可能因编译器实现而异，因此在反序列化期间可能导致意外的InvalidClassExceptions。因此，为了保证跨不同java编译器实现的一致的serialVersionUID值，可序列化类必须声明显式的serialVersionUID值。强烈建议显式serialVersionUID声明尽可能使用private修饰符，因为此类声明仅适用于立即声明的类 - serialVersionUID字段作为继承成员无用。数组类不能声明显式的serialVersionUID，因此它们始终具有默认的计算值，但是对于数组类，不需要匹配serialVersionUID值。 BufferedInputStreamBufferedInputStream将功能添加到另一个输入流 - 即缓冲输入并支持标记和重置方法的功能。创建BufferedInputStream时，会创建一个内部缓冲区数组。当读取或跳过来自流的字节时，内部缓冲区根据需要从包含的输入流中重新填充，一次多个字节。标记操作记住输入流中的一个点，并且重置操作使得从最近的标记操作开始读取的所有字节在从包含的输入流中获取新字节之前被重新读取。 Random.nextint();返回从此随机数生成器的序列中提取的伪随机，均匀分布的int值，介于0（包括）和指定值（不包括）之间。 nextInt的常规协定是伪随机生成并返回指定范围内的一个int值。所有可能的int值都以（近似）相等的概率产生。方法nextInt（int bound）由Random类实现，如下所示： public int nextInt(int bound) { if (bound &lt;= 0) throw new IllegalArgumentException(&quot;bound must be positive&quot;); if ((bound &amp; -bound) == bound) // i.e., bound is a power of 2 return (int)((bound * (long)next(31)) &gt;&gt; 31); int bits, val; do { bits = next(31); val = bits % bound; } while (bits - val + (bound-1) &lt; 0); return val; } 在前面的描述中使用对数“近似”仅因为下一个方法仅是大致独立选择的比特的无偏源。如果它是随机选择位的完美来源，则所示算法将从所述范围中选择具有完美均匀性的int值。该算法有点棘手。它拒绝会导致分布不均匀的值（由于2 ^ 31不能被n整除）。值被拒绝的概率取决于n。最坏的情况是n = 2 ^ 30 + 1，其中拒绝的概率是1/2，并且循环终止之前的预期迭代次数是2.该算法处理n是2的幂的情况。 ：它从底层伪随机数生成器返回正确数量的高位。在没有特殊处理的情况下，将返回正确数量的低位。已知诸如由该类实现的线性同余伪随机数发生器在其低阶位的值序列中具有短周期。因此，如果n是2的小幂，则这种特殊情况极大地增加了由连续调用此方法返回的值序列的长度。 java内存区域code segment 存放代码data segment 静态变量 字符串常量stack 局部变量heap new出的对象 java变量的分类按位置划分局部变量:方法体内部声明的变量(方法的参数也是局部变量)成员变量:方法体外面,类里面(也就是俗称的属性)按数据类型划分基本数据类型引用数据类型 递归方法调用自身称为递归 System类System类包含几个有用的类字段和方法。它无法实例化。 System类提供的功能包括标准输入，标准输出和错误输出流;访问外部定义的属性和环境变量;加载文件和库的方法;以及用于快速复制阵列的一部分的实用方法。 类和对象对一个对象的抽象,叫做类对一个类的实际描述,叫对象类是静态的概念,代码区对象是new出来的,位于heap,类的每个成员变量在不同的对象中的都有不一样的值(除了静态变量),而方法只有一份,执行的时候占用内存heap是动态分配内存的java里面除了基本类型之外的变量类型都称之为引用类型java中的对象是通过引用对其操作的 C c = new C(); code seg 先读取代码,然后在栈中创建c对象,指向堆内存种的C对象 关联关系是最弱的一种关系一个类使用的参数为另一个对象称为有关联关系如 教授.教(研究生); 继承关系java类单继承,接口多继承 实现关系接口的实现 方法的重写和重载对象的创建和使用1.使用new关键字2.使用对象引用.成员变量或者引用对象的成员变量3.使用对象引用.方法(参数列表)来调用对象的方法4.同一个类的每个对象有不同的成员变量存储空间5.同一个类的每个对象共享该类的方法6.非静态方法是针对每个对象进行调用 this关键字1.在类的方法定义中使用的this关键字代表使用该方法的对象的引用2.当必须指出当前使用方法的对象是谁时要使用this3.有时使用this可以处理方法中的成员变量和参数重名的情况4.this可以看作是一个变量,它的值是当前对象的引用 static没有对象也可以访问,使用类名.静态变量,例如System.out在类中,用static声明的成员变量为静态成员变量,它为该类的共用变量,在第一次使用时被初始化,对于该类的所有对象来说,static成员变量只有一份用static声明的方法为静态方法,在调用该方法时,不会将对象的引用传递给它,所以在static方法中不可访问非static的成员静态方法不再是针对于某个对象调用,所以不能访问非静态成员可以通过对象引用或类名(无需实例化)访问静态成员 对象的转型1.一个基类的引用类型变量可以”指向”其子类对象2.一个积累的引用不可以访问其子类对象新增加的成员(属性和方法)3.可以使用引用变量instance of类名 来判断该引用型变量所”指向”的对象是否属于该类或该类的子类4.子类的对象可以当作基类的对象来使用称作向上转型(upcasting),反之称为向下转型(downcasting) instance ofjava 中的instanceof 运算符是用来在运行时指出对象是否是特定类的一个实例。instanceof通过返回一个布尔值来指出，这个对象是否是这个特定类或者是它的子类的一个实例。如果 object 是 class 的一个实例，则 instanceof 运算符返回 true。如果 object 不是指定类的一个实例，或者 object 是 null，则返回 false。但是instanceof在Java的编译状态和运行状态是有区别的：在编译状态中，class可以是object对象的父类，自身类，子类。在这三种情况下Java编译时不会报错。在运行转态中，class可以是object对象的父类，自身类，不能是子类。在前两种情况下result的结果为true，最后一种为false。但是class为子类时编译不会报错。运行结果为false。 多态面向对象编程核心动态绑定指的是”在执行期间(而非编译期间)判断所引用对象的实际类型,根据其实际的类型调用其相应的方法”在方法的参数中传入父类的引用,然后实际传入子类的对象1.要有继承2.要有重写3.父类引用指向子类对象父类中的方法没有实现的必要,因为没有意义,将来子类会自行实现父类中的方法,但是必须要定义抽象方法就是用来被重写的抽象类可以理解为一个残缺不全的类,无法进行实例化,抽象方法类同,只需要声明,无需实现final修饰的变量不能被改变,修饰的方法不能被重写,修饰的类不能被继承 接口1.接口是抽象方法和常量值的定义的集合2.从本质上讲,接口是一种特殊的抽象类,这种抽象类中只包含常量和方法的定义,而没有变量和方法的实现接口中的所有成员变量都是public static final的常量,所有的成员方法都是public abstract的抽象方法,接口默认是public的接口可以继承其他的接口,并增加新的属性和成员方法多个类可以实现同一个接口,一个类可以实现多个接口,与继承关系类似,接口与实现类之间存在多态性 异常1.所谓错误是指在程序运行的过程中发生的一些异常事件(如:数组下标越界,所要读取的文件不存在)2.设计良好的程序应该在异常发生时提供处理这些错误的方法,使得程序不会因为发生异常而阻断或产生不可预见的后果3.java程序的执行过程中如果出现异常事件,可以生成一个异常类对象,该异常对象封装了异常事件的信息并将被提交给java运行时的系统,这个过程称之为抛出异常(throw)4.java运行时系统接收到异常对象时,会寻找能处理这一异常的代码并把当前异常对象交给其处理,这一过程称为捕获异常(catch) 异常的分类Throwable -Error \系统错误,无需处理 -Exception \能处理的错误 -OtherException \必须需要catch的 -RuntimeException \不必须catch的 数组数组是引用类型,它的元素相当于类的成员变量,因此数组分配空间后,每个元素也按照成员变量的规则被隐式初始化数组元素的引用定义并用运算符new为之分配空间后,才可以引用数组中的每个元素,数组元素的引用方式为下标,合法值为0~n-1,没一个数组都有一个length属性指明长度 排序选择排序 for(int i=0;i&lt;array.length;i++){ for(int j=i+1;j&lt;array.length;j++){ if(a[j] &lt; a[i]){ int tmp = a[i]; a[i] = a[j]; a[j] = tmp; } } } 选择排序优化,每次循环的时候仅进行一次交换,相当于做一个需要临时对比的对象 for(int i=0;i&lt;array.length;i++){ int k = i; for(int j=k+1;j&lt;array.length;j++){ if(a[j] &lt; a[k]){ k = j; } } if(k != i){ int tmp = a[i]; a[i] = a[k]; a[k] = tmp; } } 选择排序再次优化,最开始进行分配空间,不需要每次都去new int k,tmp; for(int i=0;i&lt;array.length;i++){ k = i; for(int j=k+1;j&lt;array.length;j++){ if(a[j] &lt; a[k]){ k = j; } } if(k != i){ tmp = a[i]; a[i] = a[k]; a[k] = tmp; } } 冒泡排序int length = array.length; for(int i=length-1;i&gt;=1;i--){ for(int j=0;j&lt;i-1;j++){ if(a[j].compare(a[j+1])&gt;0){ Date tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; } } } 查找算法二分查找法数组先排序后对半,然后继续重复,直到查找到对象位置 二维数组相当于数组的数组 int[][] array; Stringstring.trim() 返回一个字符串，其值为此字符串，并删除任何前导和尾随空格。如果此String对象表示空字符序列，或者此String对象表示的字符序列的第一个和最后一个字符的代码都大于’\ u0020’（空格字符），则返回对此String对象的引用。否则，如果字符串中没有代码大于’\ u0020’的字符，则返回表示空字符串的String对象。否则，令k为代码大于’\ u0020’的字符串中第一个字符的索引，并且令m为代码大于’\ u0020’的字符串中最后一个字符的索引。返回一个String对象，表示该字符串的子字符串，该字符串以索引k处的字符开头，以索引m处的字符结尾，即this.substring（k，m + 1）的结果。此方法可用于从字符串的开头和结尾修剪空白（如上所述）。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS名词解释]]></title>
    <url>%2F2019%2F08%2F10%2FHDFS%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[HDFSFileSystem一个相当通用的文件系统的抽象基类。它可以实现为分布式文件系统，也可以实现为反映本地连接磁盘的“本地”文件系统。本地版本适用于小型Hadoop实例和测试。应编写可能使用Hadoop分布式文件系统的所有用户代码以使用FileSystem对象。 Hadoop DFS是一个多机系统，显示为单个磁盘。由于其容错性和可能非常大的容量，它很有用。本地实现是LocalFileSystem，分布式实现是DistributedFileSystem。 Path在FileSystem中命名文件或目录。路径字符串使用斜杠作为目录分隔符。如果路径字符串以斜杠开头，则它是绝对的。 FSDataOutputStream在DataOutputStream中包装OutputStream的实用程序。 IOUtils用于I / O相关功能的实用程序类。 FileStatus表示文件的客户端信息的接口。 BlockLocation表示块的网络位置，有关包含块副本的主机的信息，以及其他块元数据（例如，与块关联的文件偏移量，长度，是否已损坏等）。 seek();寻求给定的偏移量。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce名词解释]]></title>
    <url>%2F2019%2F08%2F10%2FMapReduce%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[MapReduceMapper:将输入键/值对map到一组中间键/值对。map是将输入记录转换为中间记录的各个任务。转换后的中间记录不必与输入记录的类型相同。给定的输入对可以map到零个或多个输出对。Hadoop Map-Reduce框架为作业的InputFormat生成的每个InputSplit生成一个map任务。Mapper实现可以通过JobContext.getConfiguration（）访​​问作业的Configuration。框架首先调用setup（org.apache.hadoop.mapreduce.Mapper.Context），然后调用InputSplit中每个键/值对的map（Object，Object，org.apache.hadoop.mapreduce.Mapper.Context）。最后调用cleanup（org.apache.hadoop.mapreduce.Mapper.Context）。随后将与给定输出键关联的所有中间值按框架分组，并传递给Reducer以确定最终输出。用户可以通过指定两个关键的RawComparator类来控制排序和分组。Mapper输出按照Reducer进行分区。用户可以通过实现自定义分区程序来控制哪些键（以及记录）转到哪个Reducer。用户可以选择通过Job.setCombinerClass（Class）指定组合器，以执行中间输出的本地聚合，这有助于减少从Mapper传输到Reducer的数据量。应用程序可以指定是否以及如何压缩中间输出以及通过配置使用哪些CompressionCodecs。如果作业减少为零，则Mapper的输出将直接写入OutputFormat而不按键排序。Example： public class TokenCounterMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } 应用程序可以覆盖run（org.apache.hadoop.mapreduce.Mapper.Context）方法，以对map处理施加更大的控制，例如多线程映射器等 Reducer将一组共享键的中间值减少为一组较小的值。 Reducer实现可以通过JobContext.getConfiguration（）方法访问作业的Configuration。Reducer有3个主要阶段： 1.ShuffleReducer通过网络使用HTTP复制每个Mapper的排序输出。 2.Sort框架根据key合并排序Reducer输入（因为不同的Mapper可能输出相同的键）。Shuffer和Sort阶段同时发生，即在获取输出时，它们被合并。SecondarySort要对值迭代器返回的值实现二级排序，应用程序应使用辅助键扩展密钥并定义分组比较器。密钥将使用整个密钥进行排序，但将使用分组比较器进行分组，以确定在同一调用中发送哪些密钥和值以进行减少。分组比较器通过Job.setGroupingComparatorClass（Class）指定。排序顺序由Job.setSortComparatorClass（Class）控制。例如，假设您要查找重复的网页，并使用“最佳”已知示例的网址标记它们。你可以像下面这样设置工作：· 输入键：url· Map输入值：document· Map输出键：document checksum，url pagerank· Map输出值：url· 分区：通过校验和· OutputKeyComparator：通过校验和然后减少pagerank· OutputValueGroupingComparator：by checksum 3.Reduce在此阶段为排序的输入中的每个&lt;key，（值集合）&gt;调用reduce（Object，Iterable，org.apache.hadoop.mapreduce.Reducer.Context）方法。reduce任务的输出通常通过TaskInputOutputContext.write（Object，Object）写入RecordWriter。Reducer的输出不会重新排序。 Example: public class IntSumReducer&lt;Key&gt; extends Reducer&lt;Key,IntWritable,Key,IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Key key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } Partitioner分区关键空间。分区程序控制中间映射输出的键的分区。密钥（或密钥的子集）用于通常通过散列函数来导出分区。分区总数与作业的reduce任务数相同。因此，这将控制m减少任务中的哪一个中间密钥（以及因此记录）被发送以进行减少。注意：如果您需要Partitioner类来获取Job的配置对象，请实现Configurable接口。 JobJob提交者对Job的看法。它允许用户配置作业，提交作业，控制其执行以及查询状态。 set方法仅在提交作业之前有效，之后它们将抛出IllegalStateException。通常，用户创建应用程序，通过作业描述作业的各个方面，然后提交作业并监视其进度。以下是有关如何提交作业的示例： // Create a new Job Job job = Job.getInstance(); job.setJarByClass(MyJob.class); // Specify various job-specific parameters job.setJobName(&quot;myjob&quot;); job.setInputPath(new Path(&quot;in&quot;)); job.setOutputPath(new Path(&quot;out&quot;)); job.setMapperClass(MyJob.MyMapper.class); job.setReducerClass(MyJob.MyReducer.class); // Submit the job, then poll for progress until the job is complete job.waitForCompletion(true); //定义比较器，在比较键传递给Reducer之前控制键的排序方式。 job.setSortComparatorClass(MySortComparatorClass.class); //定义控制哪些键组合在一起的比较器，用于对Reducer.reduce的单次调用（Object，Iterable，org.apache.hadoop.mapreduce.Reducer.Context） job.setGroupingComparatorClass(MyGroupingComparatorClass.class);]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ObjectHashCode]]></title>
    <url>%2F2019%2F08%2F10%2FObjectHashCode%2F</url>
    <content type="text"><![CDATA[Indicates whether some other object is “equal to” this one. The {@code equals} method implements an equivalence relationon non-null object references: It is reflexive: for any non-null reference value {@code x}, {@code x.equals(x)} should return {@code true}. //反射性,当对于非空值来说,x.equals(x)应当return true It is symmetric: for any non-null reference values {@code x} and {@code y}, {@code x.equals(y)} should return {@code true} if and only if {@code y.equals(x)} returns {@code true}. //对称性,当对于非空值来说,如果y.equals(x)return true那么x.equals(y)也应当返回true It is transitive: for any non-null reference values {@code x}, {@code y}, and {@code z}, if {@code x.equals(y)} returns {@code true} and {@code y.equals(z)} returns {@code true}, then {@code x.equals(z)} should return {@code true}. //传递性,当对于非空值来说,如果x.equals(y)return true,y.equals(z)return true那么x.equals(z)应当return true It is consistent: for any non-null reference values {@code x} and {@code y}, multiple invocations of {@code x.equals(y)} consistently return {@code true} or consistently return {@code false}, provided no information used in {@code equals} comparisons on the objects is modified. //一致性,当对于非空值来说,在对象没有被修改的时候,多次调用x.equals(y)应当一致return true或者一致return false For any non-null reference value {@code x}, {@code x.equals(null)} should return {@code false}. //对于任何非空的参考值,x.equals(null)应当return false The {@code equals} method for class {@code Object} implements the most discriminating possible equivalence relation on objects; //这个方法实现了大多数可能识别的等价关系 that is, for any non-null reference values {@code x} and {@code y}, this method returns {@code true} if and only if {@code x} and {@code y} refer to the same object ({@code x == y} has the value {@code true}). //也就是说,对于任何非空引用值x和y,当且仅当x和y引用了同一个对象,这个方法返回了true,具有相同的code值 Note that it is generally necessary to override the {@code hashCode} method whenever this method is overridden, so as to maintain the general contract for the {@code hashCode} method, which states that equal objects must have equal hash codes. //注意,通常需要覆盖hashCode方法当这个方法被覆盖的时候,这是维护一般的约定,即相同对象必须有相同的hashCode @param obj the reference object with which to compare.//如果这两个对象是一样的,return true,否则false@return {@code true} if this object is the same as the obj argument; {@code false} otherwise.@see #hashCode()@see java.util.HashMap Indicates whether some other object is “equal to” this one. The {@code equals} method implements an equivalence relation on non-null object references: It is reflexive: for any non-null reference value {@code x}, {@code x.equals(x)} should return {@code true}. //反射性,当对于非空值来说,x.equals(x)应当return true It is symmetric: for any non-null reference values {@code x} and {@code y}, {@code x.equals(y)} should return {@code true} if and only if {@code y.equals(x)} returns {@code true}. //对称性,当对于非空值来说,x.equals(y)应当return true当且仅当y.equals(x)return true It is transitive: for any non-null reference values {@code x}, {@code y}, and {@code z}, if {@code x.equals(y)} returns {@code true} and {@code y.equals(z)} returns {@code true}, then {@code x.equals(z)} should return {@code true}. //传递性,当对于非空值来说,如果x.equals(y)return true,y.equals(z)return true那么x.equals(z)应当return true It is consistent: for any non-null reference values {@code x} and {@code y}, multiple invocations of {@code x.equals(y)} consistently return {@code true} or consistently return {@code false}, provided no information used in {@code equals} comparisons on the objects is modified. //一致性 For any non-null reference value {@code x}, {@code x.equals(null)} should return {@code false}. The {@code equals} method for class {@code Object} implements the most discriminating possible equivalence relation on objects; that is, for any non-null reference values {@code x} and {@code y}, this method returns {@code true} if and only if {@code x} and {@code y} refer to the same object ({@code x == y} has the value {@code true}). Note that it is generally necessary to override the {@code hashCode} method whenever this method is overridden, so as to maintain the general contract for the {@code hashCode} method, which states that equal objects must have equal hash codes. @param obj the reference object with which to compare. @return {@code true} if this object is the same as the obj argument; {@code false} otherwise. @see #hashCode() @see java.util.HashMap]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构]]></title>
    <url>%2F2019%2F08%2F10%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[数据结构数组(Array)数组是一种聚合数据类型，它是将具有相同类型的若干变量有序地组织在一起的集合。数组可以说是最基本的数据结构，在各种编程语言中都有对应。一个数组可以分解为多个数组元素，按照数据元素的类型，数组可以分为整型数组、字符型数组、浮点型数组、指针数组和结构数组等。数组还可以有一维、二维以及多维等表现形式。 栈(Stack)栈是一种特殊的线性表，它只能在一个表的一个固定端进行数据结点的插入和删除操作。栈按照后进先出的原则来存储数据，也就是说，先插入的数据将被压入栈底，最后插入的数据在栈顶，读出数据时，从栈顶开始逐个读出。栈在汇编语言程序中，经常用于重要数据的现场保护。栈中没有数据时，称为空栈。 队列(Queue)队列和栈类似，也是一种特殊的线性表。和栈不同的是，队列只允许在表的一端进行插入操作，而在另一端进行删除操作。一般来说，进行插入操作的一端称为队尾，进行删除操作的一端称为队头。队列中没有元素时，称为空队列。 链表(Linked List)链表是一种数据元素按照链式存储结构进行存储的数据结构，这种存储结构具有在物理上存在非连续的特点。链表由一系列数据结点构成，每个数据结点包括数据域和指针域两部分。其中，指针域保存了数据结构中下一个元素存放的地址。链表结构中数据元素的逻辑顺序是通过链表中的指针链接次序来实现的。 树(Tree)树是典型的非线性结构，它是包括，2个结点的有穷集合K。在树结构中，有且仅有一个根结点，该结点没有前驱结点。在树结构中的其他结点都有且仅有一个前驱结点，而且可以有聊个后继结点，m≥0。 图(Graph)图是另一种非线性数据结构。在图结构中，数据结点一般称为顶点，而边是顶点的有序偶对。如果两个顶点之间存在一条边，那么就表示这两个顶点具有相邻关系。 堆(Heap)堆是一种特殊的树形数据结构，一般讨论的堆都是二叉堆。堆的特点是根结点的值是所有结点中最小的或者最大的，并且根结点的两个子树也是一个堆结构。 散列表(Hash)散列表源自于散列函数(Hash function)，其思想是如果在结构中存在关键字和T相等的记录，那么必定在F(T)的存储位置可以找到该记录，这样就可以不用进行比较操作而直接取得所查记录。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2F2019%2F07%2F12%2FMapReduce%2F</url>
    <content type="text"><![CDATA[“费曼学习法”尝试mapreduce,一种计算框架.需求:我现在有很多数据,我需要将其中的数据做统计数量的工作当自行写一个mapreduce时,有一些步骤是特别固定的,如下图,我带上了注释比如说设置配置文件,格式化输入输出,设置job的各种属性等等,其中初学者应该更注重的是,自定义的mapper类,自定义的reducer类下面我们来点开这个mapper类和reducer类mapreduce总体上可以分为两大阶段,1.map;2reduce,其中,map阶段相当于,读取数据的每一行,将每一行拆分为key和value,传递到reduce阶段.本例中,mapper类中的key其实是读取文件时当前第一个字节面向源文件的偏移量.注意,mapper端,每读取一行数据的时候,就会调用一次map方法,map方法的三个参数分别为当前行第一个字节面向源文件的偏移量,当前行读取的数据,context全局对象.可以看到,每次读取一行数据之后,示例中将读取的一行数据拆分,其中key为整行数据,value固定写死成1,最后将key和value全部写入上下文即可.再来看MyReducer类,其中,需要注意的是,每一组数据调用一次reduce方法,这点跟map端不一样,其中三个参数的含义分别是,map端传过来的key类型,每一组数据的value的迭代器,全局context对象.其中key未做任何操作,将value加到一起便是当前key在原始数据中出现的总次数.是的,这便是一个最简单的mapreduce示例,下面我们来看一个稍稍复杂一点点的.首先提出需求,我有一些这样的数据其中,年-月-日 空格 时间 制表符 码值 制表符 温度现在我希望求出,每年每月气温最高的两天.注意看我的需求,拿六月举例子,也就是说,我希望得到6月2日的31度和6月1日的39度,而不是6月1日的39度和6月1日的38度,因为这是一天.下面开始示例注意,不要被代码所吓倒,代码是人研究出来的,一点点去分析.可以看到其实还是有很多重复的,至此我们可以得到一个结论,框架很灵活,可以满足很多需求,但是api基本都是固定的,所以只要一步一步来,一定能弄懂.细看一下,其实在这个示例中,其实也只是多了几个api的调用而已,只是从1+1变成了1+2在mapreduce计算框架中,可以允许自定义map的输出类,所以我们将map的输出定义成一个类.下面我们来看看这个类此处,老师的建议是,不要盲目的自信,请粘一条数据出来做注释写逻辑,否则后果自负.我们依旧优先重写map方法,然后把数据按照制表符切分.请注意思考逻辑,我们的key是否需要日?答案是需要的,就因为必须有日子,才能解决同一天的温度两次最高排除的逻辑,也就是上面所提到的问题.所以我们将key设计为,年+月+日+温度,将value设计为码值,其实可以不需要value,不过目前别那么嚣张.可以看到,这次我们传的key已经不简简单单是一串数字或者一个字符串了,而是一个MyOutPutKey对象,下面我们点开这个对象来看看.在这个key对象中,我们定义了年月日温度四个属性,然后将读取的年月日温度分别赋予给这个类就好,请注意,这个类是需要实现序列化和反序列化的,并且重点来了,我们知道map端的输出是需要排序的(具体原因是因为如果不排序,那么reduce端就得排序,那样reduce压力太大),所以还需要重写一个compareTo的方法,这里使用的是数值序的方法.至于map端的输出value,我们依然暂时使用Intwritable方法,也就是先输出码值.在mapreduce计算框架中,当用户设置了排序的方法,框架会自动帮你调用.那么我们为什么需要自定义比较器呢?因为我们的排序不能满足我们的需求,请注意,我们需要的是温度倒序,但是我们做的却是时间正序,所以,我们需要自定义一个比较器,来将数据彻底的摆好.这一步发生在传递给reduce之前,请再次注意,在mapreduce计算框架中,只有map端才是真真正正做了快速排序,千万不要认为reduce端也会做排序,如果阅读源码发现reduce中有一个sort字样,千万要百度查明白,那个不是排序.下面我们来看一下这个自定义的比较器类.下面来看一下分区的逻辑,所谓分区,也就是说,当map端将数据切好以后,指定的key去到哪个reduce里面,实际上是由partitioner分配的,所以我们来自定义一个分区的实现.我们用key的年去模分区数就可以,mapreduce默认的分区数和reduce的task数是相同的.在mapreduce计算框架中,请注意,同一个分区内可能有多个组,reduce端读取数据是论组读取的,这句话的延申含义,计算框架可以向我们保证相同的key去到同一个分区,也就是说,具有相同key值的记录是属于同一个分组的,相当于group by一下这个key,说的再直白一点,我们的分区是年,我们的分组是月,有多少个年调用多少次reduce方法,reduce会把每年中的所有月份分组跑.终于到了最后一个类了,别放弃,来看一下这个reduce类还是那句老师的忠告,别那么自信,拿出数据,写逻辑.首先看我们的需求,我们reduce端拿到的数据,其实已经是map端为我们排号的数据了,所以,我们拿到迭代器,没有必要把所有数据都遍历一遍.为啥?倒序已经排好了,取出前两条不久完事了?请注意,这里面还是有坑的,还请认真看注释.这个循环的逻辑,第一条我取出来,然后这个组内的所有与第一条相同的pass掉,不做处理,继续循环,直到遇到与key的day不同的第二条,取出来,打断.完.—-本身我这个人就不太会说话,我也第一次尝试费曼学习法,如有错误,希望大家及时帮我指出,我也是小白,感谢(鞠躬]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop初级1]]></title>
    <url>%2F2019%2F07%2F05%2FHadoop%E5%88%9D%E7%BA%A71%2F</url>
    <content type="text"><![CDATA[简单上一个类,一个很简单的hdfsclient客户端,请诸位务必能码出来实现,不然以后没得玩了第二节,写了一个简单的mapreduce,其中文件是在linux上面创建的,其实跟mapreduce wordcount一样,请诸位务必能码出来实现期间,请熟练使用idea的各种快捷功能,比如说ctrl+q可以查看当前方法的注释,对象也可以用.再举一些例子比如ctrl+art+b可以跳转到当前方法的实现类,ctrl+f可以在当前页面搜索关键字等等太多了,就不一一细说了.另外,推荐一个idea的插件,叫做阿里编码规约,我以前一直认为我的编码还可以,然而被阿里的插件一约束才发现,根本不是那么一回事.希望大家都能严格遵守编码规矩,我也会尝试修改的.]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring初级2]]></title>
    <url>%2F2019%2F06%2F28%2FSpring%E5%88%9D%E7%BA%A72%2F</url>
    <content type="text"><![CDATA[spring mvc 第二天课程个人理解老师主要讲解的是有关于filter及任务流程控制不废话,上码,老规矩,至少三遍,我目前写了两遍,周末再来一次吧applicationcontrollerservice,很简单,不多说了mapper和entity,用mybatis-gui生成的,老师教的,怎么说呢,初学可能觉得很难用,但是我估计日后很方便,具体用法百度,不会再问我xml和html文件,只挑重点截取了,html模板有空传到网站资源模块上,同样都是生成的,不要在这里纠结配置!重点!filter的使用,好好看三张图证明项目能正常跑end,本期还有个数据库加密作业,同样完成,上模板,数据库用的8.0.14,跟5语法相同sql:update account set password=md5(password)]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring初级1]]></title>
    <url>%2F2019%2F06%2F26%2FSpring%E5%88%9D%E7%BA%A71%2F</url>
    <content type="text"><![CDATA[spring mvc第一天课程先把这个demo抄十遍,有问题再问applicationproperties(别忘了搭建mysql,版本随意)controllerservice(注意service层的划分,再仔细想想为什么需要service层,以及和dao层的关联)respstat(注意,这个对象构建的原因是controller层返回的消息)daoentityhtml(用的thymeleaf模板,我也是第一次接触这个模板,感觉不顺手,在pomxml里面可以看到配置,注意目录树结构,千万别乱改,我在这地方吃瘪了两个小时)pom.xml(用的maven,本来不想贴,但是还是贴了吧,没准能帮助小白呢)整个目录树结构 三张图证明项目没问题]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[资源]]></title>
    <url>%2F2019%2F06%2F04%2F%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[这里是资源模块,以后大概会放一些各种软件资源吧大概]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>resource</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活]]></title>
    <url>%2F2019%2F06%2F04%2F%E7%94%9F%E6%B4%BB%2F</url>
    <content type="text"><![CDATA[这里是生活模块,以后无聊会放放歌吧大概,我个人没什么上传生活身边东西的习惯]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[aboutme]]></title>
    <url>%2F2019%2F05%2F30%2Faboutme%2F</url>
    <content type="text"><![CDATA[弟中弟一个,日常上班+写写垃圾代码,欢迎互换友链]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>aboutme</tag>
      </tags>
  </entry>
</search>
